{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"![[Pasted image 20250826134527.png]] I am a design-loving nerd, navigating the intersection of technology, mathematics and art, on this wild ride of figuring out ways to do something meaningful. Nowadays, I am deeply interested in the intersection of machine learning and computational finance. Econometrics and Time-Series Analysis are my craft and thus I hunt patterns and weave stories with data for a living. I speak Hindi and English to humans, while Python and Javascript to computers. I also authored a Python library that facilitates comparison and testing of dataframes. I am currently learning deep learning, financial engineering and to speak Polish and Rust. In my free time, I work as a Market Risk Quant at Union Bank of Switzerland (UBS).","title":"Home"},{"location":"Affirmation/","text":"A list of timeless reminders about how l aim to be or just how life seems to work. FOCUS ON FASTER ITERATION AND NOT ON ACCURACY The days are long but decades are short. It's important to do things fast. A week is 2% of the year. Time is the denominator. Motion creates information. You learn more per unit time because you make contact with reality more frequently. Optimize for being spectacularly right some of the time, and low-stakes wrong a lot of the time. The goal is not to avoid mistakes; the goal is to achieve uncorrelated levels of excellence in some dimension. The downsides are worth it. GO TOUCH THE METAL Hard skills are super important. Get good with your tools. I mean, really good. Get rid of all the friction. Keep oiling your tools and don't let them rust. Don't get lost in the busy bandwagon of managerial or bureaucratic duties. Those create empty suits. GET GOOD AT SMALL TALK Soft skills are even more important. Sapiens are emotional species. They cannot think objectively. Get good at telling stories. Get good at sales. Listen at least thrice more than you speak. Better to get your dopamine from improving your ideas than from having them validated If right things are said in a wrong way, they lose their righteousness. And yes! when deep into shit, keep your mouth shut. INSPIRATION IS PERISHABLE. ACT ON IT. Start before you are ready Thousands have the same idea, which you think is original and exclusive to you. Thinking is easy, doing is hard! Only execution matters. Ship it! LIFE ISN'T UNFAIR, IT IS DISPROPORTIONATELY UNFAIR. The path isn't linear. It is a Geometric Brownian motion with a drift. Make sure that the trend coefficient is always positive. There are people who are way more smarter and hardworking than you are, but are not sitting on the priviliges that you have. And vice verca. At the age of 25, it\u2019s very tempting to conflate agency and determinism in scripts. You have a default state \u201cI\u2019m a smart guy, I have these resources at hand. The future looks full of possibilities.\" And then life will hit you hard on face. Efficient Market Hypothesis is a flawed heuristic. WE ARE MONKEYS. WE COPY. You will always feel an overpowering urge to fit in and follow the herd. It's natural! It is the evolutionary make-up of your brain that you inherited from your primordial ancestors. The best thing you can do is that make sure that the herd around you is smarter and more aware than you are. HAVE SELF BELIEF TO THE POINT OF DELUSION You can do more than you think. Always try to know to what extent you are operating in a condition of learned helplessness in institutionalized environments. You are tied down by your own invisible orthodoxy. The laws of physics are the only limit And do not get trapped at local maxima. IT IS BETTER TO TRAVEL WELL THAN TO ARRIVE Life is happening, right now. There is no past. There is no future. Don't forget that you have limitless capability to respond. RAISE THE CEILING, AND NEVER THE FLOOR And yes, most important! Ego is the enemy. It is self defeating and depression inducing. The higher you go, the harder it will get to contain your ego. Never forget, you are a mere mortal chimp with a plan and also without a tail. I love ya!","title":"Affirmations"},{"location":"Affirmation/#focus-on-faster-iteration-and-not-on-accuracy","text":"The days are long but decades are short. It's important to do things fast. A week is 2% of the year. Time is the denominator. Motion creates information. You learn more per unit time because you make contact with reality more frequently. Optimize for being spectacularly right some of the time, and low-stakes wrong a lot of the time. The goal is not to avoid mistakes; the goal is to achieve uncorrelated levels of excellence in some dimension. The downsides are worth it.","title":"FOCUS ON FASTER ITERATION AND NOT ON ACCURACY"},{"location":"Affirmation/#go-touch-the-metal","text":"Hard skills are super important. Get good with your tools. I mean, really good. Get rid of all the friction. Keep oiling your tools and don't let them rust. Don't get lost in the busy bandwagon of managerial or bureaucratic duties. Those create empty suits.","title":"GO TOUCH THE METAL"},{"location":"Affirmation/#get-good-at-small-talk","text":"Soft skills are even more important. Sapiens are emotional species. They cannot think objectively. Get good at telling stories. Get good at sales. Listen at least thrice more than you speak. Better to get your dopamine from improving your ideas than from having them validated If right things are said in a wrong way, they lose their righteousness. And yes! when deep into shit, keep your mouth shut.","title":"GET GOOD AT SMALL TALK"},{"location":"Affirmation/#inspiration-is-perishable-act-on-it","text":"Start before you are ready Thousands have the same idea, which you think is original and exclusive to you. Thinking is easy, doing is hard! Only execution matters. Ship it!","title":"INSPIRATION IS PERISHABLE. ACT ON IT."},{"location":"Affirmation/#life-isnt-unfair-it-is-disproportionately-unfair","text":"The path isn't linear. It is a Geometric Brownian motion with a drift. Make sure that the trend coefficient is always positive. There are people who are way more smarter and hardworking than you are, but are not sitting on the priviliges that you have. And vice verca. At the age of 25, it\u2019s very tempting to conflate agency and determinism in scripts. You have a default state \u201cI\u2019m a smart guy, I have these resources at hand. The future looks full of possibilities.\" And then life will hit you hard on face. Efficient Market Hypothesis is a flawed heuristic.","title":"LIFE ISN'T UNFAIR, IT IS DISPROPORTIONATELY UNFAIR."},{"location":"Affirmation/#we-are-monkeys-we-copy","text":"You will always feel an overpowering urge to fit in and follow the herd. It's natural! It is the evolutionary make-up of your brain that you inherited from your primordial ancestors. The best thing you can do is that make sure that the herd around you is smarter and more aware than you are.","title":"WE ARE MONKEYS. WE COPY."},{"location":"Affirmation/#have-self-belief-to-the-point-of-delusion","text":"You can do more than you think. Always try to know to what extent you are operating in a condition of learned helplessness in institutionalized environments. You are tied down by your own invisible orthodoxy. The laws of physics are the only limit And do not get trapped at local maxima.","title":"HAVE SELF BELIEF TO THE POINT OF DELUSION"},{"location":"Affirmation/#it-is-better-to-travel-well-than-to-arrive","text":"Life is happening, right now. There is no past. There is no future. Don't forget that you have limitless capability to respond.","title":"IT IS BETTER TO TRAVEL WELL THAN TO ARRIVE"},{"location":"Affirmation/#raise-the-ceiling-and-never-the-floor","text":"And yes, most important! Ego is the enemy. It is self defeating and depression inducing. The higher you go, the harder it will get to contain your ego. Never forget, you are a mere mortal chimp with a plan and also without a tail. I love ya!","title":"RAISE THE CEILING, AND NEVER THE FLOOR"},{"location":"Arts/Async%20Programming/","text":"Event Loop At the heart of async is something called the event loop. Think of it as a super-organized waiter in a restaurant. This waiter doesn\u2019t hover at one table until the food\u2019s eaten\u2014they zip around, checking which tables need attention. The event loop does the same: it\u2019s a constant cycle that monitors tasks, schedules them when they\u2019re ready to run, and handles their results when they\u2019re done. Why is it called a loop? while true { 5. Check for ready tasks 6. Run them 7. Check for new events (network data, timers, etc.) 8. Go back to step 1 }","title":"Async Programming"},{"location":"Arts/Async%20Programming/#event-loop","text":"At the heart of async is something called the event loop. Think of it as a super-organized waiter in a restaurant. This waiter doesn\u2019t hover at one table until the food\u2019s eaten\u2014they zip around, checking which tables need attention. The event loop does the same: it\u2019s a constant cycle that monitors tasks, schedules them when they\u2019re ready to run, and handles their results when they\u2019re done. Why is it called a loop? while true { 5. Check for ready tasks 6. Run them 7. Check for new events (network data, timers, etc.) 8. Go back to step 1 }","title":"Event Loop"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/","text":"TL;DR (First Principles): When GDB executes a program, it maps your variables\u2014including arrays and matrices\u2014into memory. You can tell GDB to dump raw memory of any variable to a binary file. This lets you analyze the data outside GDB (e.g., with Python + NumPy) without GDB choking on printing large data inline. 1. Understand What You're Dumping You need: - The starting memory address ( &my_matrix ) - The exact byte size to dump ( sizeof(my_matrix) ) - Example: for a float my_matrix[100][100] : 1 sizeof ( my_matrix ) = 100 * 100 * sizeof ( float ) = 40000 bytes 2. Basic Syntax 1 dump binary memory < filename > < start_address > < end_address > Or if you know the size: 1 dump binary memory matrix.bin &my_matrix &my_matrix + sizeof(my_matrix) But GDB doesn't support pointer arithmetic like that directly on sizeof . Instead: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000 (You can replace 40000 with any exact byte count) Or calculate in GDB: 1 gdb p sizeof(my_matrix) Then manually use the value: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000 What Happens Internally GDB interprets the address &my_matrix as a raw memory location. It writes the raw byte sequence (just 1s and 0s) to matrix.bin . Load It in Python (Post-GDB) 1 2 3 4 5 import numpy as np data = np . fromfile ( \"matrix.bin\" , dtype = np . float32 ) matrix = data . reshape (( 100 , 100 )) print ( matrix ) Make sure dtype and shape match your C++ matrix layout. \ud83e\uddea Sanity Check in GDB Want to confirm it\u2019s dumping the correct values? 1 x/10f &my_matrix[0][0] # examine 10 floats Then check if those match the first line of matrix[0][:10] in NumPy. \u26a0\ufe0f Pitfalls Don\u2019t dump pointers unless they point to contiguous memory blocks. Avoid structs with padding unless you understand layout. Beware of endian issues if moving between architectures. Summary Use dump binary memory with known size + address Analyze externally with Python Great for debugging large matrices without cluttering your GDB session Want a wrapper macro or function in GDB to simplify this?","title":"Binary Dump Using GDB"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#tldr-first-principles","text":"When GDB executes a program, it maps your variables\u2014including arrays and matrices\u2014into memory. You can tell GDB to dump raw memory of any variable to a binary file. This lets you analyze the data outside GDB (e.g., with Python + NumPy) without GDB choking on printing large data inline.","title":"TL;DR (First Principles):"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#1-understand-what-youre-dumping","text":"You need: - The starting memory address ( &my_matrix ) - The exact byte size to dump ( sizeof(my_matrix) ) - Example: for a float my_matrix[100][100] : 1 sizeof ( my_matrix ) = 100 * 100 * sizeof ( float ) = 40000 bytes","title":"1. Understand What You're Dumping"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#2-basic-syntax","text":"1 dump binary memory < filename > < start_address > < end_address > Or if you know the size: 1 dump binary memory matrix.bin &my_matrix &my_matrix + sizeof(my_matrix) But GDB doesn't support pointer arithmetic like that directly on sizeof . Instead: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000 (You can replace 40000 with any exact byte count) Or calculate in GDB: 1 gdb p sizeof(my_matrix) Then manually use the value: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000","title":"2. Basic Syntax"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#what-happens-internally","text":"GDB interprets the address &my_matrix as a raw memory location. It writes the raw byte sequence (just 1s and 0s) to matrix.bin .","title":"What Happens Internally"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#load-it-in-python-post-gdb","text":"1 2 3 4 5 import numpy as np data = np . fromfile ( \"matrix.bin\" , dtype = np . float32 ) matrix = data . reshape (( 100 , 100 )) print ( matrix ) Make sure dtype and shape match your C++ matrix layout.","title":"Load It in Python (Post-GDB)"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#sanity-check-in-gdb","text":"Want to confirm it\u2019s dumping the correct values? 1 x/10f &my_matrix[0][0] # examine 10 floats Then check if those match the first line of matrix[0][:10] in NumPy.","title":"\ud83e\uddea Sanity Check in GDB"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#pitfalls","text":"Don\u2019t dump pointers unless they point to contiguous memory blocks. Avoid structs with padding unless you understand layout. Beware of endian issues if moving between architectures.","title":"\u26a0\ufe0f Pitfalls"},{"location":"C%2B%2B/Binary%20Dump%20Using%20GDB/#summary","text":"Use dump binary memory with known size + address Analyze externally with Python Great for debugging large matrices without cluttering your GDB session Want a wrapper macro or function in GDB to simplify this?","title":"Summary"},{"location":"C%2B%2B/CMake/","text":"1. Basic Commands Set a variable 1 set ( VARIABLE_NAME value ) Minimum CMake version and project declaration 1 2 cmake_minimum_required ( VERSION 3.16 ) project ( ProjectName ) Set C/C++ standard 1 2 set ( CMAKE_C_STANDARD 11 ) set ( CMAKE_CXX_STANDARD 17 ) 2. Enabling Tests 1 enable_testing () Tells CMake that the project will include tests (e.g., with GoogleTest or CTest). 3. Organising Source and Header Files Good practice: define sources and headers in variables for clarity and reuse. 1 2 3 4 5 6 7 set ( Headers Example.hpp ) set ( Sources Example.cpp ) 4. Creating a Static Library 1 add_library ( ${ PROJECT_NAME } STATIC ${ Sources } ${ Headers } ) This creates a static library using the specified source and header files. 5. Add Subdirectory 1 add_subdirectory ( test ) This adds the test directory to the build. CMake will look for a CMakeLists.txt file inside the specified directory and execute it in the context of a sub-project (a child scope of the main project). Each subdirectory must have its own CMakeLists.txt . Add Executable The add_executable() command tells CMake to build an executable from the specified source files. It takes the name of the output binary and a list of source files. This target becomes part of the build system, and CMake automatically tracks dependencies. Example: 1 add_executable ( my_app main.cpp utils.cpp ) This creates an executable named my_app compiled from main.cpp and utils.cpp . Target Link Libraries The target_link_libraries() command links a target (e.g., an executable or library) with one or more libraries. It handles dependency resolution and ensures correct include paths and linking. Usage: 1 target_link_libraries ( my_app PRIVATE mylib ) Here, my_app is linked with mylib . The PRIVATE keyword means mylib is only needed for my_app and won\u2019t propagate to targets depending on my_app . Use PUBLIC or INTERFACE for different visibility.","title":"CMake"},{"location":"C%2B%2B/CMake/#1-basic-commands","text":"","title":"1. Basic Commands"},{"location":"C%2B%2B/CMake/#set-a-variable","text":"1 set ( VARIABLE_NAME value )","title":"Set a variable"},{"location":"C%2B%2B/CMake/#minimum-cmake-version-and-project-declaration","text":"1 2 cmake_minimum_required ( VERSION 3.16 ) project ( ProjectName )","title":"Minimum CMake version and project declaration"},{"location":"C%2B%2B/CMake/#set-cc-standard","text":"1 2 set ( CMAKE_C_STANDARD 11 ) set ( CMAKE_CXX_STANDARD 17 )","title":"Set C/C++ standard"},{"location":"C%2B%2B/CMake/#2-enabling-tests","text":"1 enable_testing () Tells CMake that the project will include tests (e.g., with GoogleTest or CTest).","title":"2. Enabling Tests"},{"location":"C%2B%2B/CMake/#3-organising-source-and-header-files","text":"Good practice: define sources and headers in variables for clarity and reuse. 1 2 3 4 5 6 7 set ( Headers Example.hpp ) set ( Sources Example.cpp )","title":"3. Organising Source and Header Files"},{"location":"C%2B%2B/CMake/#4-creating-a-static-library","text":"1 add_library ( ${ PROJECT_NAME } STATIC ${ Sources } ${ Headers } ) This creates a static library using the specified source and header files.","title":"4. Creating a Static Library"},{"location":"C%2B%2B/CMake/#5-add-subdirectory","text":"1 add_subdirectory ( test ) This adds the test directory to the build. CMake will look for a CMakeLists.txt file inside the specified directory and execute it in the context of a sub-project (a child scope of the main project). Each subdirectory must have its own CMakeLists.txt .","title":"5. Add Subdirectory"},{"location":"C%2B%2B/CMake/#add-executable","text":"The add_executable() command tells CMake to build an executable from the specified source files. It takes the name of the output binary and a list of source files. This target becomes part of the build system, and CMake automatically tracks dependencies. Example: 1 add_executable ( my_app main.cpp utils.cpp ) This creates an executable named my_app compiled from main.cpp and utils.cpp .","title":"Add Executable"},{"location":"C%2B%2B/CMake/#target-link-libraries","text":"The target_link_libraries() command links a target (e.g., an executable or library) with one or more libraries. It handles dependency resolution and ensures correct include paths and linking. Usage: 1 target_link_libraries ( my_app PRIVATE mylib ) Here, my_app is linked with mylib . The PRIVATE keyword means mylib is only needed for my_app and won\u2019t propagate to targets depending on my_app . Use PUBLIC or INTERFACE for different visibility.","title":"Target Link Libraries"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/","text":"Directory Structure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 mylib/ \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 include/ \u2502 \u2514\u2500\u2500 mylib/ \u2502 \u251c\u2500\u2500 mylib.hpp \u2502 \u2514\u2500\u2500 math_utils.hpp \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 mylib.cpp \u2502 \u2514\u2500\u2500 math_utils.cpp \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 test_mylib.cpp \u2502 \u2514\u2500\u2500 test_math_utils.cpp \u251c\u2500\u2500 examples/ \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u2514\u2500\u2500 example_usage.cpp \u251c\u2500\u2500 docs/ \u2502 \u2514\u2500\u2500 README.md \u2514\u2500\u2500 cmake/ \u2514\u2500\u2500 modules/ \u2514\u2500\u2500 FindGTest.cmake Root CMakeLists.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 cmake_minimum_required ( VERSION 3.16 ) project ( mylib VERSION 1.0.0 LANGUAGES CXX ) # Set C++ standard set ( CMAKE_CXX_STANDARD 17 ) set ( CMAKE_CXX_STANDARD_REQUIRED ON ) # Set default build type if ( NOT CMAKE_BUILD_TYPE ) set ( CMAKE_BUILD_TYPE Release ) endif () # Compiler flags set ( CMAKE_CXX_FLAGS \"-Wall -Wextra\" ) set ( CMAKE_CXX_FLAGS_DEBUG \"-g\" ) set ( CMAKE_CXX_FLAGS_RELEASE \"-O3\" ) # Add cmake modules path list ( APPEND CMAKE_MODULE_PATH ${ CMAKE_CURRENT_SOURCE_DIR } /cmake/modules ) # Library sources file ( GLOB_RECURSE SOURCES \"src/*.cpp\" ) file ( GLOB_RECURSE HEADERS \"include/*.hpp\" ) # Create library add_library ( ${ PROJECT_NAME } ${ SOURCES } ${ HEADERS } ) # Include directories target_include_directories ( ${ PROJECT_NAME } PUBLIC $< BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include > $< INSTALL_INTERFACE:include > PRIVATE ${ CMAKE_CURRENT_SOURCE_DIR } /src ) # Set target properties set_target_properties ( ${ PROJECT_NAME } PROPERTIES VERSION ${ PROJECT_VERSION } SOVERSION 1 ) # Enable testing enable_testing () # Add subdirectories add_subdirectory ( tests ) add_subdirectory ( examples ) # Installation install ( TARGETS ${ PROJECT_NAME } EXPORT ${ PROJECT_NAME } Targets LIBRARY DESTINATION lib ARCHIVE DESTINATION lib RUNTIME DESTINATION bin INCLUDES DESTINATION include ) install ( DIRECTORY include/ DESTINATION include FILES_MATCHING PATTERN \"*.hpp\" ) # Export targets install ( EXPORT ${ PROJECT_NAME } Targets FILE ${ PROJECT_NAME } Targets.cmake NAMESPACE ${ PROJECT_NAME } :: DESTINATION lib/cmake/ ${ PROJECT_NAME } ) # Create config file include ( CMakePackageConfigHelpers ) write_basic_package_version_file ( \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\" VERSION ${ PROJECT_VERSION } COMPATIBILITY AnyNewerVersion ) configure_package_config_file ( \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/${PROJECT_NAME}Config.cmake.in\" \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\" INSTALL_DESTINATION lib/cmake/ ${ PROJECT_NAME } ) install ( FILES \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\" \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\" DESTINATION lib/cmake/ ${ PROJECT_NAME } ) tests/CMakeLists.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # Find Google Test find_package ( GTest REQUIRED ) # Test executable file ( GLOB TEST_SOURCES \"*.cpp\" ) add_executable ( ${ PROJECT_NAME } _tests ${ TEST_SOURCES } ) # Link libraries target_link_libraries ( ${ PROJECT_NAME } _tests PRIVATE ${ PROJECT_NAME } GTest::GTest GTest::Main pthread ) # Include directories target_include_directories ( ${ PROJECT_NAME } _tests PRIVATE ${ CMAKE_CURRENT_SOURCE_DIR } ${ CMAKE_SOURCE_DIR } /include ) # Discover tests include ( GoogleTest ) gtest_discover_tests ( ${ PROJECT_NAME } _tests ) # Add custom test target add_custom_target ( run_tests COMMAND ${ CMAKE_CTEST_COMMAND } --verbose DEPENDS ${ PROJECT_NAME } _tests ) Header Files include/mylib/mylib.hpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #pragma once #include <string> namespace mylib { class MyLib { public : MyLib (); ~ MyLib () = default ; std :: string getVersion () const ; bool initialize (); void cleanup (); private : bool m_initialized ; std :: string m_version ; }; } // namespace mylib include/mylib/math_utils.hpp 1 2 3 4 5 6 7 8 9 10 11 12 #pragma once namespace mylib { namespace math { double add ( double a , double b ); double multiply ( double a , double b ); double divide ( double a , double b ); bool isPrime ( int n ); } // namespace math } // namespace mylib Source Files src/mylib.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include \"mylib/mylib.hpp\" namespace mylib { MyLib :: MyLib () : m_initialized ( false ), m_version ( \"1.0.0\" ) {} std :: string MyLib :: getVersion () const { return m_version ; } bool MyLib :: initialize () { if ( ! m_initialized ) { // Initialization logic here m_initialized = true ; } return m_initialized ; } void MyLib :: cleanup () { if ( m_initialized ) { // Cleanup logic here m_initialized = false ; } } } // namespace mylib src/math_utils.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \"mylib/math_utils.hpp\" #include <cmath> #include <stdexcept> namespace mylib { namespace math { double add ( double a , double b ) { return a + b ; } double multiply ( double a , double b ) { return a * b ; } double divide ( double a , double b ) { if ( std :: abs ( b ) < 1e-9 ) { throw std :: invalid_argument ( \"Division by zero\" ); } return a / b ; } bool isPrime ( int n ) { if ( n <= 1 ) return false ; if ( n <= 3 ) return true ; if ( n % 2 == 0 || n % 3 == 0 ) return false ; for ( int i = 5 ; i * i <= n ; i += 6 ) { if ( n % i == 0 || n % ( i + 2 ) == 0 ) { return false ; } } return true ; } } // namespace math } // namespace mylib Test Files tests/test_mylib.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include <gtest/gtest.h> #include \"mylib/mylib.hpp\" class MyLibTest : public :: testing :: Test { protected : void SetUp () override { lib = std :: make_unique < mylib :: MyLib > (); } void TearDown () override { lib . reset (); } std :: unique_ptr < mylib :: MyLib > lib ; }; TEST_F ( MyLibTest , GetVersion ) { EXPECT_EQ ( lib -> getVersion (), \"1.0.0\" ); } TEST_F ( MyLibTest , Initialize ) { EXPECT_TRUE ( lib -> initialize ()); } TEST_F ( MyLibTest , InitializeTwice ) { EXPECT_TRUE ( lib -> initialize ()); EXPECT_TRUE ( lib -> initialize ()); // Should still return true } tests/test_math_utils.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #include <gtest/gtest.h> #include \"mylib/math_utils.hpp\" #include <stdexcept> using namespace mylib :: math ; TEST ( MathUtilsTest , Add ) { EXPECT_DOUBLE_EQ ( add ( 2.0 , 3.0 ), 5.0 ); EXPECT_DOUBLE_EQ ( add ( -1.0 , 1.0 ), 0.0 ); EXPECT_DOUBLE_EQ ( add ( 0.0 , 0.0 ), 0.0 ); } TEST ( MathUtilsTest , Multiply ) { EXPECT_DOUBLE_EQ ( multiply ( 2.0 , 3.0 ), 6.0 ); EXPECT_DOUBLE_EQ ( multiply ( -2.0 , 3.0 ), -6.0 ); EXPECT_DOUBLE_EQ ( multiply ( 0.0 , 5.0 ), 0.0 ); } TEST ( MathUtilsTest , Divide ) { EXPECT_DOUBLE_EQ ( divide ( 6.0 , 2.0 ), 3.0 ); EXPECT_DOUBLE_EQ ( divide ( -6.0 , 2.0 ), -3.0 ); EXPECT_THROW ( divide ( 5.0 , 0.0 ), std :: invalid_argument ); } TEST ( MathUtilsTest , IsPrime ) { EXPECT_FALSE ( isPrime ( 1 )); EXPECT_TRUE ( isPrime ( 2 )); EXPECT_TRUE ( isPrime ( 3 )); EXPECT_FALSE ( isPrime ( 4 )); EXPECT_TRUE ( isPrime ( 5 )); EXPECT_FALSE ( isPrime ( 9 )); EXPECT_TRUE ( isPrime ( 17 )); EXPECT_FALSE ( isPrime ( 25 )); } Example Usage examples/CMakeLists.txt 1 2 3 add_executable ( example_usage example_usage.cpp ) target_link_libraries ( example_usage PRIVATE ${ PROJECT_NAME } ) target_include_directories ( example_usage PRIVATE ${ CMAKE_SOURCE_DIR } /include ) examples/example_usage.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include <iostream> #include \"mylib/mylib.hpp\" #include \"mylib/math_utils.hpp\" int main () { mylib :: MyLib lib ; std :: cout << \"Library version: \" << lib . getVersion () << std :: endl ; if ( lib . initialize ()) { std :: cout << \"Library initialized successfully!\" << std :: endl ; // Math operations std :: cout << \"2 + 3 = \" << mylib :: math :: add ( 2.0 , 3.0 ) << std :: endl ; std :: cout << \"4 * 5 = \" << mylib :: math :: multiply ( 4.0 , 5.0 ) << std :: endl ; std :: cout << \"10 / 2 = \" << mylib :: math :: divide ( 10.0 , 2.0 ) << std :: endl ; std :: cout << \"Is 17 prime? \" << ( mylib :: math :: isPrime ( 17 ) ? \"Yes\" : \"No\" ) << std :: endl ; lib . cleanup (); } return 0 ; } .gitignore 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # Build directories build / cmake - build -*/ # IDE files . vscode / . idea / * . swp * . swo *~ # Compiled Object files * . o * . obj # Compiled Dynamic libraries * . so * . dylib * . dll # Compiled Static libraries * . a * . lib # Executables * . exe * . out * . app # CMake CMakeCache . txt CMakeFiles / cmake_install . cmake Makefile * . cmake !CMakeLists.txt !cmake/ # Testing Testing / CTestTestfile . cmake # Package files * . deb * . rpm * . tar . gz * . zip Build Instructions Using Ninja (Recommended) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Create build directory mkdir build && cd build # Configure with Ninja generator cmake -G Ninja .. # Build the project ninja # Run tests ninja test # or ctest --verbose # Run specific test ./tests/mylib_tests # Build and run example ninja example_usage ./examples/example_usage Using Make (Alternative) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Create build directory mkdir build && cd build # Configure with Make generator cmake .. # Build the project make -j $( nproc ) # Run tests make test # Build and run example make example_usage ./examples/example_usage Installing Google Test (Ubuntu/Debian) 1 2 3 4 5 6 7 8 sudo apt update sudo apt install libgtest-dev cmake ninja-build # If libgtest-dev doesn't provide compiled libraries: cd /usr/src/gtest sudo cmake CMakeLists.txt sudo make sudo cp lib/*.a /usr/lib Installing Google Test (macOS) 1 brew install googletest cmake ninja This structure provides a solid foundation for a C++ library with modern CMake practices, comprehensive testing with Google Test, and support for both Ninja and Make build systems.","title":"CPP Project Structure with Tests"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#directory-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 mylib/ \u251c\u2500\u2500 CMakeLists.txt \u251c\u2500\u2500 README.md \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 include/ \u2502 \u2514\u2500\u2500 mylib/ \u2502 \u251c\u2500\u2500 mylib.hpp \u2502 \u2514\u2500\u2500 math_utils.hpp \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 mylib.cpp \u2502 \u2514\u2500\u2500 math_utils.cpp \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u251c\u2500\u2500 test_mylib.cpp \u2502 \u2514\u2500\u2500 test_math_utils.cpp \u251c\u2500\u2500 examples/ \u2502 \u251c\u2500\u2500 CMakeLists.txt \u2502 \u2514\u2500\u2500 example_usage.cpp \u251c\u2500\u2500 docs/ \u2502 \u2514\u2500\u2500 README.md \u2514\u2500\u2500 cmake/ \u2514\u2500\u2500 modules/ \u2514\u2500\u2500 FindGTest.cmake","title":"Directory Structure"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#root-cmakeliststxt","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 cmake_minimum_required ( VERSION 3.16 ) project ( mylib VERSION 1.0.0 LANGUAGES CXX ) # Set C++ standard set ( CMAKE_CXX_STANDARD 17 ) set ( CMAKE_CXX_STANDARD_REQUIRED ON ) # Set default build type if ( NOT CMAKE_BUILD_TYPE ) set ( CMAKE_BUILD_TYPE Release ) endif () # Compiler flags set ( CMAKE_CXX_FLAGS \"-Wall -Wextra\" ) set ( CMAKE_CXX_FLAGS_DEBUG \"-g\" ) set ( CMAKE_CXX_FLAGS_RELEASE \"-O3\" ) # Add cmake modules path list ( APPEND CMAKE_MODULE_PATH ${ CMAKE_CURRENT_SOURCE_DIR } /cmake/modules ) # Library sources file ( GLOB_RECURSE SOURCES \"src/*.cpp\" ) file ( GLOB_RECURSE HEADERS \"include/*.hpp\" ) # Create library add_library ( ${ PROJECT_NAME } ${ SOURCES } ${ HEADERS } ) # Include directories target_include_directories ( ${ PROJECT_NAME } PUBLIC $< BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include > $< INSTALL_INTERFACE:include > PRIVATE ${ CMAKE_CURRENT_SOURCE_DIR } /src ) # Set target properties set_target_properties ( ${ PROJECT_NAME } PROPERTIES VERSION ${ PROJECT_VERSION } SOVERSION 1 ) # Enable testing enable_testing () # Add subdirectories add_subdirectory ( tests ) add_subdirectory ( examples ) # Installation install ( TARGETS ${ PROJECT_NAME } EXPORT ${ PROJECT_NAME } Targets LIBRARY DESTINATION lib ARCHIVE DESTINATION lib RUNTIME DESTINATION bin INCLUDES DESTINATION include ) install ( DIRECTORY include/ DESTINATION include FILES_MATCHING PATTERN \"*.hpp\" ) # Export targets install ( EXPORT ${ PROJECT_NAME } Targets FILE ${ PROJECT_NAME } Targets.cmake NAMESPACE ${ PROJECT_NAME } :: DESTINATION lib/cmake/ ${ PROJECT_NAME } ) # Create config file include ( CMakePackageConfigHelpers ) write_basic_package_version_file ( \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\" VERSION ${ PROJECT_VERSION } COMPATIBILITY AnyNewerVersion ) configure_package_config_file ( \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/${PROJECT_NAME}Config.cmake.in\" \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\" INSTALL_DESTINATION lib/cmake/ ${ PROJECT_NAME } ) install ( FILES \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\" \"${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\" DESTINATION lib/cmake/ ${ PROJECT_NAME } )","title":"Root CMakeLists.txt"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#testscmakeliststxt","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # Find Google Test find_package ( GTest REQUIRED ) # Test executable file ( GLOB TEST_SOURCES \"*.cpp\" ) add_executable ( ${ PROJECT_NAME } _tests ${ TEST_SOURCES } ) # Link libraries target_link_libraries ( ${ PROJECT_NAME } _tests PRIVATE ${ PROJECT_NAME } GTest::GTest GTest::Main pthread ) # Include directories target_include_directories ( ${ PROJECT_NAME } _tests PRIVATE ${ CMAKE_CURRENT_SOURCE_DIR } ${ CMAKE_SOURCE_DIR } /include ) # Discover tests include ( GoogleTest ) gtest_discover_tests ( ${ PROJECT_NAME } _tests ) # Add custom test target add_custom_target ( run_tests COMMAND ${ CMAKE_CTEST_COMMAND } --verbose DEPENDS ${ PROJECT_NAME } _tests )","title":"tests/CMakeLists.txt"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#header-files","text":"","title":"Header Files"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#includemylibmylibhpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #pragma once #include <string> namespace mylib { class MyLib { public : MyLib (); ~ MyLib () = default ; std :: string getVersion () const ; bool initialize (); void cleanup (); private : bool m_initialized ; std :: string m_version ; }; } // namespace mylib","title":"include/mylib/mylib.hpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#includemylibmath_utilshpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 #pragma once namespace mylib { namespace math { double add ( double a , double b ); double multiply ( double a , double b ); double divide ( double a , double b ); bool isPrime ( int n ); } // namespace math } // namespace mylib","title":"include/mylib/math_utils.hpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#source-files","text":"","title":"Source Files"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#srcmylibcpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include \"mylib/mylib.hpp\" namespace mylib { MyLib :: MyLib () : m_initialized ( false ), m_version ( \"1.0.0\" ) {} std :: string MyLib :: getVersion () const { return m_version ; } bool MyLib :: initialize () { if ( ! m_initialized ) { // Initialization logic here m_initialized = true ; } return m_initialized ; } void MyLib :: cleanup () { if ( m_initialized ) { // Cleanup logic here m_initialized = false ; } } } // namespace mylib","title":"src/mylib.cpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#srcmath_utilscpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \"mylib/math_utils.hpp\" #include <cmath> #include <stdexcept> namespace mylib { namespace math { double add ( double a , double b ) { return a + b ; } double multiply ( double a , double b ) { return a * b ; } double divide ( double a , double b ) { if ( std :: abs ( b ) < 1e-9 ) { throw std :: invalid_argument ( \"Division by zero\" ); } return a / b ; } bool isPrime ( int n ) { if ( n <= 1 ) return false ; if ( n <= 3 ) return true ; if ( n % 2 == 0 || n % 3 == 0 ) return false ; for ( int i = 5 ; i * i <= n ; i += 6 ) { if ( n % i == 0 || n % ( i + 2 ) == 0 ) { return false ; } } return true ; } } // namespace math } // namespace mylib","title":"src/math_utils.cpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#test-files","text":"","title":"Test Files"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#teststest_mylibcpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include <gtest/gtest.h> #include \"mylib/mylib.hpp\" class MyLibTest : public :: testing :: Test { protected : void SetUp () override { lib = std :: make_unique < mylib :: MyLib > (); } void TearDown () override { lib . reset (); } std :: unique_ptr < mylib :: MyLib > lib ; }; TEST_F ( MyLibTest , GetVersion ) { EXPECT_EQ ( lib -> getVersion (), \"1.0.0\" ); } TEST_F ( MyLibTest , Initialize ) { EXPECT_TRUE ( lib -> initialize ()); } TEST_F ( MyLibTest , InitializeTwice ) { EXPECT_TRUE ( lib -> initialize ()); EXPECT_TRUE ( lib -> initialize ()); // Should still return true }","title":"tests/test_mylib.cpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#teststest_math_utilscpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #include <gtest/gtest.h> #include \"mylib/math_utils.hpp\" #include <stdexcept> using namespace mylib :: math ; TEST ( MathUtilsTest , Add ) { EXPECT_DOUBLE_EQ ( add ( 2.0 , 3.0 ), 5.0 ); EXPECT_DOUBLE_EQ ( add ( -1.0 , 1.0 ), 0.0 ); EXPECT_DOUBLE_EQ ( add ( 0.0 , 0.0 ), 0.0 ); } TEST ( MathUtilsTest , Multiply ) { EXPECT_DOUBLE_EQ ( multiply ( 2.0 , 3.0 ), 6.0 ); EXPECT_DOUBLE_EQ ( multiply ( -2.0 , 3.0 ), -6.0 ); EXPECT_DOUBLE_EQ ( multiply ( 0.0 , 5.0 ), 0.0 ); } TEST ( MathUtilsTest , Divide ) { EXPECT_DOUBLE_EQ ( divide ( 6.0 , 2.0 ), 3.0 ); EXPECT_DOUBLE_EQ ( divide ( -6.0 , 2.0 ), -3.0 ); EXPECT_THROW ( divide ( 5.0 , 0.0 ), std :: invalid_argument ); } TEST ( MathUtilsTest , IsPrime ) { EXPECT_FALSE ( isPrime ( 1 )); EXPECT_TRUE ( isPrime ( 2 )); EXPECT_TRUE ( isPrime ( 3 )); EXPECT_FALSE ( isPrime ( 4 )); EXPECT_TRUE ( isPrime ( 5 )); EXPECT_FALSE ( isPrime ( 9 )); EXPECT_TRUE ( isPrime ( 17 )); EXPECT_FALSE ( isPrime ( 25 )); }","title":"tests/test_math_utils.cpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#example-usage","text":"","title":"Example Usage"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#examplescmakeliststxt","text":"1 2 3 add_executable ( example_usage example_usage.cpp ) target_link_libraries ( example_usage PRIVATE ${ PROJECT_NAME } ) target_include_directories ( example_usage PRIVATE ${ CMAKE_SOURCE_DIR } /include )","title":"examples/CMakeLists.txt"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#examplesexample_usagecpp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include <iostream> #include \"mylib/mylib.hpp\" #include \"mylib/math_utils.hpp\" int main () { mylib :: MyLib lib ; std :: cout << \"Library version: \" << lib . getVersion () << std :: endl ; if ( lib . initialize ()) { std :: cout << \"Library initialized successfully!\" << std :: endl ; // Math operations std :: cout << \"2 + 3 = \" << mylib :: math :: add ( 2.0 , 3.0 ) << std :: endl ; std :: cout << \"4 * 5 = \" << mylib :: math :: multiply ( 4.0 , 5.0 ) << std :: endl ; std :: cout << \"10 / 2 = \" << mylib :: math :: divide ( 10.0 , 2.0 ) << std :: endl ; std :: cout << \"Is 17 prime? \" << ( mylib :: math :: isPrime ( 17 ) ? \"Yes\" : \"No\" ) << std :: endl ; lib . cleanup (); } return 0 ; }","title":"examples/example_usage.cpp"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#gitignore","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # Build directories build / cmake - build -*/ # IDE files . vscode / . idea / * . swp * . swo *~ # Compiled Object files * . o * . obj # Compiled Dynamic libraries * . so * . dylib * . dll # Compiled Static libraries * . a * . lib # Executables * . exe * . out * . app # CMake CMakeCache . txt CMakeFiles / cmake_install . cmake Makefile * . cmake !CMakeLists.txt !cmake/ # Testing Testing / CTestTestfile . cmake # Package files * . deb * . rpm * . tar . gz * . zip","title":".gitignore"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#build-instructions","text":"","title":"Build Instructions"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#using-ninja-recommended","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Create build directory mkdir build && cd build # Configure with Ninja generator cmake -G Ninja .. # Build the project ninja # Run tests ninja test # or ctest --verbose # Run specific test ./tests/mylib_tests # Build and run example ninja example_usage ./examples/example_usage","title":"Using Ninja (Recommended)"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#using-make-alternative","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Create build directory mkdir build && cd build # Configure with Make generator cmake .. # Build the project make -j $( nproc ) # Run tests make test # Build and run example make example_usage ./examples/example_usage","title":"Using Make (Alternative)"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#installing-google-test-ubuntudebian","text":"1 2 3 4 5 6 7 8 sudo apt update sudo apt install libgtest-dev cmake ninja-build # If libgtest-dev doesn't provide compiled libraries: cd /usr/src/gtest sudo cmake CMakeLists.txt sudo make sudo cp lib/*.a /usr/lib","title":"Installing Google Test (Ubuntu/Debian)"},{"location":"C%2B%2B/CPP%20Project%20Structure%20with%20Tests/#installing-google-test-macos","text":"1 brew install googletest cmake ninja This structure provides a solid foundation for a C++ library with modern CMake practices, comprehensive testing with Google Test, and support for both Ninja and Make build systems.","title":"Installing Google Test (macOS)"},{"location":"C%2B%2B/CUDA/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #include <iostream> // CUDA kernel: runs on GPU __global__ void multiply_scalar ( float * vec , float scalar , int n ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < n ) { vec [ idx ] *= scalar ; } } int main () { const int N = 8 ; float h_vec [ N ] = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 }; // Host vector float * d_vec ; cudaMalloc ( & d_vec , N * sizeof ( float )); // Allocate GPU memory cudaMemcpy ( d_vec , h_vec , N * sizeof ( float ), cudaMemcpyHostToDevice ); // Copy to GPU int blockSize = 4 ; int numBlocks = ( N + blockSize - 1 ) / blockSize ; multiply_scalar <<< numBlocks , blockSize >>> ( d_vec , 2.0f , N ); // Launch kernel cudaMemcpy ( h_vec , d_vec , N * sizeof ( float ), cudaMemcpyDeviceToHost ); // Copy back to CPU cudaFree ( d_vec ); // Free GPU memory // Print result for ( int i = 0 ; i < N ; ++ i ) { std :: cout << h_vec [ i ] << \" \" ; } std :: cout << std :: endl ; return 0 ; } In CUDA's mental model: CPU (host) and GPU (device) are separate environments*. It\u2019s called __global__ because the function becomes globally visible ** across two different \"worlds\". A block is a group of threads that run together on the GPU. Suppose there are 3 blocks and you have 3 threads per block. To access the index of the element that 33rd thread should work upon is: ` 1 int idx = threadIdx . x + blockDim . x * blockIdx . x ; .x picks the first (horizontal) dimension of a CUDA thread or block. That's why you see threadIdx.x , blockIdx.x , blockDim.x . In CUDA, many things (like threadIdx , blockIdx , blockDim ) are 3D structures \u2014 they have .x , .y , and .z . - .x \u2192 along the width - .y \u2192 along the height - .z \u2192 along the depth `` \"Allocate Create, Spawn, Collect, Free**\" Allocate memory -> Copy to GPU -> Spawn kernel with block size and number of blocks parameterized -> Copy the result back to CPU -> Free the memory","title":"CUDA"},{"location":"C%2B%2B/GDB/","text":"Logging 1 2 3 4 5 set logging on set logging file gdb_output.txt set logging overwrite on # optional, overwrites existing file set logging redirect on # No echoing on terminal set logging off Breaking Execution Function Call 1 break function_name 1 break file.c:function_name 1 break 'Class::Method(Type)' 1 2 set breakpoint pending on break function_name Specific Line 1 break file . c : 42 1 break 42 When Symbol Comes into Scope 1 rbreak symbol_name When Symbol Changes Value 1 2 3 watch variable # on write rwatch variable # on read awatch variable # on read/write Requires variable to have a stable memory address (e.g., global or in-scope local). I/O Set Pretty Printing 1 set print pretty on 1 set print elements N Limits number of elements printed (default: 200). Dump Binary Memory 1 dump binary memory filename start_addr end_addr Example: 1 dump binary memory dump.bin 0x600000 0x601000 Note Use pointer arithmetic to calculate start and end addresses A useful use-case is that you can dump containers to read them back into python 1 2 3 set $start = &v[0] set $end = $start + v.size() dump binary memory <FILE_NAME>.bin $start $end","title":"GDB"},{"location":"C%2B%2B/GDB/#logging","text":"1 2 3 4 5 set logging on set logging file gdb_output.txt set logging overwrite on # optional, overwrites existing file set logging redirect on # No echoing on terminal set logging off","title":"Logging"},{"location":"C%2B%2B/GDB/#breaking-execution","text":"","title":"Breaking Execution"},{"location":"C%2B%2B/GDB/#function-call","text":"1 break function_name 1 break file.c:function_name 1 break 'Class::Method(Type)' 1 2 set breakpoint pending on break function_name","title":"Function Call"},{"location":"C%2B%2B/GDB/#specific-line","text":"1 break file . c : 42 1 break 42","title":"Specific Line"},{"location":"C%2B%2B/GDB/#when-symbol-comes-into-scope","text":"1 rbreak symbol_name","title":"When Symbol Comes into Scope"},{"location":"C%2B%2B/GDB/#when-symbol-changes-value","text":"1 2 3 watch variable # on write rwatch variable # on read awatch variable # on read/write Requires variable to have a stable memory address (e.g., global or in-scope local).","title":"When Symbol Changes Value"},{"location":"C%2B%2B/GDB/#io","text":"","title":"I/O"},{"location":"C%2B%2B/GDB/#set-pretty-printing","text":"1 set print pretty on 1 set print elements N Limits number of elements printed (default: 200).","title":"Set Pretty Printing"},{"location":"C%2B%2B/GDB/#dump-binary-memory","text":"1 dump binary memory filename start_addr end_addr Example: 1 dump binary memory dump.bin 0x600000 0x601000 Note Use pointer arithmetic to calculate start and end addresses A useful use-case is that you can dump containers to read them back into python 1 2 3 set $start = &v[0] set $end = $start + v.size() dump binary memory <FILE_NAME>.bin $start $end","title":"Dump Binary Memory"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/","text":"1. Node Structure Raw Pointer Version 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 template < typename T > struct Node { T data ; Node * next ; Node ( T value ) : data ( value ), next ( nullptr ) {} }; ```` ### Smart Pointer Version (`std::unique_ptr`) ``` cpp #include <memory> template < typename T > struct Node { T data ; std :: unique_ptr < Node < T >> next ; Node ( T value ) : data ( value ), next ( nullptr ) {} }; 2. Pointer Access -> is used to access members through a pointer. . is used to access members of an object. Example: cpp Node<int>* p = new Node<int>(10); p->data = 20; // modifies value via pointer 3. Why nullptr ? Safer than NULL or 0 . Type-safe and prevents uninitialised access. 4. std::unique_ptr Benefits Manages memory automatically through reference counting. Enforces single ownership. Prevents memory leaks. Use .get() to obtain raw pointer without transferring ownership. 5. Creating a Linked List with 5 Elements 1 2 3 4 5 6 7 auto head = std :: make_unique < Node < int >> ( 1 ); Node < int >* current = head . get (); for ( int i = 2 ; i <= 5 ; ++ i ) { current -> next = std :: make_unique < Node < int >> ( i ); current = current -> next . get (); } head owns the list. current is a non-owning raw pointer used to traverse and build the list. 6. .get() Explained Returns raw pointer from unique_ptr without giving up ownership. Use it when you need temporary access: cpp current = current->next.get(); Final List Constructed list: 1 -> 2 -> 3 -> 4 -> 5 (automatically cleaned up when head goes out of scope).","title":"Linked List in C++ with Smart Pointers"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#1-node-structure","text":"","title":"1. Node Structure"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#raw-pointer-version","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 template < typename T > struct Node { T data ; Node * next ; Node ( T value ) : data ( value ), next ( nullptr ) {} }; ```` ### Smart Pointer Version (`std::unique_ptr`) ``` cpp #include <memory> template < typename T > struct Node { T data ; std :: unique_ptr < Node < T >> next ; Node ( T value ) : data ( value ), next ( nullptr ) {} };","title":"Raw Pointer Version"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#2-pointer-access","text":"-> is used to access members through a pointer. . is used to access members of an object. Example: cpp Node<int>* p = new Node<int>(10); p->data = 20; // modifies value via pointer","title":"2. Pointer Access"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#3-why-nullptr","text":"Safer than NULL or 0 . Type-safe and prevents uninitialised access.","title":"3. Why nullptr?"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#4-stdunique_ptr-benefits","text":"Manages memory automatically through reference counting. Enforces single ownership. Prevents memory leaks. Use .get() to obtain raw pointer without transferring ownership.","title":"4. std::unique_ptr Benefits"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#5-creating-a-linked-list-with-5-elements","text":"1 2 3 4 5 6 7 auto head = std :: make_unique < Node < int >> ( 1 ); Node < int >* current = head . get (); for ( int i = 2 ; i <= 5 ; ++ i ) { current -> next = std :: make_unique < Node < int >> ( i ); current = current -> next . get (); } head owns the list. current is a non-owning raw pointer used to traverse and build the list.","title":"5. Creating a Linked List with 5 Elements"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#6-get-explained","text":"Returns raw pointer from unique_ptr without giving up ownership. Use it when you need temporary access: cpp current = current->next.get();","title":"6. .get() Explained"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#final-list","text":"Constructed list: 1 -> 2 -> 3 -> 4 -> 5 (automatically cleaned up when head goes out of scope).","title":"Final List"},{"location":"C%2B%2B/Read%20CSV%20in%20C%2B%2B/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include <iostream> #include <fstream> #include <string> #include <vector> #include <sstream> using namespace std ; vector < vector < string >> readCSV ( const string & filename ) { vector < vector < string >> data ; ifstream file ( filename ); if ( ! file . is_open ()) { return data ; // Return empty if file can't be opened } string line ; while ( getline ( file , line )) { vector < string > row ; stringstream ss ( line ); string cell ; while ( getline ( ss , cell , ',' )) { row . push_back ( cell ); } data . push_back ( row ); } return data ; } What is fstream and iostream ? Data plumbing system! C++ streams create channels for information to flow between your program and the outside world (screen, files, memory). iostream defines the basic plumbing system, while fstream adds special connectors specifically for working with files. ifstream file(filename) : The constructor calls operating system functions to locate the file, check permissions, and obtain a file handle (a reference number the OS uses to track open files). Once opened, the stream object configures internal buffers to efficiently read data from disk when requested. What is sstream ? cio`","title":"Read CSV in C++"},{"location":"C%2B%2B/Read%20CSV%20in%20C%2B%2B/#what-is-fstream-and-iostream","text":"Data plumbing system! C++ streams create channels for information to flow between your program and the outside world (screen, files, memory). iostream defines the basic plumbing system, while fstream adds special connectors specifically for working with files. ifstream file(filename) : The constructor calls operating system functions to locate the file, check permissions, and obtain a file handle (a reference number the OS uses to track open files). Once opened, the stream object configures internal buffers to efficiently read data from disk when requested.","title":"What is fstream and iostream?"},{"location":"C%2B%2B/Read%20CSV%20in%20C%2B%2B/#what-is-sstream","text":"cio`","title":"What is sstream?"},{"location":"C%2B%2B/Reading%20Args/","text":"What is argc and argv? argc : number of command-line arguments. argv : array of C-strings holding those arguments. argv[0] is the program name (the executable's name used to start the program). argv[1] to argv[argc-1] are the actual command-line arguments. Example: running ./myapp arg1 arg2 sets argv[0]=\"./myapp\" , argv[1]=\"arg1\" , argv[2]=\"arg2\" . 1 2 3 int main ( int argc , char * argv []) { } char* is a pointer to a character or to the first character in a C-string (null-terminated array of chars).","title":"Reading Args"},{"location":"C%2B%2B/Reading%20Args/#what-is-argc-and-argv","text":"argc : number of command-line arguments. argv : array of C-strings holding those arguments. argv[0] is the program name (the executable's name used to start the program). argv[1] to argv[argc-1] are the actual command-line arguments. Example: running ./myapp arg1 arg2 sets argv[0]=\"./myapp\" , argv[1]=\"arg1\" , argv[2]=\"arg2\" . 1 2 3 int main ( int argc , char * argv []) { } char* is a pointer to a character or to the first character in a C-string (null-terminated array of chars).","title":"What is argc and argv?"},{"location":"C%2B%2B/Shared%20vs%20Static%20libraries/","text":"Shared Libraries : - Definition : Dynamically linked libraries ( .so on Unix, .dll on Windows) loaded at runtime. - Size : Smaller executable size; library code is not embedded in the binary. - Linking : Linked at runtime via dynamic linker; requires library presence on the system. - Updates : Can be updated independently without recompiling the executable. - Performance : Slight runtime overhead due to dynamic linking. - Distribution : Must be distributed with the executable or installed on the target system. - Memory : Shared across multiple processes, reducing memory usage. - Example : libc.so on Linux. Static Libraries : - Definition : Archives ( .a on Unix, .lib on Windows) embedded into the executable at compile time. - Size : Larger executable size; library code is included in the binary. - Linking : Linked at compile time; no external dependencies at runtime. - Updates : Requires recompilation of the executable to incorporate library updates. - Performance : Faster at runtime; no dynamic linking overhead. - Distribution : Self-contained executable; no need to distribute the library separately. - Memory : Each executable has its own copy, increasing memory usage. - Example : libc.a on Linux. Key Differences : - Linking Time : Static at compile time; shared at runtime. - Executable Size : Static produces larger binaries; shared keeps them smaller. - Portability : Static is more portable (no runtime dependencies); shared requires compatible libraries on the system. - Maintenance : Shared allows easier library updates; static requires recompilation. - Use Case : Use static for standalone apps or specific versioning; use shared for reduced size and shared resources.","title":"Shared vs Static libraries"},{"location":"C%2B%2B/The%20Art%20of%20Higher%20Order%20Functions/","text":"Functional programming is awesome. The concept of a function is so pure. You have this elegant thing that takes an input and gives you an output, and no matter what, the same input always gives you the same output. The key thing hat ensures that it gives you the same answer always for infinity is that there's no state and hence no side-effects. Higher-order functions are beautiful. They take the idea of a function\u2014this clean little input-output machine\u2014and elevate it. Instead of just working with numbers or strings, now functions can take in other functions. They can return functions. They can build behaviour out of behaviour. And suddenly, your functions are composable, modular, and expressive and still the logic and the essence of a function remains pure, untouched. Let's create one that measures execution time. We will write this simple wrapper that takes any arbitrary function as input and gives you an enhanced function as output. The key thing that makes this approach so clean is that we're not touching the original function's logic or introducing side-effects. Suppose you have a function which generates a sequence of n integers starting from start , increasing by step . It returns the result as a pre-allocated std::vector<int> for performance. 1 2 3 4 5 6 7 std :: vector < int > generate_sequence ( int start , int n , int step ) { std :: vector < int > result ; result . reserve ( n ); for ( int i = 0 ; i < n ; ++ i ) result . push_back ( start + i * step ); return result ; } Now here come the fun part: 1 2 3 4 5 6 7 8 9 10 11 template < typename Func > auto timed ( Func f ) { return [ f ]( auto && ... args ) { auto start = std :: chrono :: high_resolution_clock :: now (); auto result = f ( std :: forward < decltype ( args ) > ( args )...); auto end = std :: chrono :: high_resolution_clock :: now (); std :: chrono :: duration < double > elapsed = end - start ; std :: cout << \"Elapsed time: \" << elapsed . count () << \" seconds \\n \" ; return result ; }; } Let\u2019s unpack what\u2019s happening inside. auto&&... args is what makes the wrapper flexible. It says: give me any number of inputs, of any type\u2014whether they\u2019re temporary values flying in, or persistent references you want to preserve. decltype(args) captures the true nature of each argument. It doesn\u2019t just look at the value\u2014it remembers how it was passed in: was it moveable? Was it a reference? And then comes the real hero: std::forward<decltype(args)>(args)... . This is a bit tricky! It makes sure each argument is forwarded exactly as it arrived\u2014if it was an rvalue, it stays one; if it was an lvalue, we respect that. No surprises. If like me, you also find lvalue and rvalue intimidating to comprehend, here's a simple example to distinguish lvalues and rvalues: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int x = 5 ; // 'x' is an lvalue - it has a name and storage location int y = x + 1 ; // 'x' is an lvalue, but 'x + 1' is an rvalue (temporary result) int & ref = x ; // Works: can bind lvalue reference to lvalue // int& ref2 = x + 1; // Error: can't bind lvalue reference to rvalue int && rref = x + 1 ; // Works: can bind rvalue reference to rvalue // int&& rref2 = x; // Error: can't bind rvalue reference to lvalue // Function that takes rvalue reference void process ( int && val ) { // Can modify val since we know it's a temporary } process ( 10 ); // Works: 10 is an rvalue // process(x); // Error: x is an lvalue // This works - std::move converts lvalue to rvalue process ( std :: move ( x )); 1 2 3 4 int main () { auto timed_gen = timed ( generate_sequence ); auto result = timed_gen ( 0 , 10 , 2 ); }","title":"The Art of Higher Order Functions"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/","text":"Parallelism boils down to two core ideas. Both might make your code faster, but let\u2019s be real\u2014most of the time, we\u2019re just praying it doesn\u2019t crash harder. Data Parallelism: You\u2019ve got a pile of potatoes and two people (P1 and P2). So you say, \u201cP1, take the first half. P2, take the second.\u201d Both are doing the same task\u2014just on different chunks. That\u2019s data parallelism. Same code runs everywhere ( SPMD: Single Program, Multiple Data ), only the data changes. Functional Parallelism: Same potatoes, same two people. But now you say, \u201cP1, wash them. Then hand them to P2 to chop.\u201d Different tasks for different people. That\u2019s functional parallelism. Work is split by function, not by data. Data Parallelism: Do the same thing to everything Data parallelism is about doing one thing, perfectly, across many pieces of data. You take an operation\u2014say, addition\u2014and you apply it across an entire array, not by looping, but by letting hardware chew through many elements at once. This kind of parallelism shines when operations on elements are independent of each other i.e. P1 chopping potato 1 doesn't impact P2 chopping potato 2. 1 2 3 4 5 6 7 8 9 10 11 12 // Let us multiply a C++ vector with a scalar. #include <vector> #include <omp.h> using namespace std ; vector < double > multiply_vec_with_scalar ( const vector < double >& V , double s ) { vector < double > vec ( V . size ()); #pragma omp parallel for for ( size_t i = 0 ; i < V . size (); ++ i ) { vec [ i ] = V [ i ] * scalar ; } return vec ; } Task Parallelism: Different tasks run at the same time Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores. Taking this idea a notch further, we can also think of data based parallelism Under the hood, the CPU doesn\u2019t think in individual numbers anymore. It packs data into wide vector registers: 128 bits, 256 bits, even 512 bits wide. Instead of adding two numbers, it adds eight numbers (256 bit register can accommodate 8 32-bit floats), or sixteen, in a single breath. This is what SIMD (Single Instruction, Multiple Data) is. It's minimal. It's elegant. You don't multiply effort; you multiply throughput . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include <immintrin.h> void scalar_multiply ( double * data , size_t size , double scalar ) { __m256d scalar_vec = _mm256_set1_pd ( scalar ); // Load scalar into 4 slots size_t i ; for ( i = 0 ; i < size - 3 ; i += 4 ) { __m256d data_vec = _mm256_loadu_pd ( & data [ i ]); // Load 4 elements __m256d result = _mm256_mul_pd ( data_vec , scalar_vec ); // Multiply _mm256_storeu_pd ( & data [ i ], result ); // Store back } // Handle remaining elements for (; i < size ; ++ i ) { data [ i ] *= scalar ; } } Task Parallelism: Different tasks run at the same time Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores. Core Ingredients -> if you are new to CUDA In CUDA's mental model: CPU (host) and GPU (device) are separate environments .It\u2019s called __global__ because the function becomes globally visible across two different \"worlds\": It is defined on the GPU . It can be called from the CPU . A CUDA kernel is a function that runs on the GPU . When you call a kernel, thousands of lightweight threads are launched to execute it in parallel \u2014 each thread usually works on a small piece of data.","title":"The Two Kinds of Parallelism"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#data-parallelism-do-the-same-thing-to-everything","text":"Data parallelism is about doing one thing, perfectly, across many pieces of data. You take an operation\u2014say, addition\u2014and you apply it across an entire array, not by looping, but by letting hardware chew through many elements at once. This kind of parallelism shines when operations on elements are independent of each other i.e. P1 chopping potato 1 doesn't impact P2 chopping potato 2. 1 2 3 4 5 6 7 8 9 10 11 12 // Let us multiply a C++ vector with a scalar. #include <vector> #include <omp.h> using namespace std ; vector < double > multiply_vec_with_scalar ( const vector < double >& V , double s ) { vector < double > vec ( V . size ()); #pragma omp parallel for for ( size_t i = 0 ; i < V . size (); ++ i ) { vec [ i ] = V [ i ] * scalar ; } return vec ; }","title":"Data Parallelism: Do the same thing to everything"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#task-parallelism-different-tasks-run-at-the-same-time","text":"Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores. Taking this idea a notch further, we can also think of data based parallelism Under the hood, the CPU doesn\u2019t think in individual numbers anymore. It packs data into wide vector registers: 128 bits, 256 bits, even 512 bits wide. Instead of adding two numbers, it adds eight numbers (256 bit register can accommodate 8 32-bit floats), or sixteen, in a single breath. This is what SIMD (Single Instruction, Multiple Data) is. It's minimal. It's elegant. You don't multiply effort; you multiply throughput . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include <immintrin.h> void scalar_multiply ( double * data , size_t size , double scalar ) { __m256d scalar_vec = _mm256_set1_pd ( scalar ); // Load scalar into 4 slots size_t i ; for ( i = 0 ; i < size - 3 ; i += 4 ) { __m256d data_vec = _mm256_loadu_pd ( & data [ i ]); // Load 4 elements __m256d result = _mm256_mul_pd ( data_vec , scalar_vec ); // Multiply _mm256_storeu_pd ( & data [ i ], result ); // Store back } // Handle remaining elements for (; i < size ; ++ i ) { data [ i ] *= scalar ; } }","title":"Task Parallelism: Different tasks run at the same time"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#task-parallelism-different-tasks-run-at-the-same-time_1","text":"Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores.","title":"Task Parallelism: Different tasks run at the same time"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#core-ingredients-if-you-are-new-to-cuda","text":"In CUDA's mental model: CPU (host) and GPU (device) are separate environments .It\u2019s called __global__ because the function becomes globally visible across two different \"worlds\": It is defined on the GPU . It can be called from the CPU . A CUDA kernel is a function that runs on the GPU . When you call a kernel, thousands of lightweight threads are launched to execute it in parallel \u2014 each thread usually works on a small piece of data.","title":"Core Ingredients -&gt; if you are new to CUDA"},{"location":"C%2B%2B/Unions/","text":"Unions are pure first-principles: bits are bits, and you\u2019re the one giving them meaning. Memory is just a sequence of bytes divided into slots. From this ground truth, a C++ union is as a single slot\u2014say, 4 bytes\u2014that can wear different hats: union Data { int i; float f; char c; }; lets you store an int , a float , or a char in that one spot. Set f = 3.14 , and those bytes ( 0x4048F5C3 ) are written on memory. It\u2019s sized for the largest stored value ( float ), and you decide what\u2019s active, no hand-holding from the compiler. If you overwrite `f with int i = 3`. The value in the union is mutated and you have to manually keep a track of these mutations. * Avoid 'naked\u2019 unions; wrap them in a class together with a type field; Compare this to a struct : it\u2019s a wardrobe with a slot per item \u2014an int slot, a float slot, a char slot\u2014each with its own space (e.g., 9 bytes total for struct Data { int i; float f; char c; }; ). 1 2 3 4 union Data { int i ; float f ; char c ; }; //one 4-byte chunk (sized for float) Data d ; d . i = 42 ; printf ( \"%d \\n \" , d . i ); // 42 d . f = 3.14 ; printf ( \"%f \\n \" , d . f ); // f overwrites i enum { INT , FLOAT , CHAR } type ; type = FLOAT ; // you decide what\u2019s live.","title":"Unions"},{"location":"Competitive%20Probability/Coins/","text":"Question Anna and Brenda are playing a game. They repeatedly toss a coin. Anna wins if 3 heads appear. Brenda wins if 3 tails appear. The heads and tails do not need to be consecutive. What is the expected number of coin tosses for a winner to be determined?","title":"Coins"},{"location":"Competitive%20Probability/Collecting%20Toys%20I/","text":"Every box of cereal contains one toy from a group of 5 distinct toys, each of which is mutually independent from the others and is equally likely to be within a given box. How many distinct toys can you expect to collect if you bought 7 boxes?","title":"Collecting Toys I"},{"location":"Competitive%20Probability/Combinatorics/","text":"1. Permute 2 objects of 2 distinct types in 4 spots. You're placing 2 A-type and 2 B-type objects into 4 positions. The number of distinct permutations is: \\[ \\frac{4!}{2! \\cdot 2!} = 6 \\] Memory Trick: Think of the 4 spots as a 4-letter word made of 2 As and 2 Bs (e.g., AABB). To remember the count: * Total ways to arrange 4 items = 4! * But A and B repeat , so divide by repeats : * 2! for A's * 2! for B's * So: $$ \\frac{4!}{2! \\cdot 2!} = 6 $$","title":"Combinatorics"},{"location":"Competitive%20Probability/Combinatorics/#1-permute-2-objects-of-2-distinct-types-in-4-spots","text":"You're placing 2 A-type and 2 B-type objects into 4 positions. The number of distinct permutations is: \\[ \\frac{4!}{2! \\cdot 2!} = 6 \\]","title":"1. Permute 2 objects of 2 distinct types in 4 spots."},{"location":"Competitive%20Probability/Combinatorics/#memory-trick","text":"Think of the 4 spots as a 4-letter word made of 2 As and 2 Bs (e.g., AABB). To remember the count: * Total ways to arrange 4 items = 4! * But A and B repeat , so divide by repeats : * 2! for A's * 2! for B's * So: $$ \\frac{4!}{2! \\cdot 2!} = 6 $$","title":"Memory Trick:"},{"location":"Competitive%20Probability/Coupon%20Collector/","text":"TL;DR If there are \\(N\\) different items (like toys or coupons), and each time you pick one at random, it gets harder to find the missing ones as your collection grows. * First item is always new. * Second one is still easy to find. * But the last few are rare\u2014you keep getting duplicates. The average number of total picks you need to get all \\(N\\) different items is about: $$ N \\times (\\ln N + 0.577) $$ This grows a bit faster than linear. For 50 items, you'd need about 225 tries. Problem Statement You have \\(N\\) unique coupons. Each time you draw, you get one coupon chosen uniformly at random (with replacement). Goal: Find how many draws it takes on average to collect all \\(N\\) distinct coupons. Building Blocks [[Geometric Distribution]]: Geometric distribution tells you the probability of first success in a series of repeated experiments. The expectation of Geometric Distribution is 1/p. [[Expectation]]: It is probability weighted average of a distribution. Thing of it as centre of gravity. Linearity of [[Expectation]]: Sum of expectations is Expectation of the sum. How to think about it? Think of collecting the coupons one by one: Stage 1: How many draws to get the first new coupon? \u2192 Always 1 (since you start with none). Let us call it \\(T_1\\) Stage 2: Now you have 1 unique coupon. What's the chance the next draw gives a new one? Here we measure the probability of success till we get our first success. This is where [[Geometric Distribution]] is useful. Let us call this \\(T_2\\) \u2192 Probability = \\(\\frac{N - 1}{N}\\) , so expected number of tries = \\(\\frac{N}{N - 1}\\) Stage 3: Now you have 2 unique coupons. Probability next is new = \\(\\frac{N - 2}{N}\\) , expected tries = \\(\\frac{N}{N - 2}\\) . Let us call this \\(T_3\\) And so on... By linearity of expectations: $$ E(T) = E(T_1 + \\dots + T_n) = E(T_1) + \\dots + E(T_n)$$ This implies: $$\\begin{aligned} E(T) &= 1+ \\frac{N}{N-1} + \\frac{N}{N-2} + \\dots + \\frac{N}{N-k+1} \\ &= N \\left[\\frac{1}{N} + \\dots + \\frac{1}{N-k+1} + 1 \\right] \\end{aligned} $$ The term in the bracket is a [[Harmonic Sum]], which is \\(\\log(n)\\) Hence Expectation is \\(n \\log(n)\\)","title":"Coupon Collector"},{"location":"Competitive%20Probability/Coupon%20Collector/#tldr","text":"If there are \\(N\\) different items (like toys or coupons), and each time you pick one at random, it gets harder to find the missing ones as your collection grows. * First item is always new. * Second one is still easy to find. * But the last few are rare\u2014you keep getting duplicates. The average number of total picks you need to get all \\(N\\) different items is about: $$ N \\times (\\ln N + 0.577) $$ This grows a bit faster than linear. For 50 items, you'd need about 225 tries.","title":"TL;DR"},{"location":"Competitive%20Probability/Coupon%20Collector/#problem-statement","text":"You have \\(N\\) unique coupons. Each time you draw, you get one coupon chosen uniformly at random (with replacement). Goal: Find how many draws it takes on average to collect all \\(N\\) distinct coupons.","title":"Problem Statement"},{"location":"Competitive%20Probability/Coupon%20Collector/#building-blocks","text":"[[Geometric Distribution]]: Geometric distribution tells you the probability of first success in a series of repeated experiments. The expectation of Geometric Distribution is 1/p. [[Expectation]]: It is probability weighted average of a distribution. Thing of it as centre of gravity. Linearity of [[Expectation]]: Sum of expectations is Expectation of the sum.","title":"Building Blocks"},{"location":"Competitive%20Probability/Coupon%20Collector/#how-to-think-about-it","text":"Think of collecting the coupons one by one: Stage 1: How many draws to get the first new coupon? \u2192 Always 1 (since you start with none). Let us call it \\(T_1\\) Stage 2: Now you have 1 unique coupon. What's the chance the next draw gives a new one? Here we measure the probability of success till we get our first success. This is where [[Geometric Distribution]] is useful. Let us call this \\(T_2\\) \u2192 Probability = \\(\\frac{N - 1}{N}\\) , so expected number of tries = \\(\\frac{N}{N - 1}\\) Stage 3: Now you have 2 unique coupons. Probability next is new = \\(\\frac{N - 2}{N}\\) , expected tries = \\(\\frac{N}{N - 2}\\) . Let us call this \\(T_3\\) And so on... By linearity of expectations: $$ E(T) = E(T_1 + \\dots + T_n) = E(T_1) + \\dots + E(T_n)$$ This implies: $$\\begin{aligned} E(T) &= 1+ \\frac{N}{N-1} + \\frac{N}{N-2} + \\dots + \\frac{N}{N-k+1} \\ &= N \\left[\\frac{1}{N} + \\dots + \\frac{1}{N-k+1} + 1 \\right] \\end{aligned} $$ The term in the bracket is a [[Harmonic Sum]], which is \\(\\log(n)\\) Hence Expectation is \\(n \\log(n)\\)","title":"How to think about it?"},{"location":"Competitive%20Probability/Expectation/","text":"Linearity of Expectation Linearity comes straight from the definition of expectation as a weighted average. Because summation (or integration) is a linear operator, expectation inherits linearity. \\[ \\mathbb{E}[X + Y] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty (x + y) f_{X,Y}(x, y) \\, dx\\,dy $$ $$ = \\int x f_{X,Y}(x, y)\\,dx\\,dy + \\int y f_{X,Y}(x, y)\\,dx\\,dy = \\mathbb{E}[X] + \\mathbb{E}[Y] \\] Because integration is linear , expectation is linear.","title":"Expectation"},{"location":"Competitive%20Probability/Expectation/#linearity-of-expectation","text":"Linearity comes straight from the definition of expectation as a weighted average. Because summation (or integration) is a linear operator, expectation inherits linearity. \\[ \\mathbb{E}[X + Y] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty (x + y) f_{X,Y}(x, y) \\, dx\\,dy $$ $$ = \\int x f_{X,Y}(x, y)\\,dx\\,dy + \\int y f_{X,Y}(x, y)\\,dx\\,dy = \\mathbb{E}[X] + \\mathbb{E}[Y] \\] Because integration is linear , expectation is linear.","title":"Linearity of Expectation"},{"location":"Competitive%20Probability/Geometric%20Distribution/","text":"geometric-distribution A geometric distribution models the number of trials you need to run until you get your first success in a sequence of independent attempts, each with the same probability of success. Picture flipping a coin until you hit heads: it\u2019s a simple waiting game where the odds don\u2019t change, and the question is \u201chow long until I win?\u201d Let\u2019s build it up. Start with a single trial \u2014say, a coin flip with success probability - \\(p\\) (e.g., \\(p = 0.5\\) for heads). - Failure is \\(1 - p\\) , and each trial is independent, like rolling a die unaware of past rolls. - Now, imagine waiting for the first success: you might fail \\(k-1\\) times (probability \\((1-p)^{k-1}\\) ), - Then succeed on the \\(k_{th}\\) try (probability \\(p\\) ). The geometric distribution\u2019s probability mass function is: \\(P(X = k) = (1-p)^{k-1} \\cdot p\\) where \\(k = 1, 2, 3, \\ldots\\) . It\u2019s a chain of failures capped by a win, derived from multiplying these basic probabilities together\u2014exponential decay until the breakthrough. Expectation You're trying to get a new coupon you don't already have. If you already have 1 out of \\(N\\) coupons, then there are \\(N - 1\\) new ones left. The chance the next coupon is new = \\(\\frac{N - 1}{N}\\) So, the expected number of draws to get a new one is: \\[ \\text{Expected tries} = \\frac{1}{\\text{Probability}} = \\frac{N}{N - 1} \\] This is just how probability works: if success chance is \\(p\\) , then expected number of trials until success = \\(\\frac{1}{p}\\) . Let \\(X\\) denote the number of tries till the first success. \\( \\(P(X=n) = (1-p)^n \\times p\\) \\) Now the expectation can be written as: $$\\sum^\\infty n (1-p)^n p $$ The sum of a Arithmetic-Geometric progression is: $$ S = \\frac{a}{1-r} + \\frac{dr}{(1-r)^2}$$ Hence the expectation is: \\( \\(E(X) = \\frac{1}{p}\\) \\)","title":"geometric-distribution"},{"location":"Competitive%20Probability/Geometric%20Distribution/#geometric-distribution","text":"A geometric distribution models the number of trials you need to run until you get your first success in a sequence of independent attempts, each with the same probability of success. Picture flipping a coin until you hit heads: it\u2019s a simple waiting game where the odds don\u2019t change, and the question is \u201chow long until I win?\u201d Let\u2019s build it up. Start with a single trial \u2014say, a coin flip with success probability - \\(p\\) (e.g., \\(p = 0.5\\) for heads). - Failure is \\(1 - p\\) , and each trial is independent, like rolling a die unaware of past rolls. - Now, imagine waiting for the first success: you might fail \\(k-1\\) times (probability \\((1-p)^{k-1}\\) ), - Then succeed on the \\(k_{th}\\) try (probability \\(p\\) ). The geometric distribution\u2019s probability mass function is: \\(P(X = k) = (1-p)^{k-1} \\cdot p\\) where \\(k = 1, 2, 3, \\ldots\\) . It\u2019s a chain of failures capped by a win, derived from multiplying these basic probabilities together\u2014exponential decay until the breakthrough.","title":"geometric-distribution"},{"location":"Competitive%20Probability/Geometric%20Distribution/#expectation","text":"You're trying to get a new coupon you don't already have. If you already have 1 out of \\(N\\) coupons, then there are \\(N - 1\\) new ones left. The chance the next coupon is new = \\(\\frac{N - 1}{N}\\) So, the expected number of draws to get a new one is: \\[ \\text{Expected tries} = \\frac{1}{\\text{Probability}} = \\frac{N}{N - 1} \\] This is just how probability works: if success chance is \\(p\\) , then expected number of trials until success = \\(\\frac{1}{p}\\) . Let \\(X\\) denote the number of tries till the first success. \\( \\(P(X=n) = (1-p)^n \\times p\\) \\) Now the expectation can be written as: $$\\sum^\\infty n (1-p)^n p $$ The sum of a Arithmetic-Geometric progression is: $$ S = \\frac{a}{1-r} + \\frac{dr}{(1-r)^2}$$ Hence the expectation is: \\( \\(E(X) = \\frac{1}{p}\\) \\)","title":"Expectation"},{"location":"Competitive%20Probability/Markov%20Processes/","text":"TLDR: For any Markov chain problem, Always ask: What\u2019s my state? Where does it end? How does it move? That\u2019s the recipe for modeling any Markov problem. Problem 1: You roll a fair 6-sided die repeatedly. What's the probability that you see all odd values {1,3,5} before you see any even value {2,4,6}? Step 1: Define States Let \\(P_k\\) be the probability of success after seeing \\(k\\) distinct odd numbers, for \\(k=\\{0,1,2,3\\}\\) : State 0 : no odd values seen State 1 : seen 1 distinct odd value State 2 : seen 2 distinct odd values State 3 : absorbing success state (all 3 seen) Any even \u2192 absorbing failure state (probability = 0) Note What is absorbing? In a Markov chain, a state is absorbing if once you enter it, you can\u2019t leave. It\u2019s a \u201cterminal\u201d outcome. In the problem: Success state: all 3 odd faces seen \u2192 absorbing (you stop rolling) and Failure** state: any even face appears \u2192 absorbing (you stop rolling) Step 2: Set Boundary Conditions \\(P_3 = 1\\) (success: already collected all 3 odds) If an even number is drawn in any state \u2192 probability = 3 Step 3: Write Recurrence (Transition Probabilities) At state \\(k\\) (for \\(k<3\\) ): - With probability \\(1/2\\) , you roll an even \u2192 failure \u2192 contribute 0 - With probability \\(1/2\\) , you roll an odd: - Chance it\u2019s a new odd: \\(\\frac{3-k}{3}\\) \u2192 go to state \\(k+1\\) - Chance it\u2019s a repeat odd: \\(\\frac{k}{3}\\) \u2192 stay in state \\(k\\) So, The total probability of eventual success from state \\(k\\) is the expected value of what happens after the next roll**, weighted by the probabilities of each type of outcome. \\[ P_k = \\underbrace{\\frac{1}{2} \\cdot 0}_{\\text{even} \\to \\text{failure}} + \\underbrace{\\frac{1}{2}}_{\\text{odd roll}} \\cdot \\left( \\underbrace{\\frac{3-k}{3} \\cdot P_{k+1}}_{\\text{new odd} \\to \\text{progress}} + \\underbrace{\\frac{k}{3} \\cdot P_k}_{\\text{repeat} \\to \\text{stay}} \\right) \\] Step 4: Solve Recursively (Work Backwards) Start from \\(P_3 = 1\\) . Now solve for \\(P_2\\) } Solve: Final Answer: \\[ \\boxed{P(\\text{all odd before even}) = \\frac{1}{20}} \\] Problem 2: Gambler's Ruin Problem Find the expected number of flips of a fair coin needed to have 10 more heads flipped more than tails or 7 more tails flipped more than heads. insight When you have sequence of events think of Markov Processes or Random Walks . Let \\(X_n\\) be the head-tail difference. $$ X_{n+1} =\\begin{cases} X_n+1,&\\tfrac12\\,,\\ X_n-1,&\\tfrac12\\, \\end{cases}$$ The difference increases by 1 with probability 1/2 and vice-versa. Now we need to find the probability: \\(P(X_n = 10 \\ \\cup \\ X_n = -7)\\) . Here 10 and -7 and absorption states, where the process terminates. Everything in between is a transient state \u2014you keep walking until you fall into a boundary. What are absorption states? In random walks, absorbing states are like traps \u2014 once you're in, you're stuck. The rest of the states are transient \u2014 you might visit them many times, but you'll eventually leave. The question becomes: \"How long does the walker wander before falling into one of the traps?\" Define a new RV \\(i = X_n + 7\\) $$ i_{n+1} =\\begin{cases} i+1,&\\tfrac12\\,,\\ i-1,&\\tfrac12\\, \\end{cases}$$ Now we need to find the probability: \\(i = 7 \\ \\cup \\ i = 17\\) We start from 7 and terminate at 17. Here \\(N=17\\) and \\(i=7\\) probability_concept **Mean absorption time of a Markov Process: \\[E_i = i(N - i ) = 7 \\times (17 - 7) = 70\\]","title":"Markov Processes"},{"location":"Competitive%20Probability/Markov%20Processes/#problem-1","text":"You roll a fair 6-sided die repeatedly. What's the probability that you see all odd values {1,3,5} before you see any even value {2,4,6}?","title":"Problem 1:"},{"location":"Competitive%20Probability/Markov%20Processes/#step-1-define-states","text":"Let \\(P_k\\) be the probability of success after seeing \\(k\\) distinct odd numbers, for \\(k=\\{0,1,2,3\\}\\) : State 0 : no odd values seen State 1 : seen 1 distinct odd value State 2 : seen 2 distinct odd values State 3 : absorbing success state (all 3 seen) Any even \u2192 absorbing failure state (probability = 0) Note What is absorbing? In a Markov chain, a state is absorbing if once you enter it, you can\u2019t leave. It\u2019s a \u201cterminal\u201d outcome. In the problem: Success state: all 3 odd faces seen \u2192 absorbing (you stop rolling) and Failure** state: any even face appears \u2192 absorbing (you stop rolling)","title":"Step 1: Define States"},{"location":"Competitive%20Probability/Markov%20Processes/#step-2-set-boundary-conditions","text":"\\(P_3 = 1\\) (success: already collected all 3 odds) If an even number is drawn in any state \u2192 probability = 3","title":"Step 2: Set Boundary Conditions"},{"location":"Competitive%20Probability/Markov%20Processes/#step-3-write-recurrence-transition-probabilities","text":"At state \\(k\\) (for \\(k<3\\) ): - With probability \\(1/2\\) , you roll an even \u2192 failure \u2192 contribute 0 - With probability \\(1/2\\) , you roll an odd: - Chance it\u2019s a new odd: \\(\\frac{3-k}{3}\\) \u2192 go to state \\(k+1\\) - Chance it\u2019s a repeat odd: \\(\\frac{k}{3}\\) \u2192 stay in state \\(k\\) So, The total probability of eventual success from state \\(k\\) is the expected value of what happens after the next roll**, weighted by the probabilities of each type of outcome. \\[ P_k = \\underbrace{\\frac{1}{2} \\cdot 0}_{\\text{even} \\to \\text{failure}} + \\underbrace{\\frac{1}{2}}_{\\text{odd roll}} \\cdot \\left( \\underbrace{\\frac{3-k}{3} \\cdot P_{k+1}}_{\\text{new odd} \\to \\text{progress}} + \\underbrace{\\frac{k}{3} \\cdot P_k}_{\\text{repeat} \\to \\text{stay}} \\right) \\]","title":"Step 3: Write Recurrence (Transition Probabilities)"},{"location":"Competitive%20Probability/Markov%20Processes/#step-4-solve-recursively-work-backwards","text":"Start from \\(P_3 = 1\\) . Now solve for \\(P_2\\) } Solve:","title":"Step 4: Solve Recursively (Work Backwards)"},{"location":"Competitive%20Probability/Markov%20Processes/#final-answer","text":"\\[ \\boxed{P(\\text{all odd before even}) = \\frac{1}{20}} \\]","title":"Final Answer:"},{"location":"Competitive%20Probability/Markov%20Processes/#problem-2-gamblers-ruin-problem","text":"Find the expected number of flips of a fair coin needed to have 10 more heads flipped more than tails or 7 more tails flipped more than heads.","title":"Problem 2: Gambler's Ruin Problem"},{"location":"Competitive%20Probability/Markov%20Processes/#insight-when-you-have-sequence-of-events-think-of-markov-processes-or-random-walks","text":"Let \\(X_n\\) be the head-tail difference. $$ X_{n+1} =\\begin{cases} X_n+1,&\\tfrac12\\,,\\ X_n-1,&\\tfrac12\\, \\end{cases}$$ The difference increases by 1 with probability 1/2 and vice-versa. Now we need to find the probability: \\(P(X_n = 10 \\ \\cup \\ X_n = -7)\\) . Here 10 and -7 and absorption states, where the process terminates. Everything in between is a transient state \u2014you keep walking until you fall into a boundary. What are absorption states? In random walks, absorbing states are like traps \u2014 once you're in, you're stuck. The rest of the states are transient \u2014 you might visit them many times, but you'll eventually leave. The question becomes: \"How long does the walker wander before falling into one of the traps?\" Define a new RV \\(i = X_n + 7\\) $$ i_{n+1} =\\begin{cases} i+1,&\\tfrac12\\,,\\ i-1,&\\tfrac12\\, \\end{cases}$$ Now we need to find the probability: \\(i = 7 \\ \\cup \\ i = 17\\) We start from 7 and terminate at 17. Here \\(N=17\\) and \\(i=7\\)","title":"insight When you have sequence of events think of Markov Processes or Random Walks."},{"location":"Competitive%20Probability/Markov%20Processes/#probability_concept-mean-absorption-time-of-a-markov-process","text":"\\[E_i = i(N - i ) = 7 \\times (17 - 7) = 70\\]","title":"probability_concept **Mean absorption time of a Markov Process:"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/","text":"The expected number of trials to get \\(n\\) successes in a Bernoulli process with success probability \\(p\\) is: \\[ \\mathbb{E}[\\text{trials}] = \\frac{n}{p} \\] Why? (First Principles) Each success takes on average \\(\\frac{1}{p}\\) trials (this is just the expectation of a geometric distribution). So if you need \\(n\\) successes, and each is independent: \\[ \\mathbb{E}[\\text{trials}] = \\underbrace{\\frac{1}{p} + \\frac{1}{p} + \\cdots + \\frac{1}{p}}_{n \\text{ times}} = \\frac{n}{p} \\] Example If the probability of success is 0.2 (e.g., hitting a target with 20% accuracy), and you want 5 hits: $$ \\mathbb{E}[\\text{trials}] = \\frac{5}{0.2} = 25 $$ On average, you'd expect to take 25 shots to land 5 hits. Relationship with [[Geometric Distribution]] The Negative Binomial Distribution generalizes this by modeling the number of trials needed to achieve r successes . It\u2019s essentially the sum of r independent geometric distributions (each representing the trials until a single success). The Geometric Distribution is a special case of the Negative Binomial Distribution where r=1r = 1r=1. The Negative Binomial extends the Geometric Distribution by counting the trials needed for more than one success, not just the first one.","title":"Negative Binomial Distribution"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/#why-first-principles","text":"Each success takes on average \\(\\frac{1}{p}\\) trials (this is just the expectation of a geometric distribution). So if you need \\(n\\) successes, and each is independent: \\[ \\mathbb{E}[\\text{trials}] = \\underbrace{\\frac{1}{p} + \\frac{1}{p} + \\cdots + \\frac{1}{p}}_{n \\text{ times}} = \\frac{n}{p} \\]","title":"Why? (First Principles)"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/#example","text":"If the probability of success is 0.2 (e.g., hitting a target with 20% accuracy), and you want 5 hits: $$ \\mathbb{E}[\\text{trials}] = \\frac{5}{0.2} = 25 $$ On average, you'd expect to take 25 shots to land 5 hits.","title":"Example"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/#relationship-with-geometric-distribution","text":"The Negative Binomial Distribution generalizes this by modeling the number of trials needed to achieve r successes . It\u2019s essentially the sum of r independent geometric distributions (each representing the trials until a single success). The Geometric Distribution is a special case of the Negative Binomial Distribution where r=1r = 1r=1. The Negative Binomial extends the Geometric Distribution by counting the trials needed for more than one success, not just the first one.","title":"Relationship with [[Geometric Distribution]]"},{"location":"Competitive%20Probability/Optimal%20Substructure%20Reasoning/","text":"You are given 8 fair coins and flip all of them at once. Afterwards, you are allowed to reflip as many coins as you would like one time each. At the end, you are given $1 for each head that appears. Assuming optimal play, find the fair value of this game. Each coin's state (H or T) can be treated independently .So for a single coin: Decision: If the coin is Head (H) : Keep: get $1 Reflip: 50% H, 50% T \u2192 EV = $0.5 Best choice: keep it (since $1 > $0.5) If the coin is Tail (T) : Keep: $0 Reflip: EV = $0.5 Best choice: reflip it (since $0.5 > $0) This decision is independent for each coin \u2192 meaning we solve a subproblem for each coin , and just sum up the optimal outcomes.","title":"Optimal Substructure Reasoning"},{"location":"Competitive%20Probability/Optimal%20Substructure%20Reasoning/#decision","text":"If the coin is Head (H) : Keep: get $1 Reflip: 50% H, 50% T \u2192 EV = $0.5 Best choice: keep it (since $1 > $0.5) If the coin is Tail (T) : Keep: $0 Reflip: EV = $0.5 Best choice: reflip it (since $0.5 > $0) This decision is independent for each coin \u2192 meaning we solve a subproblem for each coin , and just sum up the optimal outcomes.","title":"Decision:"},{"location":"Competitive%20Probability/Place%20and%20Take/","text":"hard You are playing a one-player game with two opaque boxes. At each turn, you can choose to either \"place\" or \"take\". \"Place\" places $1 from a third party into one box randomly. \"Take\" empties out one box randomly and that money is yours. This game consists of 100 turns where you must either place or take. Assuming optimal play, what is the expected payoff of this game? Note that you do not know how much money you have taken until the end of the game. Thought Process Think of every dollar you drop in as a goldfish in a pond. Each time you \u201ctake,\u201d you scoop one of the two ponds at random, so any given goldfish has a 1/2 chance of getting caught. If you plan to scoop k times, the chance a goldfish eventually gets caught is $$ 1 - (\\frac{1}{2})^k $$ How? Probability of not getting caught in \\(k\\) attempts = \\((1/2)^k\\) Probability of getting caught atleast once = \\(1 - (1/2)^k\\) You take \\(k\\) attempts so you place \\(100-k\\) attempts. So payoff function: $$ (100-k)(1 - (\\frac{1}{2})^k) $$ Maximise this payoff as: \\[ \\text{arg max}_k (100-k)(1 - 0.5^k) \\] Differential it wrt \\(k\\) and set the derivative to 0. \\[ \\begin{aligned} \\text{Let} \\ u &= 1-0.5^k \\\\ du/dk &= k0.5^{k-1} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{Let} \\ v &= 100-k \\\\ du/dv &= -1 \\\\ \\end{aligned} \\] $$ \\begin{aligned} 0.5^k -1 + k0.5^{k-1}(100-k) = 0 \\ 0.5^k + 2k0.5^{k}(100-k) = 1 \\ \\text{Let} \\ 0.5^k = T \\ T - 200kT - 2k^2T =1 \\end{aligned} $$","title":"hard"},{"location":"Competitive%20Probability/Place%20and%20Take/#hard","text":"You are playing a one-player game with two opaque boxes. At each turn, you can choose to either \"place\" or \"take\". \"Place\" places $1 from a third party into one box randomly. \"Take\" empties out one box randomly and that money is yours. This game consists of 100 turns where you must either place or take. Assuming optimal play, what is the expected payoff of this game? Note that you do not know how much money you have taken until the end of the game.","title":"hard"},{"location":"Competitive%20Probability/Place%20and%20Take/#thought-process","text":"Think of every dollar you drop in as a goldfish in a pond. Each time you \u201ctake,\u201d you scoop one of the two ponds at random, so any given goldfish has a 1/2 chance of getting caught. If you plan to scoop k times, the chance a goldfish eventually gets caught is $$ 1 - (\\frac{1}{2})^k $$ How? Probability of not getting caught in \\(k\\) attempts = \\((1/2)^k\\) Probability of getting caught atleast once = \\(1 - (1/2)^k\\) You take \\(k\\) attempts so you place \\(100-k\\) attempts. So payoff function: $$ (100-k)(1 - (\\frac{1}{2})^k) $$ Maximise this payoff as: \\[ \\text{arg max}_k (100-k)(1 - 0.5^k) \\] Differential it wrt \\(k\\) and set the derivative to 0. \\[ \\begin{aligned} \\text{Let} \\ u &= 1-0.5^k \\\\ du/dk &= k0.5^{k-1} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{Let} \\ v &= 100-k \\\\ du/dv &= -1 \\\\ \\end{aligned} \\] $$ \\begin{aligned} 0.5^k -1 + k0.5^{k-1}(100-k) = 0 \\ 0.5^k + 2k0.5^{k}(100-k) = 1 \\ \\text{Let} \\ 0.5^k = T \\ T - 200kT - 2k^2T =1 \\end{aligned} $$","title":"Thought Process"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/","text":"To resolve merge conflicts with master so that your changes override the conflicting changes from master , follow these steps: 1. Fetch and merge master 1 2 git fetch origin git merge origin/master If there are conflicts, Git will pause and mark the conflicting files. What is the difference between git fetch and git pull ?: git fetch downloads objects/refs from another repository (or remote) but doesn't merge them into your working directory. git pull does both: it fetches and then merges. 2. Overwrite conflicts with your version For each conflicted file: 1 git checkout --ours <file> --ours = your branch (i.e., your changes override master ) Repeat this for all conflicted files or run: 1 git diff --name-only --diff-filter = U | xargs git checkout --ours 3. Mark conflicts resolved 1 git add . 4. Complete the merge 1 git commit Alternative: Rebase with your changes overriding master 1 git rebase origin/master Then during each conflict: 1 2 3 git checkout --ours <file> git add <file> git rebase --continue Warning: This strategy discards all changes made in master for the conflicted files, so use it only when you're sure your changes must prevail.","title":"Resolve Merge Conflicts"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#1-fetch-and-merge-master","text":"1 2 git fetch origin git merge origin/master If there are conflicts, Git will pause and mark the conflicting files. What is the difference between git fetch and git pull ?: git fetch downloads objects/refs from another repository (or remote) but doesn't merge them into your working directory. git pull does both: it fetches and then merges.","title":"1. Fetch and merge master"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#2-overwrite-conflicts-with-your-version","text":"For each conflicted file: 1 git checkout --ours <file> --ours = your branch (i.e., your changes override master ) Repeat this for all conflicted files or run: 1 git diff --name-only --diff-filter = U | xargs git checkout --ours","title":"2. Overwrite conflicts with your version"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#3-mark-conflicts-resolved","text":"1 git add .","title":"3. Mark conflicts resolved"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#4-complete-the-merge","text":"1 git commit","title":"4. Complete the merge"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#alternative-rebase-with-your-changes-overriding-master","text":"1 git rebase origin/master Then during each conflict: 1 2 3 git checkout --ours <file> git add <file> git rebase --continue Warning: This strategy discards all changes made in master for the conflicted files, so use it only when you're sure your changes must prevail.","title":"Alternative: Rebase with your changes overriding master"},{"location":"Dark%20Arts/Valeyere%20%282025%29/","text":"Keep two running trackers: - tracker A = \u201chow jumpy prices have been lately\u201d (volatility) - tracker B = \u201chow prices have moved lately\u201d (returns), but scaled by tracker A Update both after every new price. The ratio tells you how many standard deviations today\u2019s price move is, smoothed over time. Large positive = trend up, large negative = trend down. Step-by-step Compute a scale-free return $$r_{t}= \\frac{P_t-P_{t-1}}{P_{t-1}}\\quad\\text{or}\\quad\\ln(P_t/P_{t-1}) $$ Using percent/log makes the metric independent of the absolute price level. Update volatility (\u03c3\u00b2) first Exponential moving average of squared returns with decay \u03b7: $$\\sigma^{2}{t}= (1-\\eta)\\,\\sigma^{2}{t-1}+\\eta\\,r_{t}^{2} $$ This is a one-line \u201cmemory\u201d of recent squared moves; smaller \u03b7 = longer memory. Normalise today\u2019s return $$z_t = \\frac{r_t}{\\sigma_t+\\epsilon} $$ \u03b5 is a tiny constant to prevent division by zero. Update the signal Another exponential average, but of the z-scores: $$ \\phi_t = (1-\\eta)\\,\\phi_{t-1} + \\sqrt{\\eta}\\,z_t $$ The \\(\\sqrt{\\eta}\\) factor keeps the signal\u2019s variance near 1 when returns are white noise. So \u03c6 = +2.0 means: the exponentially weighted average of 112 day returns is two standard deviations above zero.","title":"Valeyere (2025)"},{"location":"Linear%20Algebra/Inverse%20of%20the%20Matrix/","text":"Tip A matrix that can be inverted is one that preserves the essential structure of the space\u2014 it may stretch, rotate, reflect, but it does not collapse dimensions or fold the space upon itself in an irreversible way . The determinant, that mysterious quantity, what is it but a measure of how the transformation changes the \"volume\" of space?","title":"Inverse of the Matrix"},{"location":"Linear%20Algebra/Linear%20Map/","text":"A linear map is a function that takes vectors, transforms them into new vectors, and makes sure that adding and scaling vectors still works the same way before and after the transformation. A linear map is just a rule that takes a vector and turns it into another vector without bending or breaking its structure . It respects addition and scaling. Think of it like applying a fixed transformation that preserves lines and the origin. Definition A linear map (or linear transformation ) is a function \\(T: V \\to W\\) between two [[Vector Space]] such that for all vectors \\(\\mathbf{u}, \\mathbf{v} \\in V\\) and all scalars \\(c \\in \\mathbb{R}\\) (or any field), it satisfies: Additivity : Transformation on addition is addition of transformations: \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) Homogeneity (scalar multiplication) : Transformation on a vector scalar product is same as scalar multiplied to a transformed vector. \\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) These two together mean: $$ T(a\\mathbf{u} + b\\mathbf{v}) = aT(\\mathbf{u}) + bT(\\mathbf{v}) $$ Key Properties The origin maps to origin : \\(T(\\mathbf{0}) = \\mathbf{0}\\) Linear maps can be represented as matrices when vector spaces are finite-dimensional. If \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) , then \\(T(\\mathbf{x}) = A\\mathbf{x}\\) for some \\(m \\times n\\) matrix \\(A\\) . Examples Scaling : \\(T(x) = 2x\\) \u2192 doubles every input \u2192 linear. Rotation in the plane \u2192 linear. \\(T(x) = x + 1\\) \u2192 not linear because it doesn\u2019t preserve the origin. Geometric Intuition A linear map: * Preserves straight lines. * Preserves ratios along lines. * Doesn\u2019t twist, curve, or displace the origin. Why It Matters Linear maps are the building blocks of all transformations in linear algebra. Every matrix is a linear map. They're fundamental in: Solving systems of equations. Understanding projections, rotations, shears. Machine learning (every neural net layer is mostly a linear map + nonlinearity).","title":"Linear Map"},{"location":"Linear%20Algebra/Linear%20Map/#definition","text":"A linear map (or linear transformation ) is a function \\(T: V \\to W\\) between two [[Vector Space]] such that for all vectors \\(\\mathbf{u}, \\mathbf{v} \\in V\\) and all scalars \\(c \\in \\mathbb{R}\\) (or any field), it satisfies: Additivity : Transformation on addition is addition of transformations: \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) Homogeneity (scalar multiplication) : Transformation on a vector scalar product is same as scalar multiplied to a transformed vector. \\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) These two together mean: $$ T(a\\mathbf{u} + b\\mathbf{v}) = aT(\\mathbf{u}) + bT(\\mathbf{v}) $$","title":"Definition"},{"location":"Linear%20Algebra/Linear%20Map/#key-properties","text":"The origin maps to origin : \\(T(\\mathbf{0}) = \\mathbf{0}\\) Linear maps can be represented as matrices when vector spaces are finite-dimensional. If \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) , then \\(T(\\mathbf{x}) = A\\mathbf{x}\\) for some \\(m \\times n\\) matrix \\(A\\) .","title":"Key Properties"},{"location":"Linear%20Algebra/Linear%20Map/#examples","text":"Scaling : \\(T(x) = 2x\\) \u2192 doubles every input \u2192 linear. Rotation in the plane \u2192 linear. \\(T(x) = x + 1\\) \u2192 not linear because it doesn\u2019t preserve the origin.","title":"Examples"},{"location":"Linear%20Algebra/Linear%20Map/#geometric-intuition","text":"A linear map: * Preserves straight lines. * Preserves ratios along lines. * Doesn\u2019t twist, curve, or displace the origin.","title":"Geometric Intuition"},{"location":"Linear%20Algebra/Linear%20Map/#why-it-matters","text":"Linear maps are the building blocks of all transformations in linear algebra. Every matrix is a linear map. They're fundamental in: Solving systems of equations. Understanding projections, rotations, shears. Machine learning (every neural net layer is mostly a linear map + nonlinearity).","title":"Why It Matters"},{"location":"Linear%20Algebra/Matrix%20Multiplication/","text":"When I multiply A by B, I think of it as A acting on each column of B separately. Each column of the result is A transforming one of B's columns. Each row of C would be some combination of the rows of B, weighted by the corresponding row of A! That matrix multiplication represents the composition of linear transformations - the chaining together of ways to reshape space itself. but necessary for the transformations to be composable. And what of the result? If A is m\u00d7n and B is n\u00d7p, what can we say about AB? B transforms p-dimensional vectors to n-dimensional vectors, then A transforms those to m-dimensional vectors. The composition goes from p dimensions to m dimensions. First transformation B acts on space, then transformation A acts on the result. AB represents the combined effect - what happens when we perform B first, then A.","title":"Matrix Multiplication"},{"location":"Linear%20Algebra/The%20Deeper%20Meaning%20of%20Matrix%20Transpose/","text":"What are Covectors? Covectors are linear functions that take a vector as input and produce a scalar (a number) as output. They are described as \"measuring devices\" that assign a value to a vector in a way that respects linearity. For example: If a covector measures vector \\(v_1\\) as \\(a_1\\) and \\(v_2\\) as \\(a_2\\) , it measures: \\(v_1 + v_2\\) as \\(a_1 + a_2\\) . If a vector \\(v_1\\) is scaled by a factor \\(\\lambda\\) , the covector measures \\(\\lambda v_1\\) as \\(\\lambda a_1\\) . Example in 2D: A covector represented as \\((a, b)\\) measures a vector \\((x, y)\\) by computing \\(ax + by\\) , a linear combination that yields a scalar.","title":"The Deeper Meaning of Matrix Transpose"},{"location":"Linear%20Algebra/The%20Deeper%20Meaning%20of%20Matrix%20Transpose/#what-are-covectors","text":"Covectors are linear functions that take a vector as input and produce a scalar (a number) as output. They are described as \"measuring devices\" that assign a value to a vector in a way that respects linearity. For example: If a covector measures vector \\(v_1\\) as \\(a_1\\) and \\(v_2\\) as \\(a_2\\) , it measures: \\(v_1 + v_2\\) as \\(a_1 + a_2\\) . If a vector \\(v_1\\) is scaled by a factor \\(\\lambda\\) , the covector measures \\(\\lambda v_1\\) as \\(\\lambda a_1\\) . Example in 2D: A covector represented as \\((a, b)\\) measures a vector \\((x, y)\\) by computing \\(ax + by\\) , a linear combination that yields a scalar.","title":"What are Covectors?"},{"location":"Linear%20Algebra/Vector%20Projection/","text":"Vector projection is equivalent to the dot product. The dot product of r and s is the projection of the vector \\(r\\) on the vector \\(s\\) . For two n component vectors, the dot product is defined as: $$ a . b = a_1b_1 + a_2b_2 \\dots + a_nb_n $$ Geometrically, for two vectors r and s, the projection appears as follows: It is equivalent to: $$ r . s = |r| |s| \\cos \\theta $$ Basis To be considered as a basis, a set of vectors must: - Be linearly independent - Span the space Basis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis). ![[Pasted image 20250429102423.png]] Change of Basis Suppose we have a vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) or \\(1\\hat{i} + 3\\hat{j}\\) . Here \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis vectors. These vectors are also known as a standard basis. Here, \\(\\hat{i}\\) is a unit vector in the \\(x\\) direction and \\(\\hat{j}\\) is a unit vector in the \\(y\\) direction. The vector \\(\\begin{pmatrix} 5 \\cr -1 \\end{pmatrix}\\) can be written as a linear combination of the basis vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) as follows: $$\\begin{pmatrix}1 & 0\\\\ 0 & 1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} $$ Now suppose we want to change the basis vectors to \\(\\begin{pmatrix} 1 \\cr 1 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 1 \\cr -1 \\end{pmatrix}\\) . Now instead of \\(\\hat{i}\\) and \\(\\hat{j}\\) , we have \\(\\hat{i'}\\) and \\(\\hat{j'}\\) . We can write the vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) as a linear combination of the new basis vectors as follows: $$\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} = 2 \\cdot \\begin{pmatrix}2 \\cr 3\\end{pmatrix} $$ Useful Links to Consult: https://www.nagwa.com/en/explainers/792181370490/","title":"Vector Projection"},{"location":"Linear%20Algebra/Vector%20Projection/#basis","text":"To be considered as a basis, a set of vectors must: - Be linearly independent - Span the space Basis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis). ![[Pasted image 20250429102423.png]]","title":"Basis"},{"location":"Linear%20Algebra/Vector%20Projection/#change-of-basis","text":"Suppose we have a vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) or \\(1\\hat{i} + 3\\hat{j}\\) . Here \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis vectors. These vectors are also known as a standard basis. Here, \\(\\hat{i}\\) is a unit vector in the \\(x\\) direction and \\(\\hat{j}\\) is a unit vector in the \\(y\\) direction. The vector \\(\\begin{pmatrix} 5 \\cr -1 \\end{pmatrix}\\) can be written as a linear combination of the basis vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) as follows: $$\\begin{pmatrix}1 & 0\\\\ 0 & 1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} $$ Now suppose we want to change the basis vectors to \\(\\begin{pmatrix} 1 \\cr 1 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 1 \\cr -1 \\end{pmatrix}\\) . Now instead of \\(\\hat{i}\\) and \\(\\hat{j}\\) , we have \\(\\hat{i'}\\) and \\(\\hat{j'}\\) . We can write the vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) as a linear combination of the new basis vectors as follows: $$\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} = 2 \\cdot \\begin{pmatrix}2 \\cr 3\\end{pmatrix} $$","title":"Change of Basis"},{"location":"Linear%20Algebra/Vector%20Projection/#useful-links-to-consult","text":"https://www.nagwa.com/en/explainers/792181370490/","title":"Useful Links to Consult:"},{"location":"Linear%20Algebra/Vector%20Space/","text":"TL;DR (First Principles): A vector space is where linear algebra happens. If you can: * Add any two elements, * Multiply them by any scalar, * And still stay inside the same set, Then you're in a vector space. Examples \\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) . Formal Definition A vector space \\(V\\) over a field \\(\\mathbb{F}\\) (usually \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) ) is a set equipped with two operations: Vector addition : \\(\\mathbf{u} + \\mathbf{v} \\in V\\) Scalar multiplication : \\(c\\mathbf{v} \\in V\\) , where \\(c \\in \\mathbb{F}\\) These operations must satisfy 8 axioms (rules), such as: Closure under addition and scalar multiplication Associativity , commutativity of addition Distributivity of scalar multiplication over scalars and vectors Existence of zero vector and additive inverses Multiplying by 1 leaves vector unchanged Intuition If you can: Add any two elements, Multiply them by any scalar, And still stay inside the same set, Then you're in a vector space. Examples \\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) . Non-Examples Vectors with a non-standard addition (e.g., max instead of +) Vectors that can\u2019t be scaled (e.g., integers aren't closed under division) Why It Matters Vector spaces provide the stage for linear algebra. Once you\u2019re in a vector space: You can apply linear maps (transformations). You can define bases , dimension , and coordinates . Concepts like span, linear independence, subspaces, and projections make sense. In short, vector spaces are where linear algebra happens .","title":"Vector Space"},{"location":"Linear%20Algebra/Vector%20Space/#examples","text":"\\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) .","title":"Examples"},{"location":"Linear%20Algebra/Vector%20Space/#formal-definition","text":"A vector space \\(V\\) over a field \\(\\mathbb{F}\\) (usually \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) ) is a set equipped with two operations: Vector addition : \\(\\mathbf{u} + \\mathbf{v} \\in V\\) Scalar multiplication : \\(c\\mathbf{v} \\in V\\) , where \\(c \\in \\mathbb{F}\\) These operations must satisfy 8 axioms (rules), such as: Closure under addition and scalar multiplication Associativity , commutativity of addition Distributivity of scalar multiplication over scalars and vectors Existence of zero vector and additive inverses Multiplying by 1 leaves vector unchanged","title":"Formal Definition"},{"location":"Linear%20Algebra/Vector%20Space/#intuition","text":"If you can: Add any two elements, Multiply them by any scalar, And still stay inside the same set, Then you're in a vector space.","title":"Intuition"},{"location":"Linear%20Algebra/Vector%20Space/#examples_1","text":"\\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) .","title":"Examples"},{"location":"Linear%20Algebra/Vector%20Space/#non-examples","text":"Vectors with a non-standard addition (e.g., max instead of +) Vectors that can\u2019t be scaled (e.g., integers aren't closed under division)","title":"Non-Examples"},{"location":"Linear%20Algebra/Vector%20Space/#why-it-matters","text":"Vector spaces provide the stage for linear algebra. Once you\u2019re in a vector space: You can apply linear maps (transformations). You can define bases , dimension , and coordinates . Concepts like span, linear independence, subspaces, and projections make sense. In short, vector spaces are where linear algebra happens .","title":"Why It Matters"},{"location":"Polish/Connectors/","text":"Here\u2019s a concise Polish connecting words (conjunctions) cheat sheet with English meanings and usage context: Cause / Reason Polish English Example bo because Nie id\u0119, bo jestem chory. poniewa\u017c because / since Zosta\u0142em, poniewa\u017c pada\u0142o. gdy\u017c because (formal) Nie przyszed\u0142, gdy\u017c by\u0142 zaj\u0119ty. Effect / Result Polish English Example dlatego that\u2019s why Pada, dlatego nie id\u0119. wi\u0119c so Jest p\u00f3\u017ano, wi\u0119c id\u0119 spa\u0107. zatem therefore (formal) By\u0142o ciemno, zatem zapali\u0142 \u015bwiat\u0142o. Contrast / Opposition Polish English Example ale but Chc\u0119 i\u015b\u0107, ale jestem zm\u0119czony. jednak however / yet By\u0142o ciep\u0142o, jednak pada\u0142o. lecz but / yet (formal) Chcia\u0142, lecz nie m\u00f3g\u0142. mimo \u017ce although Mimo \u017ce pada\u0142o, wyszed\u0142. Purpose Polish English Example \u017ceby so that / in order to Ucz\u0119 si\u0119, \u017ceby zda\u0107 egzamin. aby so that (formal) Pracuj\u0119, aby mie\u0107 pieni\u0105dze. Condition Polish English Example je\u015bli if Je\u015bli chcesz, mo\u017cemy i\u015b\u0107. je\u017celi if (formal) Je\u017celi pada, zostaj\u0119 w domu. gdy when / if Gdy b\u0119dzie gotowe, dam zna\u0107. Want more categories (time, comparisons, etc.) or flashcard format?","title":"Connectors"},{"location":"Polish/notes/","text":"Serial, Polish, English, Pronunciation 1, Kto to jest , Who is this?, Kto to yest 2, Pami\u0119tam , I remember, Paa-mie(n)-entam 3, Czytamy , To read, Chetame 4, Ale , But, Aale 5, Dlaczego , Why, Dlachego 6, Z\u0142y , Bad, Zwee 7, Zeszyt , Notebook, Zesh-zet 8, Gumka , Eraser 9, P\u0142yn , Liquid, Pwin 10, D\u0142ugopis , Pen , Dwoo-go-pis 11, Okno , Window, Okno 12, Klucz , Key, Kluch 13, Ksi\u0105\u017cka , Book, Kshyanka 14, Tablica , Board, Taa-blee-tsa 15, Krzes\u0142o , Chair, Ksheshwo 16, Segregator , Folder 17, Kubek , Mug 18, Gumka , Eraser 19, Mapa , Map 20, Znany , Known / Famous, 21, Nieznany , Unknown 22, Znajomy , Friend, Znayome 23, To jest Kom\u00f3rka , This is a cellphone 24, A co to jest? , And what is this?, Aa so to yest? 24, To jestem ja , This is me, To yestem ya 25, Kole\u017canka , Classmate, Kolezhanka 26, Wakacje , Holidays, Vakatsye 27, Zdj\u0119cie , Photo, 28, Prawda , Truth 29, Fa\u0142sz , Lie 30, Modna , Fashionable 31, Nic Specjalnego , Nothing special, Nits Spets-yalnego 32, O\u0142\u00f3wek , Pencil 33, Mam tylko jeden , I have only one 34, Ja mam d\u0142ugopis , I have a pen 35, Mami , Mom 36, Tati , Dad 37, Przystojny , Handsome 38, I co tam jest? , And what's there? 39, Mam Pyntanie , I have a question 40, Modne , Fashionable 41, Rodzaj , Kind 42, Nijaki , Neutral 43, Na Pocz\u0105tku , At the beginning 44, Na ko\u0144cu , At the end, Naa koen-tsu 45, Mi\u0142y , Nice, Meewe 46, Bogaty , Rich 47, Biedny , Poor 48, Pomara\u0144czowy , Orange 49, Zielony , Green 50, Rozowy , Pink 51, Niebieski , Blue 52, Czerwony , Red 53, Granatowy , Navy Blue 54, Br\u0105zowy , Brown 55, Szary , Grey 56, Srebrny , Silver 57, Z\u0142oty , Gold 58, Czarny , Black 59, Bia\u0142y , White 60, \u017b\u00f3\u0142ty , Yellow 61, Jak si\u0119 masz? , How are you? 62, Dobrze , Good 63, \u0179le , Bad 64, S\u0142owa , Words 65, Zdania , Sentences 66, Czytamy , We read 67, Ta Kom\u00f3rka , This cellphone 68, Ten artysta jest przystojny , This artist is handsome 69, Ta pani jest bogata , This lady is rich 70, Stolik , Table 71, Ta gumka jest r\u00f3\u017cowa , This eraser is pink 72, Ta Kom\u00f3rka jest czarna , This cellphone is black 73, Ta torba jest fioletowa , This bag is purple 74, Ten Klucz jest srebrny , This key is silver 75, Pantera , Panther 76, Humor , Mood 77, Niebo , Sky 78, Czy to jest kawa? , Is this coffee? 79, O\u0142owek jest szara , The pencil is grey 80, Znam Angielski , I know English 81, Co to znaczy? , What does it mean? 82, Zart , Joke 83, Pami\u0119tam , I remember 84, Ale , But 85, \u015bwietnie , Great 86, Troch\u0119 , A little 87, Wiem , I know 88, Racj\u0119 , Right 89, Mnie Te\u017c , Me too 90, i teraz , And now 91, ja lubi\u0119 , I like 92, ty lubisz , You like 93, Co to ciebie , What about you? 94, Oni mieszkaj\u0105 , They live 95, \u0142atwo , Easy 96, My mieszkamy , We live 97, Czy oni ta\u0144cz\u0105 sals\u0119? , Do they dance salsa? 98, Wy mieszkacie w Warszawie , You live in Warsaw 99, Palicie , You smoke 100, My , We 101, Ona lubi , She likes 102, Oni lubi\u0105 , They like 103, On lubi , He likes 104, One lubi\u0105 , They like 105, Oni nie pal\u0105 , They do not smoke 106, Ja cieszy\u0107 si\u0119 , I enjoy 107, Smutna , Sad 108, My\u015ble\u0107 , To think 109, Wysoki , Tall 110, Niski , Short 111, Trudny , Difficult 112, Zdrowy , Healthy 113, Chora , Sick 114, Brudna , Dirty 115, Czysta , Clean 116, Gruby , Fat 117, Szczup\u0142y , Slim 118, M\u0142ody , Young 119, Stary , Old 120, \u0141adny , Pretty 121, Brzydki , Ugly 122, Zm\u0119czony , Tired 123, Leniwa , Lazy 124, Pi\u0119kna i Smutna , Beautiful and sad 125, Ciekawe, jaki on jest , I wonder what he is like 126, Umie szybko decydowa\u0107 , He can decide quickly 127, Zdecydowany , Decisive 128, Wie, ile jest warty i jakie ma mo\u017cliwo\u015bci , He knows how much he is worth and what he is capable of 129, Nie lubi m\u00f3wi\u0107 o swoich uczuciach , He does not like to talk about his feelings 130, Otwarta , Open 131, Chaotyczny , Chaotic 132, Uczuciach , Feelings 133, Towarzyski , Sociable 134, Ciekawy , Curious 135, Energiczny , Energetic 136, Romanticzny , Romantic 137, \u017byczliwo\u015b\u0107 , Kindness 138, Szczery , Honest 139, Weso\u0142y , Cheerful 140, Nie pami\u0119tam dobrze , I do not remember well 141, Jestem pewny \u017ce ona jest energiczna, spontaniczna i bardzo sympatyczna , I am sure she is energetic, spontaneous and very nice 142, Atrakcyjna , Attractive 143, Zdanie , Sentence 144, 11, Jedena\u015bcie, Yedenashchie 145, 12, Dwana\u015bcie, Dvanashchie 146, 13, Trzyna\u015bcie, Trinashchie 147, 14, Czterna\u015bcie, Chetirnashchie 148, 15, Pi\u0119tna\u015bcie, Pyentnashchie 149, 16, Szesna\u015bcie, Shesnashchie 150, 17, Siedemna\u015bcie 151, 18, Osiemna\u015bcie 152, 19, Dziewi\u0119tna\u015bcie 153, 20, Dwadzie\u015bcia 154, 10, Dziesi\u0119\u0107 155, 20, Dwadzie\u015bcia 156, 30, Trzydzie\u015bci 157, 40, Czterdzie\u015bci 158, 50, Pi\u0119\u0107dziesi\u0105t 159, 60, Sze\u015b\u0107dziesi\u0105t 160, 70, Siedemdziesi\u0105t 161, 80, Osiemdziesi\u0105t 162, 90, Dziewi\u0119\u0107dziesi\u0105t 163, 100, Sto 164, Ile masz lat? , How old are you? 165, Mam 24 lata , I am 20 years old 166, Mam 25 lat , I am 25 years old 167, Mam 22 lata , I am 22 years old 168, Mam 23 lata , I am 23 years old 169, Mam 21 lat , I am 21 years old 170, Aktor , Actor 171, Aktorka , Actress 172, Piosenkarz , Singer 173, Piosenkarka , Singer 174, Muzyk , Musician 175, Zawody , Professions, Zaaavody 176, Lekarz , Doctor 177, Kucharz , Cook, kukharz 178, In\u017cynier , Engineer, Inzineer 179, Policjant , Policeman, Politsyant 180, Dziennikarz , Journalist, 181, Kelner , Waiter 182, Informatyk , IT specialist 183, Rolnik , Farmer 184, Sprzedawca , Salesman, 185, Emeryt , Pensioner 186, Kelner , Waiter 187, Tancerz , Dancer, Taanserka 188, Urz\u0119dnik , Clerk, 189, Kim jeste\u015b? , Who are you? 190, Jestem Analitykiem Ryzyka, I am a Risk analyst 191, \u017bona, Wife 192, Syn, Son, Sen 193, Skomplikowany, Complicated 194, Gram na gitarze, I play the guitar 196, Tu jest moja rodzina, Here is my family 197, Utalentowana, Talented, Ootalentovana 198. Ksi\u0119gowa, Accountant, Ksyegova 199, Krawiec, Tailor, Krawiets 200, Stolarz, Carpenter, Stolarz 201, Kierowca, Driver, Kierovtsa 202, Listonosz, Postman, Listonosh 203, Stra\u017cak, Fireman, Strazhak 204, Piel\u0119gniarka, Nurse, Pieliegnarka 205, Graj\u0105 na gitarze, They play the guitar 206, Co pan robi? , What are you doing? 207, Chwileczk\u0119, Just a moment, Khweeletschka 208, Nudz\u0119 si\u0119, I am bored, Noodzhe she 209, Chodzi\u0107 na spacery, To go for a walk, Khodzits na spatseree 210, Robi\u0107 Zdj\u0119cia, To take pictures, Robich Zdjenshia 211, Czyta\u0107 ksi\u0105\u017cki, To read books, Shitach ksiyonszhki 212, P\u0142ywa\u0107, To swim, Pwivach 213, Robi\u0107 Zdj\u0119cia, To take pictures, Robich Zdjenshia 214, Lubi\u0119 czyta\u0107 ksi\u0105\u017cki, I like to read books, Lubien shitach ksiyonszhki 215, Mam dwa psy, I have two dogs, Mam Dva psi 216, On ma psa, He has a dog, On ma psa 217, Politycy Przepraszaj\u0105 za swoje b\u0142\u0119dy, Politicians apologize for their mistakes, Politsytsi Przepraszayon za svoye bwendy 218, Oni maj\u0105 psa, They have a dog, Oni mayom psa 219, Ona pyta o drog\u0119, She asks for directions, Ona peta o drogen 220, Czy ty mowisz po angielsku? , Do you speak English? 221, Ja my\u015bl\u0119, \u017ce polski jest trudny, I think Polish is difficult, Ya meswen, zhe polski yest troodne 222, Czy oni ta\u0144cz\u0105 sals\u0119? , Do they dance salsa?, Shay oni taanshchayon salsen 223, Zdanje, Sentence, Zdanye 224, Nie rozumiem to zdanie, I do not understand this sentence, Nye rozoomyem to zdanye 225, A ty co lubisz robi\u0107? , And what do you like to do?, A ti tso lubish robich 226, To fantastyczne, It is fantastic, To fantastishne 227, Idziemy ta\u0144czy\u0107, Let's go dancing, Idzhiemy taanshich 228, Gospodyni, Hostess, Gospodeni 229, Gospodarz, Host, Gospodarz 230, Co robisz po lekcji, What are you doing after class, Tso robish po lektsyi 231, No nie wiem, Well, I do not know, No nye vyem 232, Mami mailuje, Mom is emailing, Mami maylooye 233, Oni du\u017co podr\u00f3\u017cuj\u0105, They travel a lot, Oni doozho podroozhooyon 234, Co ty teraz gotujesz? , What are you cooking now?, Tso ti teraz gotuyesh 234, Uwe telephonuje, Uwe is calling, Uwe telephonoye 235, Gdjie jestes spaceruj\u0119 po parku, Where are you walking in the park, Gdjie yestesh spatserooyen po parkoo 236, Czy wy pracujecie w weekendy? , Do you work on weekends?, Tsi vi pratsuyetse v vyekendy 237, Relaxowa\u0107, To relax, Relaksowach 238, Je\u017cd\u017c\u0119 na rowerze, I ride a bike, Yezhdzhe na rovetshe 239, Chodz\u0119 na zakupy, I go shopping, Khodzhe na zakopy 240, Ogl\u0105dam telewizj\u0119, I watch TV, Oglondam televeezyen 241, Lubisz s\u0142ucha\u0107 muzyki? , Do you like listening to music?, Lubish swhukhach mooziky 243, Gra\u0107 w pi\u0142k\u0119 no\u017cn\u0105, To play football, Grach v pionka nozhna 244, Jestem Zm\u0119czany, I am tired, Yestem zmenchany 245, Masz wolny czas czy jeste\u015b zaj\u0119ty? , Do you have free time or are you busy?, Mash volny chas tsi yestesh zayenty 246, Rozumiem troch\u0119 po polsku, I understand a little Polish, Rozoomyem trokhe po polsku 247, Jutro, Tomorrow, Yootro 248, Wczoraj, Yesterday, Vchoray 249, Dzi\u015b, Today, Dzish 250, Przystojnego brata, Handsome brother, Pristoyenego brata 251, \u0142adn\u0105 siostr\u0119, Pretty sister, Ladnaun siostraun 252, Monika lubi kaw\u0119, Monika likes coffee, Monika lubi kave 253, Lubi\u0119 muzyk\u0119 klasyczn\u0105, I like classical music, Lubie mooziky klasichnay 254, Ochot\u0119 na kaw\u0119, I feel like coffee, Okhoty na kave 255, sp\u00f3\u0142g\u0142oska, consonant, spoolgoshka 256, Czy co\u015b jeszcze, Anything else, Tsi tsoys yeshche 257, Lubi\u0119 polsk\u0105 literatur\u0119, I like Polish literature, Lubie polskaun literatooren 258, Pada deszcz, It is raining, Pada deshch 259, Czy c\u00f3s jeszcze, Anything else, Tsi tsoys yeshche 260, Pi\u0119tro, Floor, Pyentro 261, Opcja, Option, Optsya 262, S\u0142odkie Czerwony, Sweet Red, Swodkie Chervony 263, Morela, Apricot, Morela 264, Brzoskwinia, Peach, Brzoskvinia 265, Seler, Celery, Seler 266, Cebula, Onion, Tseboola 267, Czosnek, Garlic, Chosnek 268, Czerwona papryka, Red pepper, Chervona paprika 269, Pomidor, Tomato, Pomidor 270, Og\u00f3rek, Cucumber, Ogorek 271, Sa\u0142ata, Lettuce, Salata 272, Marchewka, Carrot, Markhevka 273, Ziemniak, Potato, Zhemnyak 274, Jab\u0142ko, Apple, Yabko 275, Gruszka, Pear, Grooshka 276, Banan, Banana, Banan 277, Cytryna, Lemon, Tsitryna 278, Pomara\u0144cza, Orange, Pomarancha 279, Mandarynka, Tangerine, Mandarynka 280, Grejpfrut, Grapefruit, Greypfroot 281, Winogrono, Grape, Vinogrono 282, Malina, Raspberry, Malina 283, Truskawka, Strawberry, Trooskafka 284, Wi\u015bnia, Cherry, Veesnya 285, Kszta\u0142t, Shape, Kshtawt 286, S\u0142odki, Sweet, Swodki 287, Kwa\u015bny, Sour, Kwashny 288, Gorzki, Bitter, Gorzki 289, Ostry, Spicy, Ostry 290, S\u0142ony, Salty, Swony 291, Delikatny, Delicate, Delikatny 292, Pod\u0142uzny, Longitudinal, Podwoozhny 293, Okr\u0105g\u0142y, Round, Okrongwy 294, Kwadratowy, Square, Kwadratowy 295, Tr\u00f3jk\u0105tny, Triangular, Trookontny 296, Prostok\u0105tny, Rectangular, Prostokontny 297, Pomara\u0144cza jest kwa\u015bna i okr\u0105g\u0142a, The orange is sour and round, Pomarancha yest kwashna i okrongwa","title":"Notes"},{"location":"Polish/rules/","text":"a is female o and e is child everything else is male How to identify the gender in Polish? \u0142, k, g, h - Rodzaj m\u0119ski a, \u0119, i, y - Rodzaj \u017ce\u0144ski o, e, m - Rodzaj nijaki Jaki is male Jaka is female Jakie is child For human male subjects Use 'tego' for adjectives Use 'a' for nouns Use 'o' for things For human female subjects Use '\u0105' for adjectives Use '\u0119' for nouns 'Wy' - you (plural) - 'Wy jeste\u015bcie' - you are 'My' - We (plural) - 'My jeste\u015bmy' - we are 'Ja' - I (singular) - 'Ja jestem' - I am 'Ty' - You (singular) - 'Ty jeste\u015b' - you are 'On' - He (singular) - 'On mieszka' - he lives 'Ona' - She (singular) - 'Ona mieszka' - she lives 'Ono' - It (singular) - 'Ono mieszka' - it lives 'Oni' - They (plural) - 'Oni mieszkaj\u0105' - they live Adjectives This is dirty - To jest brudne This is clean - To jest czyste He is fat - On jest gruby He is slim - On jest szczup\u0142y He is young - On jest m\u0142ody He is old - On jest stary She is pretty - Ona jest \u0142adna She is ugly - Ona jest brzydka She is sick - Ona jest chora He is healthy - On jest zdrowy This tea is hot - Ta herbata jest gor\u0105ca I am rich - Ja jestem bogata I am poor - Ja jestem biedna This drink is cold - Ten nap\u00f3j jest zimny Are you old? - Czy ty jeste\u015b stary? No I am young - Nie, ja jestem m\u0142ody Polish is easy - Polski jest \u0142atwy Polish is difficult - Polski jest trudny","title":"Rules"},{"location":"Polish/rules/#how-to-identify-the-gender-in-polish","text":"\u0142, k, g, h - Rodzaj m\u0119ski a, \u0119, i, y - Rodzaj \u017ce\u0144ski o, e, m - Rodzaj nijaki Jaki is male Jaka is female Jakie is child For human male subjects Use 'tego' for adjectives Use 'a' for nouns Use 'o' for things For human female subjects Use '\u0105' for adjectives Use '\u0119' for nouns 'Wy' - you (plural) - 'Wy jeste\u015bcie' - you are 'My' - We (plural) - 'My jeste\u015bmy' - we are 'Ja' - I (singular) - 'Ja jestem' - I am 'Ty' - You (singular) - 'Ty jeste\u015b' - you are 'On' - He (singular) - 'On mieszka' - he lives 'Ona' - She (singular) - 'Ona mieszka' - she lives 'Ono' - It (singular) - 'Ono mieszka' - it lives 'Oni' - They (plural) - 'Oni mieszkaj\u0105' - they live","title":"How to identify the gender in Polish?"},{"location":"Polish/rules/#adjectives","text":"This is dirty - To jest brudne This is clean - To jest czyste He is fat - On jest gruby He is slim - On jest szczup\u0142y He is young - On jest m\u0142ody He is old - On jest stary She is pretty - Ona jest \u0142adna She is ugly - Ona jest brzydka She is sick - Ona jest chora He is healthy - On jest zdrowy This tea is hot - Ta herbata jest gor\u0105ca I am rich - Ja jestem bogata I am poor - Ja jestem biedna This drink is cold - Ten nap\u00f3j jest zimny Are you old? - Czy ty jeste\u015b stary? No I am young - Nie, ja jestem m\u0142ody Polish is easy - Polski jest \u0142atwy Polish is difficult - Polski jest trudny","title":"Adjectives"},{"location":"Polish/sentences/","text":"Serial, Polish, English, Rule 1, Lubi\u0119 je\u015b\u0107 bu\u0142ka z mas\u0142em i pi\u0107 herbat\u0119 na \u015bniadanie., I like to eat bread with butter and drink tea for breakfast., 2, Ja wol\u0119 je\u015b\u0107 kanapka z serem, I prefer to eat a sandwich with cheese, With 'Ja' use '\u0119'. 3, Wol\u0119 pi\u0107 indii herbat\u0119 4, Ja lubi\u0119 je\u015b\u0107 kanapka z serem i pi\u0107 herbat\u0119 na \u015bniadanie., 5,","title":"Sentences"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/","text":"Quadratic utility is defined as: $$ U(W) = aW - bW^2 $$ where: - \\(W\\) = wealth, - \\(a, b > 0\\) are constants. Why utility function is assumed this way? If utility is quadratic: $$ U(W) = aW - bW^2 $$ then the expected utility is: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b \\mathbb{E}[W^2] $$ Expand \\(\\mathbb{E}[W^2]\\) using the formula: $$ \\mathbb{E}[W^2] = (\\mathbb{E}[W])^2 + \\text{Var}(W) $$ This comes from the relationship: \\(\\text{Var}(X) = E(X)^2 - E(X^2)\\) thus: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b\\left((\\mathbb{E}[W])^2 + \\text{Var}(W)\\right) $$ Grouping terms: $$ \\mathbb{E}[U(W)] = (a - 2b \\mathbb{E}[W])\\mathbb{E}[W] - b \\, \\text{Var}(W) $$ Interpretation: - You maximize expected utility by increasing expected return \\(\\mathbb{E}[W]\\) - and minimizing variance \\(\\text{Var}(W)\\) . Thus, maximizing expected quadratic utility reduces to a trade-off between expected return and variance , which is exactly the objective of mean-variance optimization . Why Mean Variance Optimization is a projection problem? You are projecting the expected return vector \\(\\alpha\\) onto the weight space, using a distance measured by the covariance matrix \\(\\Omega\\) . The space (portfolio space or weight space) is \\(\\mathbb{R}^n\\) for \\(n\\) assets. Instead of normal Euclidean distance, you use risk-based distance : $$ d(w) = \\sqrt{w^T \\Omega w} $$ This risk-based distance stretches and twists the space \u2014 the geometry is no longer flat like Euclidean space. \"We are finding the weight vector \\(\ud835\udc64\\) that points in the best possible direction towards the return vector \\(\\alpha\\) , but measured using risk distance defined by \\(\\omega\\) . Optimization Problem $$ \\max_w \\quad \\alpha^T w \\quad \\text{subject to} \\quad w^T \\Omega w \\leq \\sigma^2 $$ Lagrangian leads to: $$ \\Omega w \\propto \\alpha \\quad \\Rightarrow \\quad w \\propto \\Omega^{-1} \\alpha $$ Thus, optimal weight is proportional to \\(\\Omega^{-1} \\alpha\\) \u2014 meaning alpha corrected by risk geometry .","title":"Mean Variance Portfolio Optimization"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/#why-utility-function-is-assumed-this-way","text":"If utility is quadratic: $$ U(W) = aW - bW^2 $$ then the expected utility is: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b \\mathbb{E}[W^2] $$ Expand \\(\\mathbb{E}[W^2]\\) using the formula: $$ \\mathbb{E}[W^2] = (\\mathbb{E}[W])^2 + \\text{Var}(W) $$ This comes from the relationship: \\(\\text{Var}(X) = E(X)^2 - E(X^2)\\) thus: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b\\left((\\mathbb{E}[W])^2 + \\text{Var}(W)\\right) $$ Grouping terms: $$ \\mathbb{E}[U(W)] = (a - 2b \\mathbb{E}[W])\\mathbb{E}[W] - b \\, \\text{Var}(W) $$ Interpretation: - You maximize expected utility by increasing expected return \\(\\mathbb{E}[W]\\) - and minimizing variance \\(\\text{Var}(W)\\) . Thus, maximizing expected quadratic utility reduces to a trade-off between expected return and variance , which is exactly the objective of mean-variance optimization .","title":"Why utility function is assumed this way?"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/#why-mean-variance-optimization-is-a-projection-problem","text":"You are projecting the expected return vector \\(\\alpha\\) onto the weight space, using a distance measured by the covariance matrix \\(\\Omega\\) . The space (portfolio space or weight space) is \\(\\mathbb{R}^n\\) for \\(n\\) assets. Instead of normal Euclidean distance, you use risk-based distance : $$ d(w) = \\sqrt{w^T \\Omega w} $$ This risk-based distance stretches and twists the space \u2014 the geometry is no longer flat like Euclidean space. \"We are finding the weight vector \\(\ud835\udc64\\) that points in the best possible direction towards the return vector \\(\\alpha\\) , but measured using risk distance defined by \\(\\omega\\) .","title":"Why Mean Variance Optimization is a projection problem?"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/#optimization-problem","text":"$$ \\max_w \\quad \\alpha^T w \\quad \\text{subject to} \\quad w^T \\Omega w \\leq \\sigma^2 $$ Lagrangian leads to: $$ \\Omega w \\propto \\alpha \\quad \\Rightarrow \\quad w \\propto \\Omega^{-1} \\alpha $$ Thus, optimal weight is proportional to \\(\\Omega^{-1} \\alpha\\) \u2014 meaning alpha corrected by risk geometry .","title":"Optimization Problem"},{"location":"Quant%20Finance/Factor%20Mimicking%20Portfolio/","text":"Daily asset returns r_{i,t} for a liquid universe. Daily VIX return f_{\\text{VIX},t} and any other observed factors f_{\\ell,t}. Estimate betas (time-series regressions) r_{i,t} \\;=\\; \\beta_{i,\\text{VIX}}\\,f_{\\text{VIX},t}\\;+\\;\\sum_{\\ell\\neq\\text{VIX}}\\beta_{i,\\ell}\\,f_{\\ell,t}\\;+\\;\\varepsilon_{i,t}. Collect all \\beta_{i,\\cdot} into the loading matrix B and The phrase means: once you\u2019ve run each time\u2010series regression r_{i,t} \\;=\\;\\sum_{\\ell}\\beta_{i,\\ell}\\,f_{\\ell,t}\\;+\\;\\varepsilon_{i,t}, you collect all \\beta_{i,\\ell} into one matrix \\[ B = \\begin{pmatrix} \\beta_{1,1} & \\beta_{1,2} & \\cdots\\\\ \\beta_{2,1} & \\beta_{2,2} & \\cdots\\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix}, \\] and you form the vector of residuals for each asset \\varepsilon_{i,t}. Then \\Omega_{\\varepsilon} \\;=\\; \\operatorname{Cov}\\bigl(\\varepsilon_{t}\\bigr) is the N\\times N covariance matrix whose (i,j) entry is \\mathbb{E}[\\varepsilon_{i,t}\\,\\varepsilon_{j,t}].","title":"Factor Mimicking Portfolio"},{"location":"Quant%20Finance/GARCH%20%281%2C1%29/","text":"The general principle is to model volatility as a time-varying, latent stochastic process whose current value depends on past information.","title":"GARCH (1,1)"},{"location":"Quant%20Finance/Stylized%20Facts%20for%20Return%20Distribution/","text":"Stylized facts are empirical features of financial return data Lack of Autocorrelation in Returns Daily returns typically show little to no correlation with their past values. Heavy Tails Return distributions exhibit more frequent extreme outcomes (both positive and negative) than would be expected under a normal distribution. This property is known as \u201cheavy tails\u201d or \u201cleptokurtosis.\u201d In practice, this means large losses or gains happen more often than simple models assume, which has implications for risk management. Volatility Clustering Periods of high volatility tend to follow other high-volatility periods, and the same applies to low-volatility periods. Although returns themselves are not significantly correlated, the magnitude or intensity of returns tends to be. This persistence in volatility is known as volatility clustering, and it suggests that volatility is time-varying rather than constant. Aggregational Gaussianity Although daily returns are not normally distributed, when returns are aggregated over longer time periods (such as weeks or months), their distribution tends to look more like a normal distribution.","title":"Stylized Facts for Return Distribution"},{"location":"Rust/Async%20Rust/","text":"What is a runtime? Tokio is the async runtime for Rust. Think: thread pool + task scheduler + event loop. It drives your futures to completion. It gives you #[tokio::main] to start your async main() . It provides async I/O (files, network, timers, etc.). Tokio's Job: Poll your futures repeatedly . Wait for external events (e.g., HTTP done, file ready). Wake up your future and call poll() again only when needed . One-liner intuition: poll() = \"Are you done yet?\" Future: \"No.\" \u2192 Pending Future: \"Yes!\" \u2192 Ready(output) FuturesUnordered","title":"Async Rust"},{"location":"Rust/Async%20Rust/#what-is-a-runtime","text":"Tokio is the async runtime for Rust. Think: thread pool + task scheduler + event loop. It drives your futures to completion. It gives you #[tokio::main] to start your async main() . It provides async I/O (files, network, timers, etc.).","title":"What is a runtime?"},{"location":"Rust/Async%20Rust/#tokios-job","text":"Poll your futures repeatedly . Wait for external events (e.g., HTTP done, file ready). Wake up your future and call poll() again only when needed .","title":"Tokio's Job:"},{"location":"Rust/Async%20Rust/#one-liner-intuition","text":"poll() = \"Are you done yet?\" Future: \"No.\" \u2192 Pending Future: \"Yes!\" \u2192 Ready(output)","title":"One-liner intuition:"},{"location":"Rust/Async%20Rust/#futuresunordered","text":"","title":"FuturesUnordered"},{"location":"Rust/Rust%20Jupyter%20Notebooks/","text":"1 2 cargo install evcxr_jupyter evcxr_jupyter --install Select Rust Jupyter Kernel in VSCode Caveats Dependencies are added via :dep <DEPENDENCY_NAME> = \"<VERSION_NAME> and are compiled everytime you restart kernel. Keep all dependencies in separate cell.","title":"Rust Jupyter Notebooks"},{"location":"Rust/Rust%20Jupyter%20Notebooks/#caveats","text":"Dependencies are added via :dep <DEPENDENCY_NAME> = \"<VERSION_NAME> and are compiled everytime you restart kernel. Keep all dependencies in separate cell.","title":"Caveats"},{"location":"Rust/Tying%20Traits%20to%20Lifetime/","text":"Every method in this trait that hands out or consumes references is chained to a specific borrow duration. 1 2 3 4 5 6 7 struct Person <' a > { name : & ' a str , } trait Named { fn name ( & self ) -> & str ; } // Compiler will shout an error if you implement Named on person 1 2 3 trait Named <' a > { fn name ( & self ) -> & ' a str ; } Now the methods in the trait are parameterized with a lifetime parameter. Now we\u2019re saying: \u201cWhen something implements Named<'a> , its name() method gives back a string slice that lives as long as 'a .\u201d","title":"Tying Traits to Lifetime"},{"location":"Spells/Rsync/","text":"Synchronise files in Linux 1 rsync -av --exclude-from=.gitignore path1/ path2/","title":"Rsync"},{"location":"Spells/Rsync/#synchronise-files-in-linux","text":"1 rsync -av --exclude-from=.gitignore path1/ path2/","title":"Synchronise files in Linux"},{"location":"Untitled/Graphs/","text":"A beautiful way to recurse in Python 1 2 3 def find_group_leader ( node : int ) -> int : # If node is its own leader, return it; otherwise, find the leader recursively return node if group_leader [ node ] == node else find_group_leader ( group_leader [ node ])","title":"Graphs"}]}