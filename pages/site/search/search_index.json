{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"I am a design-loving nerd, navigating the intersection of technology, mathematics and art, on this wild ride of figuring out ways to do something meaningful. Nowadays, I am deeply interested in the intersection of machine learning and computational finance. Econometrics and Time-Series Analysis are my craft and thus I hunt patterns and weave stories with data for a living. I speak Hindi and English to humans, while Python and Javascript to computers. I also authored a Python library that facilitates comparison and testing of dataframes. I am currently learning deep learning, financial engineering and to speak Polish, C++ and Rust. In my free time, I work as a Pricing Quant at HSBC.","title":"Home"},{"location":"Affirmations/","text":"A list of timeless reminders about how l aim to be or just how life seems to work. FOCUS ON FASTER ITERATION AND NOT ON ACCURACY It's important to do things fast. A week is 2% of the year. Time is the denominator. Motion creates information. You learn more per unit time because you make contact with reality more frequently. Optimize for being spectacularly right some of the time, and low-stakes wrong a lot of the time. The goal is not to avoid mistakes; the goal is to achieve uncorrelated levels of excellence in some dimension. The downsides are worth it. GO TOUCH THE METAL Hard skills are super important. Get good with your tools. I mean, really good. Get rid of all the friction. Keep oiling your tools and don't let them rust. Don't get lost in the busy bandwagon of managerial or bureaucratic duties. Those create empty suits. GET GOOD AT SMALL TALK Soft skills are even more important. Sapiens are emotional species. They cannot think objectively. Get good at telling stories. Get good at sales. Listen at least thrice more than you speak. Better to get your dopamine from improving your ideas than from having them validated If right things are said in a wrong way, they lose their righteousness. And yes! when deep into shit, keep your mouth shut. INSPIRATION IS PERISHABLE. ACT ON IT. Start before you are ready Thousands have the same idea, which you think is original and exclusive to you. Thinking is easy, doing is hard! Only execution matters. Ship it! LIFE ISN'T UNFAIR, IT IS DISPROPORTIONATELY UNFAIR. The path isn't linear. It is a Geometric Brownian motion with a drift. Make sure that the trend coefficient is always positive. There are people who are way more smarter and hardworking than you are, but are not sitting on the privileges that you have. And vice verca. At the age of 25, it\u2019s very tempting to conflate agency and determinism in scripts. You have a default state \u201cI\u2019m a smart guy, I have these resources at hand. The future looks full of possibilities.\" And then life will hit you hard on face. Efficient Market Hypothesis is a flawed heuristic. WE ARE MONKEYS. WE COPY. You will always feel an overpowering urge to fit in and follow the herd. It's natural! It is the evolutionary make-up of your brain that you inherited from your primordial ancestors. The best thing you can do is that make sure that the herd around you is smarter and more aware than you are. HAVE SELF BELIEF TO THE POINT OF DELUSION You can do more than you think. Always try to know to what extent you are operating in a condition of learned helplessness in institutionalized environments. You are tied down by your own invisible orthodoxy. The laws of physics are the only limit And do not get trapped at local maxima. IT IS BETTER TO TRAVEL WELL THAN TO ARRIVE Life is happening, right now. There is no past. There is no future. Don't forget that you have limitless capability to respond. Think about happiness in 10-20 years from now - hedonic (here and now) and eudaimonic (long term meaning & satisfaction); it's hard to have both - one has to be strategic, lucky, energetic and self-aware to be in a spot where you can have both. RAISE THE CEILING, AND NEVER THE FLOOR A lot of games glamourized by the society aren't actually worth winning at all. And yes, most important! Ego is the enemy. It is self defeating and depression inducing. The higher you go, the harder it will get to contain your ego. Never forget, y ou are a mere mortal chimp with a plan and also without a tail. I love you!","title":"Affirmations"},{"location":"Affirmations/#focus-on-faster-iteration-and-not-on-accuracy","text":"It's important to do things fast. A week is 2% of the year. Time is the denominator. Motion creates information. You learn more per unit time because you make contact with reality more frequently. Optimize for being spectacularly right some of the time, and low-stakes wrong a lot of the time. The goal is not to avoid mistakes; the goal is to achieve uncorrelated levels of excellence in some dimension. The downsides are worth it.","title":"FOCUS ON FASTER ITERATION AND NOT ON ACCURACY"},{"location":"Affirmations/#go-touch-the-metal","text":"Hard skills are super important. Get good with your tools. I mean, really good. Get rid of all the friction. Keep oiling your tools and don't let them rust. Don't get lost in the busy bandwagon of managerial or bureaucratic duties. Those create empty suits.","title":"GO TOUCH THE METAL"},{"location":"Affirmations/#get-good-at-small-talk","text":"Soft skills are even more important. Sapiens are emotional species. They cannot think objectively. Get good at telling stories. Get good at sales. Listen at least thrice more than you speak. Better to get your dopamine from improving your ideas than from having them validated If right things are said in a wrong way, they lose their righteousness. And yes! when deep into shit, keep your mouth shut.","title":"GET GOOD AT SMALL TALK"},{"location":"Affirmations/#inspiration-is-perishable-act-on-it","text":"Start before you are ready Thousands have the same idea, which you think is original and exclusive to you. Thinking is easy, doing is hard! Only execution matters. Ship it!","title":"INSPIRATION IS PERISHABLE. ACT ON IT."},{"location":"Affirmations/#life-isnt-unfair-it-is-disproportionately-unfair","text":"The path isn't linear. It is a Geometric Brownian motion with a drift. Make sure that the trend coefficient is always positive. There are people who are way more smarter and hardworking than you are, but are not sitting on the privileges that you have. And vice verca. At the age of 25, it\u2019s very tempting to conflate agency and determinism in scripts. You have a default state \u201cI\u2019m a smart guy, I have these resources at hand. The future looks full of possibilities.\" And then life will hit you hard on face. Efficient Market Hypothesis is a flawed heuristic.","title":"LIFE ISN'T UNFAIR, IT IS DISPROPORTIONATELY UNFAIR."},{"location":"Affirmations/#we-are-monkeys-we-copy","text":"You will always feel an overpowering urge to fit in and follow the herd. It's natural! It is the evolutionary make-up of your brain that you inherited from your primordial ancestors. The best thing you can do is that make sure that the herd around you is smarter and more aware than you are.","title":"WE ARE MONKEYS. WE COPY."},{"location":"Affirmations/#have-self-belief-to-the-point-of-delusion","text":"You can do more than you think. Always try to know to what extent you are operating in a condition of learned helplessness in institutionalized environments. You are tied down by your own invisible orthodoxy. The laws of physics are the only limit And do not get trapped at local maxima.","title":"HAVE SELF BELIEF TO THE POINT OF DELUSION"},{"location":"Affirmations/#it-is-better-to-travel-well-than-to-arrive","text":"Life is happening, right now. There is no past. There is no future. Don't forget that you have limitless capability to respond. Think about happiness in 10-20 years from now - hedonic (here and now) and eudaimonic (long term meaning & satisfaction); it's hard to have both - one has to be strategic, lucky, energetic and self-aware to be in a spot where you can have both.","title":"IT IS BETTER TO TRAVEL WELL THAN TO ARRIVE"},{"location":"Affirmations/#raise-the-ceiling-and-never-the-floor","text":"A lot of games glamourized by the society aren't actually worth winning at all. And yes, most important! Ego is the enemy. It is self defeating and depression inducing. The higher you go, the harder it will get to contain your ego. Never forget, y ou are a mere mortal chimp with a plan and also without a tail. I love you!","title":"RAISE THE CEILING, AND NEVER THE FLOOR"},{"location":"CFA%20Notes/","text":"What is CEO duality? What is common law? What is the theory of stakeholder interest?","title":"CFA Notes"},{"location":"What%20is%20Price%3F/","text":"","title":"What is Price?"},{"location":"Arts/Art%20of%20Git/","text":"Resolve-All-Conflicts-by-Choosing-Ours (Cherry-Pick) Tell Git: list conflicted files, keep our side in every one, stage the lot, and push the cherry-pick forward What it does, step by step git diff --name-only --diff-filter=U \u2014 list files currently in conflict. For each file, git checkout --ours \u2014 discard \u201ctheirs,\u201d keep ours . git add -A \u2014 mark all as resolved. git cherry-pick --continue \u2014 resume the cherry-pick as if nothing happened. Use this when you intentionally want your branch to win every conflict. If you don\u2019t mean it, don\u2019t run it. PowerShell 1 2 3 git diff - -name-only - -diff-filter = U | ForEach -Object { git checkout - -ours $_ } git add -A git cherry-pick - -continue Bash 1 2 3 git diff --name-only --diff-filter = U | xargs -I {} git checkout --ours \"{}\" git add -A git cherry-pick --continue Difference between merge and rebase The key distinction is that merge preserves history , while rebase rewrites it . What is history?: In Git, history refers to the complete record of all commits and the relationships between them. It's the chronological sequence of changes that have been made to a repository over time. Merge vs Rebase \u2014 The Only Difference That Matters Merge : preserves history. Adds a merge commit. Rebase : rewrites history. Pretends your work always started on the new base. Git Merge \u2014 Preserves History Takes the common ancestor, slams both branches together, makes a new merge commit. History becomes a fork and join. Messy, but transparent. Use when the branch is shared \u2014 you want a paper trail. Visual Merge (messy but honest): 1 2 3 4 5 A---B---C (main) \\ D---E (feature) \\ F (merge commit) Rebase (clean but fake): 1 2 3 A---B---C (main) \\ D'---E' (feature rebased)","title":"Art of Git"},{"location":"Arts/Art%20of%20Git/#resolve-all-conflicts-by-choosing-ours-cherry-pick","text":"Tell Git: list conflicted files, keep our side in every one, stage the lot, and push the cherry-pick forward What it does, step by step git diff --name-only --diff-filter=U \u2014 list files currently in conflict. For each file, git checkout --ours \u2014 discard \u201ctheirs,\u201d keep ours . git add -A \u2014 mark all as resolved. git cherry-pick --continue \u2014 resume the cherry-pick as if nothing happened. Use this when you intentionally want your branch to win every conflict. If you don\u2019t mean it, don\u2019t run it.","title":"Resolve-All-Conflicts-by-Choosing-Ours (Cherry-Pick)"},{"location":"Arts/Art%20of%20Git/#powershell","text":"1 2 3 git diff - -name-only - -diff-filter = U | ForEach -Object { git checkout - -ours $_ } git add -A git cherry-pick - -continue","title":"PowerShell"},{"location":"Arts/Art%20of%20Git/#bash","text":"1 2 3 git diff --name-only --diff-filter = U | xargs -I {} git checkout --ours \"{}\" git add -A git cherry-pick --continue","title":"Bash"},{"location":"Arts/Art%20of%20Git/#difference-between-merge-and-rebase","text":"The key distinction is that merge preserves history , while rebase rewrites it . What is history?: In Git, history refers to the complete record of all commits and the relationships between them. It's the chronological sequence of changes that have been made to a repository over time.","title":"Difference between merge and rebase"},{"location":"Arts/Art%20of%20Git/#merge-vs-rebase-the-only-difference-that-matters","text":"Merge : preserves history. Adds a merge commit. Rebase : rewrites history. Pretends your work always started on the new base.","title":"Merge vs Rebase \u2014 The Only Difference That Matters"},{"location":"Arts/Art%20of%20Git/#git-merge-preserves-history","text":"Takes the common ancestor, slams both branches together, makes a new merge commit. History becomes a fork and join. Messy, but transparent. Use when the branch is shared \u2014 you want a paper trail.","title":"Git Merge \u2014 Preserves History"},{"location":"Arts/Art%20of%20Git/#visual","text":"Merge (messy but honest): 1 2 3 4 5 A---B---C (main) \\ D---E (feature) \\ F (merge commit) Rebase (clean but fake): 1 2 3 A---B---C (main) \\ D'---E' (feature rebased)","title":"Visual"},{"location":"Arts/Art%20of%20Shell/","text":"xargs xargs is plumbing. It takes a stream of text, splits it on whitespace, and bolts the pieces onto another command. By default it spits them into echo , but the moment you point it at something real \u2014 rm , git , whatever \u2014 it turns into a batch-launcher. For example: 1 git diff --name-only --diff-filter = U | xargs -I {} git checkout --ours {} This one-liner pulls out every conflicted file and runs git checkout --ours on each.","title":"Art of Shell"},{"location":"Arts/Art%20of%20Shell/#xargs","text":"xargs is plumbing. It takes a stream of text, splits it on whitespace, and bolts the pieces onto another command. By default it spits them into echo , but the moment you point it at something real \u2014 rm , git , whatever \u2014 it turns into a batch-launcher. For example: 1 git diff --name-only --diff-filter = U | xargs -I {} git checkout --ours {} This one-liner pulls out every conflicted file and runs git checkout --ours on each.","title":"xargs"},{"location":"Arts/Async%20Programming/","text":"Event Loop At the heart of async is something called the event loop. Think of it as a super-organized waiter in a restaurant. This waiter doesn\u2019t hover at one table until the food\u2019s eaten\u2014they zip around, checking which tables need attention. The event loop does the same: it\u2019s a constant cycle that monitors tasks, schedules them when they\u2019re ready to run, and handles their results when they\u2019re done. Why is it called a loop? while true { 5. Check for ready tasks 6. Run them 7. Check for new events (network data, timers, etc.) 8. Go back to step 1 }","title":"Async Programming"},{"location":"Arts/Async%20Programming/#event-loop","text":"At the heart of async is something called the event loop. Think of it as a super-organized waiter in a restaurant. This waiter doesn\u2019t hover at one table until the food\u2019s eaten\u2014they zip around, checking which tables need attention. The event loop does the same: it\u2019s a constant cycle that monitors tasks, schedules them when they\u2019re ready to run, and handles their results when they\u2019re done. Why is it called a loop? while true { 5. Check for ready tasks 6. Run them 7. Check for new events (network data, timers, etc.) 8. Go back to step 1 }","title":"Event Loop"},{"location":"Books/The%20Zen%20and%20the%20Art%20of%20Motorcycle%20Maintenance/","text":"meaning isn\u2019t something you \u201cfind out there.\u201d It\u2019s in the act of caring. When you adjust a carburetor with real attention, or write a line that feels right, that care is meaning. He calls it \u201cQuality,\u201d but really it\u2019s that moment where you\u2019re fully engaged and the world feels aligned. If you care, it\u2019s alive. If you don\u2019t, it\u2019s empty.","title":"The Zen and the Art of Motorcycle Maintenance"},{"location":"C%2B%2B/Concepts/","text":"Difference between Pointers and References Pointers Typename* It doesn\u2019t hold an object, it holds an address. A mailbox, not the letter. To use what\u2019s inside, you dereference it with * or -> . That\u2019s why you see it in linked lists, tries, or anything that needs \u201cmaybe there, maybe not\u201d semantics. You can reassign it. You can set it to nullptr . You can leak memory if you forget to clean up. Example: 1 2 TypeName * ptr = new TypeName (); ptr -> data = 5 ; Here you carved out memory on the heap, stashed its address in ptr , and poked at it with -> . Pass &obj into a function expecting a TypeName* and you\u2019re literally saying: here\u2019s where the object lives. References TypeName& TypeName& is stricter. It\u2019s a reference, an alias welded onto one existing object. No nulls, no rebinding. You treat it like the object itself\u2014just obj.data , no pointer gymnastics. That makes it perfect for function parameters: 1 void update ( TypeName & obj ) { obj . data = 10 ; } The compiler guarantees you\u2019re not dealing with a ghost. No null checks, no dangling reassignments. The confusion comes from the symbols: * and & moonlight as both declaration operators and runtime operators. * means \u201cthis is a pointer type\u201d and also \u201cdereference this pointer.\u201d & means \u201cthis is a reference type\u201d and also \u201cgive me the address of this object.\u201d Same glyph, different roles. 1 2 3 int n = 0 int & r = n ; // You have direct access to r through n. int * p = & n ; // You need to deref p to access n. Uninitialized Locals = Garbage In c++, a local variable without an initializer doesn\u2019t start at zero \u2014 it just inherits whatever junk was left in that memory slot. 1 2 3 4 5 6 7 8 9 10 #include <iostream> using namespace std ; int main () { int x ; // uninitialized cout << x ; // prints garbage (leftover stack data) int y = 0 ; // explicitly initialized cout << \" \\n \" << y ; // always prints 0 } locals live on the stack, and the compiler won\u2019t wipe the stack clean for you. that\u2019s why x can look like a random number. globals/statics get zeroed by the runtime, but locals are your responsibility. rule: if you care about the value, initialize it. always. Operator Overloading Operator overloading is just teaching C++ what everyday symbols mean for your custom types. By default, + works on ints, not on your Vector2D . 1 2 3 Vector2D operator + ( const Vector2D & a , const Vector2D & b ) { return { a . x + b . x , a . y + b . y }; } Vector2D at the start \u2192 this function spits out a Vector2D . That\u2019s the result of a + b . operator+ \u2192 the magic name. You\u2019re telling the compiler \u201cwhen someone writes + between two Vector2D s, call this.\u201d (const Vector2D& a, const Vector2D& b) \u2192 both operands come in as references (no copying), locked as const (you promise not to mutate them). Inside: return {a.x + b.x, a.y + b.y}; \u2192 builds a new Vector2D whose components are just the sum of the inputs. No side effects, no hidden tricks. You give it two vectors, you get back their sum. Exactly how + should feel. Static Members \u2014 One Definition, Always Global In C++, static data members are a two-step ritual. You declare them inside the class to announce their existence. But that\u2019s just a promise, not storage. To actually give them memory, you must define them once in the global scope \u2014 outside any function, method, or class body. The declaration says \u201cevery object shares this.\u201d The definition says \u201chere\u2019s where it actually lives.\u201d 1 2 3 4 5 6 #include <string> ; struct Human { static std :: string species ; // declared inside class }; std :: string Human :: species = \"Homo Sapien\" ; // defined once, in global scope Every Coord points to the same len_x . No duplication, no per-object copies. Static methods obey the same law. Declared inside the class, defined outside with ClassName:: . One definition, global, or the linker will scream. This is not optional. It\u2019s the rule.","title":"Concepts"},{"location":"C%2B%2B/Concepts/#difference-between-pointers-and-references","text":"","title":"Difference between Pointers and References"},{"location":"C%2B%2B/Concepts/#pointers-typename","text":"It doesn\u2019t hold an object, it holds an address. A mailbox, not the letter. To use what\u2019s inside, you dereference it with * or -> . That\u2019s why you see it in linked lists, tries, or anything that needs \u201cmaybe there, maybe not\u201d semantics. You can reassign it. You can set it to nullptr . You can leak memory if you forget to clean up. Example: 1 2 TypeName * ptr = new TypeName (); ptr -> data = 5 ; Here you carved out memory on the heap, stashed its address in ptr , and poked at it with -> . Pass &obj into a function expecting a TypeName* and you\u2019re literally saying: here\u2019s where the object lives.","title":"Pointers Typename*"},{"location":"C%2B%2B/Concepts/#references-typename","text":"TypeName& is stricter. It\u2019s a reference, an alias welded onto one existing object. No nulls, no rebinding. You treat it like the object itself\u2014just obj.data , no pointer gymnastics. That makes it perfect for function parameters: 1 void update ( TypeName & obj ) { obj . data = 10 ; } The compiler guarantees you\u2019re not dealing with a ghost. No null checks, no dangling reassignments. The confusion comes from the symbols: * and & moonlight as both declaration operators and runtime operators. * means \u201cthis is a pointer type\u201d and also \u201cdereference this pointer.\u201d & means \u201cthis is a reference type\u201d and also \u201cgive me the address of this object.\u201d Same glyph, different roles. 1 2 3 int n = 0 int & r = n ; // You have direct access to r through n. int * p = & n ; // You need to deref p to access n.","title":"References TypeName&amp;"},{"location":"C%2B%2B/Concepts/#uninitialized-locals-garbage","text":"In c++, a local variable without an initializer doesn\u2019t start at zero \u2014 it just inherits whatever junk was left in that memory slot. 1 2 3 4 5 6 7 8 9 10 #include <iostream> using namespace std ; int main () { int x ; // uninitialized cout << x ; // prints garbage (leftover stack data) int y = 0 ; // explicitly initialized cout << \" \\n \" << y ; // always prints 0 } locals live on the stack, and the compiler won\u2019t wipe the stack clean for you. that\u2019s why x can look like a random number. globals/statics get zeroed by the runtime, but locals are your responsibility. rule: if you care about the value, initialize it. always.","title":"Uninitialized Locals = Garbage"},{"location":"C%2B%2B/Concepts/#operator-overloading","text":"Operator overloading is just teaching C++ what everyday symbols mean for your custom types. By default, + works on ints, not on your Vector2D . 1 2 3 Vector2D operator + ( const Vector2D & a , const Vector2D & b ) { return { a . x + b . x , a . y + b . y }; } Vector2D at the start \u2192 this function spits out a Vector2D . That\u2019s the result of a + b . operator+ \u2192 the magic name. You\u2019re telling the compiler \u201cwhen someone writes + between two Vector2D s, call this.\u201d (const Vector2D& a, const Vector2D& b) \u2192 both operands come in as references (no copying), locked as const (you promise not to mutate them). Inside: return {a.x + b.x, a.y + b.y}; \u2192 builds a new Vector2D whose components are just the sum of the inputs. No side effects, no hidden tricks. You give it two vectors, you get back their sum. Exactly how + should feel.","title":"Operator Overloading"},{"location":"C%2B%2B/Concepts/#static-members-one-definition-always-global","text":"In C++, static data members are a two-step ritual. You declare them inside the class to announce their existence. But that\u2019s just a promise, not storage. To actually give them memory, you must define them once in the global scope \u2014 outside any function, method, or class body. The declaration says \u201cevery object shares this.\u201d The definition says \u201chere\u2019s where it actually lives.\u201d 1 2 3 4 5 6 #include <string> ; struct Human { static std :: string species ; // declared inside class }; std :: string Human :: species = \"Homo Sapien\" ; // defined once, in global scope Every Coord points to the same len_x . No duplication, no per-object copies. Static methods obey the same law. Declared inside the class, defined outside with ClassName:: . One definition, global, or the linker will scream. This is not optional. It\u2019s the rule.","title":"Static Members \u2014 One Definition, Always Global"},{"location":"C%2B%2B/GDB/","text":"Logging 1 2 3 4 5 set logging on set logging file gdb_output.txt set logging overwrite on # optional, overwrites existing file set logging redirect on # No echoing on terminal set logging off Breaking Execution Function Call 1 break function_name 1 break file.c:function_name 1 break 'Class::Method(Type)' 1 2 set breakpoint pending on break function_name Specific Line 1 break file . c : 42 1 break 42 When Symbol Comes into Scope 1 rbreak symbol_name When Symbol Changes Value 1 2 3 watch variable # on write rwatch variable # on read awatch variable # on read/write Requires variable to have a stable memory address (e.g., global or in-scope local). I/O Set Pretty Printing 1 set print pretty on 1 set print elements N Limits number of elements printed (default: 200). Dump Binary Memory 1 dump binary memory filename start_addr end_addr Example: 1 dump binary memory dump.bin 0x600000 0x601000 Note Use pointer arithmetic to calculate start and end addresses A useful use-case is that you can dump containers to read them back into python 1 2 3 set $start = &v[0] set $end = $start + v.size() dump binary memory <FILE_NAME>.bin $start $end TL;DR (First Principles): When GDB executes a program, it maps your variables\u2014including arrays and matrices\u2014into memory. You can tell GDB to dump raw memory of any variable to a binary file. This lets you analyze the data outside GDB (e.g., with Python + NumPy) without GDB choking on printing large data inline. 1. Understand What You're Dumping You need: - The starting memory address ( &my_matrix ) - The exact byte size to dump ( sizeof(my_matrix) ) - Example: for a float my_matrix[100][100] : 1 sizeof ( my_matrix ) = 100 * 100 * sizeof ( float ) = 40000 bytes 2. Basic Syntax 1 dump binary memory < filename > < start_address > < end_address > Or if you know the size: 1 dump binary memory matrix.bin &my_matrix &my_matrix + sizeof(my_matrix) But GDB doesn't support pointer arithmetic like that directly on sizeof . Instead: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000 (You can replace 40000 with any exact byte count) Or calculate in GDB: 1 gdb p sizeof(my_matrix) Then manually use the value: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000 What Happens Internally GDB interprets the address &my_matrix as a raw memory location. It writes the raw byte sequence (just 1s and 0s) to matrix.bin . Load It in Python (Post-GDB) 1 2 3 4 5 import numpy as np data = np . fromfile ( \"matrix.bin\" , dtype = np . float32 ) matrix = data . reshape (( 100 , 100 )) print ( matrix ) Make sure dtype and shape match your C++ matrix layout. \ud83e\uddea Sanity Check in GDB Want to confirm it\u2019s dumping the correct values? 1 x/10f &my_matrix[0][0] # examine 10 floats Then check if those match the first line of matrix[0][:10] in NumPy. Pitfalls: Don\u2019t dump pointers unless they point to contiguous memory blocks. Avoid structs with padding unless you understand layout. Beware of endian issues if moving between architectures.","title":"GDB"},{"location":"C%2B%2B/GDB/#logging","text":"1 2 3 4 5 set logging on set logging file gdb_output.txt set logging overwrite on # optional, overwrites existing file set logging redirect on # No echoing on terminal set logging off","title":"Logging"},{"location":"C%2B%2B/GDB/#breaking-execution","text":"","title":"Breaking Execution"},{"location":"C%2B%2B/GDB/#function-call","text":"1 break function_name 1 break file.c:function_name 1 break 'Class::Method(Type)' 1 2 set breakpoint pending on break function_name","title":"Function Call"},{"location":"C%2B%2B/GDB/#specific-line","text":"1 break file . c : 42 1 break 42","title":"Specific Line"},{"location":"C%2B%2B/GDB/#when-symbol-comes-into-scope","text":"1 rbreak symbol_name","title":"When Symbol Comes into Scope"},{"location":"C%2B%2B/GDB/#when-symbol-changes-value","text":"1 2 3 watch variable # on write rwatch variable # on read awatch variable # on read/write Requires variable to have a stable memory address (e.g., global or in-scope local).","title":"When Symbol Changes Value"},{"location":"C%2B%2B/GDB/#io","text":"","title":"I/O"},{"location":"C%2B%2B/GDB/#set-pretty-printing","text":"1 set print pretty on 1 set print elements N Limits number of elements printed (default: 200).","title":"Set Pretty Printing"},{"location":"C%2B%2B/GDB/#dump-binary-memory","text":"1 dump binary memory filename start_addr end_addr Example: 1 dump binary memory dump.bin 0x600000 0x601000 Note Use pointer arithmetic to calculate start and end addresses A useful use-case is that you can dump containers to read them back into python 1 2 3 set $start = &v[0] set $end = $start + v.size() dump binary memory <FILE_NAME>.bin $start $end","title":"Dump Binary Memory"},{"location":"C%2B%2B/GDB/#tldr-first-principles","text":"When GDB executes a program, it maps your variables\u2014including arrays and matrices\u2014into memory. You can tell GDB to dump raw memory of any variable to a binary file. This lets you analyze the data outside GDB (e.g., with Python + NumPy) without GDB choking on printing large data inline.","title":"TL;DR (First Principles):"},{"location":"C%2B%2B/GDB/#1-understand-what-youre-dumping","text":"You need: - The starting memory address ( &my_matrix ) - The exact byte size to dump ( sizeof(my_matrix) ) - Example: for a float my_matrix[100][100] : 1 sizeof ( my_matrix ) = 100 * 100 * sizeof ( float ) = 40000 bytes","title":"1. Understand What You're Dumping"},{"location":"C%2B%2B/GDB/#2-basic-syntax","text":"1 dump binary memory < filename > < start_address > < end_address > Or if you know the size: 1 dump binary memory matrix.bin &my_matrix &my_matrix + sizeof(my_matrix) But GDB doesn't support pointer arithmetic like that directly on sizeof . Instead: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000 (You can replace 40000 with any exact byte count) Or calculate in GDB: 1 gdb p sizeof(my_matrix) Then manually use the value: 1 dump binary memory matrix.bin &my_matrix (&my_matrix)+40000","title":"2. Basic Syntax"},{"location":"C%2B%2B/GDB/#what-happens-internally","text":"GDB interprets the address &my_matrix as a raw memory location. It writes the raw byte sequence (just 1s and 0s) to matrix.bin .","title":"What Happens Internally"},{"location":"C%2B%2B/GDB/#load-it-in-python-post-gdb","text":"1 2 3 4 5 import numpy as np data = np . fromfile ( \"matrix.bin\" , dtype = np . float32 ) matrix = data . reshape (( 100 , 100 )) print ( matrix ) Make sure dtype and shape match your C++ matrix layout.","title":"Load It in Python (Post-GDB)"},{"location":"C%2B%2B/GDB/#sanity-check-in-gdb","text":"Want to confirm it\u2019s dumping the correct values? 1 x/10f &my_matrix[0][0] # examine 10 floats Then check if those match the first line of matrix[0][:10] in NumPy. Pitfalls: Don\u2019t dump pointers unless they point to contiguous memory blocks. Avoid structs with padding unless you understand layout. Beware of endian issues if moving between architectures.","title":"\ud83e\uddea Sanity Check in GDB"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/","text":"1. Node Structure Raw Pointer Version 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 template < typename T > struct Node { T data ; Node * next ; Node ( T value ) : data ( value ), next ( nullptr ) {} }; ```` ### Smart Pointer Version (`std::unique_ptr`) ``` cpp #include <memory> template < typename T > struct Node { T data ; std :: unique_ptr < Node < T >> next ; Node ( T value ) : data ( value ), next ( nullptr ) {} }; 2. Pointer Access -> is used to access members through a pointer. . is used to access members of an object. Example: cpp Node<int>* p = new Node<int>(10); p->data = 20; // modifies value via pointer 3. Why nullptr ? Safer than NULL or 0 . Type-safe and prevents uninitialised access. 4. std::unique_ptr Benefits Manages memory automatically through reference counting. Enforces single ownership. Prevents memory leaks. Use .get() to obtain raw pointer without transferring ownership. 5. Creating a Linked List with 5 Elements 1 2 3 4 5 6 7 auto head = std :: make_unique < Node < int >> ( 1 ); Node < int >* current = head . get (); for ( int i = 2 ; i <= 5 ; ++ i ) { current -> next = std :: make_unique < Node < int >> ( i ); current = current -> next . get (); } head owns the list. current is a non-owning raw pointer used to traverse and build the list. 6. .get() Explained Returns raw pointer from unique_ptr without giving up ownership. Use it when you need temporary access: cpp current = current->next.get(); Final List Constructed list: 1 -> 2 -> 3 -> 4 -> 5 (automatically cleaned up when head goes out of scope).","title":"Linked List in C++ with Smart Pointers"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#1-node-structure","text":"","title":"1. Node Structure"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#raw-pointer-version","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 template < typename T > struct Node { T data ; Node * next ; Node ( T value ) : data ( value ), next ( nullptr ) {} }; ```` ### Smart Pointer Version (`std::unique_ptr`) ``` cpp #include <memory> template < typename T > struct Node { T data ; std :: unique_ptr < Node < T >> next ; Node ( T value ) : data ( value ), next ( nullptr ) {} };","title":"Raw Pointer Version"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#2-pointer-access","text":"-> is used to access members through a pointer. . is used to access members of an object. Example: cpp Node<int>* p = new Node<int>(10); p->data = 20; // modifies value via pointer","title":"2. Pointer Access"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#3-why-nullptr","text":"Safer than NULL or 0 . Type-safe and prevents uninitialised access.","title":"3. Why nullptr?"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#4-stdunique_ptr-benefits","text":"Manages memory automatically through reference counting. Enforces single ownership. Prevents memory leaks. Use .get() to obtain raw pointer without transferring ownership.","title":"4. std::unique_ptr Benefits"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#5-creating-a-linked-list-with-5-elements","text":"1 2 3 4 5 6 7 auto head = std :: make_unique < Node < int >> ( 1 ); Node < int >* current = head . get (); for ( int i = 2 ; i <= 5 ; ++ i ) { current -> next = std :: make_unique < Node < int >> ( i ); current = current -> next . get (); } head owns the list. current is a non-owning raw pointer used to traverse and build the list.","title":"5. Creating a Linked List with 5 Elements"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#6-get-explained","text":"Returns raw pointer from unique_ptr without giving up ownership. Use it when you need temporary access: cpp current = current->next.get();","title":"6. .get() Explained"},{"location":"C%2B%2B/Linked%20List%20in%20C%2B%2B%20with%20Smart%20Pointers/#final-list","text":"Constructed list: 1 -> 2 -> 3 -> 4 -> 5 (automatically cleaned up when head goes out of scope).","title":"Final List"},{"location":"C%2B%2B/Read%20CSV%20in%20C%2B%2B/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include <iostream> #include <fstream> #include <string> #include <vector> #include <sstream> using namespace std ; vector < vector < string >> readCSV ( const string & filename ) { vector < vector < string >> data ; ifstream file ( filename ); if ( ! file . is_open ()) { return data ; // Return empty if file can't be opened } string line ; while ( getline ( file , line )) { vector < string > row ; stringstream ss ( line ); string cell ; while ( getline ( ss , cell , ',' )) { row . push_back ( cell ); } data . push_back ( row ); } return data ; } What is fstream and iostream ? Data plumbing system! C++ streams create channels for information to flow between your program and the outside world (screen, files, memory). iostream defines the basic plumbing system, while fstream adds special connectors specifically for working with files. ifstream file(filename) : The constructor calls operating system functions to locate the file, check permissions, and obtain a file handle (a reference number the OS uses to track open files). Once opened, the stream object configures internal buffers to efficiently read data from disk when requested. What is sstream ? cio`","title":"Read CSV in C++"},{"location":"C%2B%2B/Read%20CSV%20in%20C%2B%2B/#what-is-fstream-and-iostream","text":"Data plumbing system! C++ streams create channels for information to flow between your program and the outside world (screen, files, memory). iostream defines the basic plumbing system, while fstream adds special connectors specifically for working with files. ifstream file(filename) : The constructor calls operating system functions to locate the file, check permissions, and obtain a file handle (a reference number the OS uses to track open files). Once opened, the stream object configures internal buffers to efficiently read data from disk when requested.","title":"What is fstream and iostream?"},{"location":"C%2B%2B/Read%20CSV%20in%20C%2B%2B/#what-is-sstream","text":"cio`","title":"What is sstream?"},{"location":"C%2B%2B/Reading%20Args/","text":"What is argc and argv? argc : number of command-line arguments. argv : array of C-strings holding those arguments. argv[0] is the program name (the executable's name used to start the program). argv[1] to argv[argc-1] are the actual command-line arguments. Example: running ./myapp arg1 arg2 sets argv[0]=\"./myapp\" , argv[1]=\"arg1\" , argv[2]=\"arg2\" . 1 2 3 int main ( int argc , char * argv []) { } char* is a pointer to a character or to the first character in a C-string (null-terminated array of chars).","title":"Reading Args"},{"location":"C%2B%2B/Reading%20Args/#what-is-argc-and-argv","text":"argc : number of command-line arguments. argv : array of C-strings holding those arguments. argv[0] is the program name (the executable's name used to start the program). argv[1] to argv[argc-1] are the actual command-line arguments. Example: running ./myapp arg1 arg2 sets argv[0]=\"./myapp\" , argv[1]=\"arg1\" , argv[2]=\"arg2\" . 1 2 3 int main ( int argc , char * argv []) { } char* is a pointer to a character or to the first character in a C-string (null-terminated array of chars).","title":"What is argc and argv?"},{"location":"C%2B%2B/Shared%20vs%20Static%20libraries/","text":"Shared Libraries : Definition : Dynamically linked libraries ( .so on Unix, .dll on Windows) loaded at runtime . Size : Smaller executable size; library code is not embedded in the binary. Linking : Linked at runtime via dynamic linker; requires library presence on the system. Updates : Can be updated independently without recompiling the executable. Performance : Slight runtime overhead due to dynamic linking. Distribution : Must be distributed with the executable or installed on the target system. Memory : Shared across multiple processes, reducing memory usage. Example : libc.so on Linux. Static Libraries : - Definition : Archives ( .a on Unix, .lib on Windows) embedded into the executable at compile time. Size : Larger executable size; library code is included in the binary. Linking : Linked at compile time; no external dependencies at runtime. Updates : Requires recompilation of the executable to incorporate library updates. Performance : Faster at runtime; no dynamic linking overhead. Distribution : Self-contained executable; no need to distribute the library separately. Memory : Each executable has its own copy, increasing memory usage. Example : libc.a on Linux.","title":"Shared vs Static libraries"},{"location":"C%2B%2B/The%20Art%20of%20Higher%20Order%20Functions/","text":"Functional programming is awesome. The concept of a function is so pure. You have this elegant thing that takes an input and gives you an output, and no matter what, the same input always gives you the same output. The key thing hat ensures that it gives you the same answer always for infinity is that there's no state and hence no side-effects. Higher-order functions are beautiful. They take the idea of a function\u2014this clean little input-output machine\u2014and elevate it. Instead of just working with numbers or strings, now functions can take in other functions. They can return functions. They can build behaviour out of behaviour. And suddenly, your functions are composable, modular, and expressive and still the logic and the essence of a function remains pure, untouched. Let's create one that measures execution time. We will write this simple wrapper that takes any arbitrary function as input and gives you an enhanced function as output. The key thing that makes this approach so clean is that we're not touching the original function's logic or introducing side-effects. Suppose you have a function which generates a sequence of n integers starting from start , increasing by step . It returns the result as a pre-allocated std::vector<int> for performance. 1 2 3 4 5 6 7 std :: vector < int > generate_sequence ( int start , int n , int step ) { std :: vector < int > result ; result . reserve ( n ); for ( int i = 0 ; i < n ; ++ i ) result . push_back ( start + i * step ); return result ; } Now here come the fun part: 1 2 3 4 5 6 7 8 9 10 11 template < typename Func > auto timed ( Func f ) { return [ f ]( auto && ... args ) { auto start = std :: chrono :: high_resolution_clock :: now (); auto result = f ( std :: forward < decltype ( args ) > ( args )...); auto end = std :: chrono :: high_resolution_clock :: now (); std :: chrono :: duration < double > elapsed = end - start ; std :: cout << \"Elapsed time: \" << elapsed . count () << \" seconds \\n \" ; return result ; }; } Let\u2019s unpack what\u2019s happening inside. auto&&... args is what makes the wrapper flexible. It says: give me any number of inputs, of any type\u2014whether they\u2019re temporary values flying in, or persistent references you want to preserve. decltype(args) captures the true nature of each argument. It doesn\u2019t just look at the value\u2014it remembers how it was passed in: was it moveable? Was it a reference? And then comes the real hero: std::forward<decltype(args)>(args)... . This is a bit tricky! It makes sure each argument is forwarded exactly as it arrived\u2014if it was an rvalue, it stays one; if it was an lvalue, we respect that. No surprises. If like me, you also find lvalue and rvalue intimidating to comprehend, here's a simple example to distinguish lvalues and rvalues: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int x = 5 ; // 'x' is an lvalue - it has a name and storage location int y = x + 1 ; // 'x' is an lvalue, but 'x + 1' is an rvalue (temporary result) int & ref = x ; // Works: can bind lvalue reference to lvalue // int& ref2 = x + 1; // Error: can't bind lvalue reference to rvalue int && rref = x + 1 ; // Works: can bind rvalue reference to rvalue // int&& rref2 = x; // Error: can't bind rvalue reference to lvalue // Function that takes rvalue reference void process ( int && val ) { // Can modify val since we know it's a temporary } process ( 10 ); // Works: 10 is an rvalue // process(x); // Error: x is an lvalue // This works - std::move converts lvalue to rvalue process ( std :: move ( x )); 1 2 3 4 int main () { auto timed_gen = timed ( generate_sequence ); auto result = timed_gen ( 0 , 10 , 2 ); }","title":"The Art of Higher Order Functions"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/","text":"Parallelism boils down to two core ideas. Both might make your code faster, but let\u2019s be real\u2014most of the time, we\u2019re just praying it doesn\u2019t crash harder. Data Parallelism: You\u2019ve got a pile of potatoes and two people (P1 and P2). So you say, \u201cP1, take the first half. P2, take the second.\u201d Both are doing the same task\u2014just on different chunks. That\u2019s data parallelism. Same code runs everywhere ( SPMD: Single Program, Multiple Data ), only the data changes. Functional Parallelism: Same potatoes, same two people. But now you say, \u201cP1, wash them. Then hand them to P2 to chop.\u201d Different tasks for different people. That\u2019s functional parallelism. Work is split by function, not by data. Data Parallelism: Do the same thing to everything Data parallelism is about doing one thing, perfectly, across many pieces of data. You take an operation\u2014say, addition\u2014and you apply it across an entire array, not by looping, but by letting hardware chew through many elements at once. This kind of parallelism shines when operations on elements are independent of each other i.e. P1 chopping potato 1 doesn't impact P2 chopping potato 2. 1 2 3 4 5 6 7 8 9 10 11 12 // Let us multiply a C++ vector with a scalar. #include <vector> #include <omp.h> using namespace std ; vector < double > multiply_vec_with_scalar ( const vector < double >& V , double s ) { vector < double > vec ( V . size ()); #pragma omp parallel for for ( size_t i = 0 ; i < V . size (); ++ i ) { vec [ i ] = V [ i ] * scalar ; } return vec ; } Task Parallelism: Different tasks run at the same time Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores. Taking this idea a notch further, we can also think of data based parallelism Under the hood, the CPU doesn\u2019t think in individual numbers anymore. It packs data into wide vector registers: 128 bits, 256 bits, even 512 bits wide. Instead of adding two numbers, it adds eight numbers (256 bit register can accommodate 8 32-bit floats), or sixteen, in a single breath. This is what SIMD (Single Instruction, Multiple Data) is. It's minimal. It's elegant. You don't multiply effort; you multiply throughput . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include <immintrin.h> void scalar_multiply ( double * data , size_t size , double scalar ) { __m256d scalar_vec = _mm256_set1_pd ( scalar ); // Load scalar into 4 slots size_t i ; for ( i = 0 ; i < size - 3 ; i += 4 ) { __m256d data_vec = _mm256_loadu_pd ( & data [ i ]); // Load 4 elements __m256d result = _mm256_mul_pd ( data_vec , scalar_vec ); // Multiply _mm256_storeu_pd ( & data [ i ], result ); // Store back } // Handle remaining elements for (; i < size ; ++ i ) { data [ i ] *= scalar ; } } Task Parallelism: Different tasks run at the same time Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores. Core Ingredients -> if you are new to CUDA In CUDA's mental model: CPU (host) and GPU (device) are separate environments .It\u2019s called __global__ because the function becomes globally visible across two different \"worlds\": It is defined on the GPU . It can be called from the CPU . A CUDA kernel is a function that runs on the GPU . When you call a kernel, thousands of lightweight threads are launched to execute it in parallel \u2014 each thread usually works on a small piece of data.","title":"The Two Kinds of Parallelism"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#data-parallelism-do-the-same-thing-to-everything","text":"Data parallelism is about doing one thing, perfectly, across many pieces of data. You take an operation\u2014say, addition\u2014and you apply it across an entire array, not by looping, but by letting hardware chew through many elements at once. This kind of parallelism shines when operations on elements are independent of each other i.e. P1 chopping potato 1 doesn't impact P2 chopping potato 2. 1 2 3 4 5 6 7 8 9 10 11 12 // Let us multiply a C++ vector with a scalar. #include <vector> #include <omp.h> using namespace std ; vector < double > multiply_vec_with_scalar ( const vector < double >& V , double s ) { vector < double > vec ( V . size ()); #pragma omp parallel for for ( size_t i = 0 ; i < V . size (); ++ i ) { vec [ i ] = V [ i ] * scalar ; } return vec ; }","title":"Data Parallelism: Do the same thing to everything"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#task-parallelism-different-tasks-run-at-the-same-time","text":"Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores. Taking this idea a notch further, we can also think of data based parallelism Under the hood, the CPU doesn\u2019t think in individual numbers anymore. It packs data into wide vector registers: 128 bits, 256 bits, even 512 bits wide. Instead of adding two numbers, it adds eight numbers (256 bit register can accommodate 8 32-bit floats), or sixteen, in a single breath. This is what SIMD (Single Instruction, Multiple Data) is. It's minimal. It's elegant. You don't multiply effort; you multiply throughput . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include <immintrin.h> void scalar_multiply ( double * data , size_t size , double scalar ) { __m256d scalar_vec = _mm256_set1_pd ( scalar ); // Load scalar into 4 slots size_t i ; for ( i = 0 ; i < size - 3 ; i += 4 ) { __m256d data_vec = _mm256_loadu_pd ( & data [ i ]); // Load 4 elements __m256d result = _mm256_mul_pd ( data_vec , scalar_vec ); // Multiply _mm256_storeu_pd ( & data [ i ], result ); // Store back } // Handle remaining elements for (; i < size ; ++ i ) { data [ i ] *= scalar ; } }","title":"Task Parallelism: Different tasks run at the same time"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#task-parallelism-different-tasks-run-at-the-same-time_1","text":"Each task is a distinct thing that runs simultaneously, each act independent yet part of the same grand spectacle. They might touch shared memory. They might have side-effects. They might even compete with each other. They might blow-up. At the system level, the CPU spins up threads or processes. It schedules them across cores. It juggles context switches, locks, semaphores.","title":"Task Parallelism: Different tasks run at the same time"},{"location":"C%2B%2B/The%20Two%20Kinds%20of%20Parallelism/#core-ingredients-if-you-are-new-to-cuda","text":"In CUDA's mental model: CPU (host) and GPU (device) are separate environments .It\u2019s called __global__ because the function becomes globally visible across two different \"worlds\": It is defined on the GPU . It can be called from the CPU . A CUDA kernel is a function that runs on the GPU . When you call a kernel, thousands of lightweight threads are launched to execute it in parallel \u2014 each thread usually works on a small piece of data.","title":"Core Ingredients -&gt; if you are new to CUDA"},{"location":"C%2B%2B/Unions/","text":"Unions are pure first-principles: bits are bits, and you\u2019re the one giving them meaning. Memory is just a sequence of bytes divided into slots. From this ground truth, a C++ union is as a single slot\u2014say, 4 bytes\u2014that can wear different hats: union Data { int i; float f; char c; }; lets you store an int , a float , or a char in that one spot. Set f = 3.14 , and those bytes ( 0x4048F5C3 ) are written on memory. It\u2019s sized for the largest stored value ( float ), and you decide what\u2019s active, no hand-holding from the compiler. If you overwrite `f with int i = 3`. The value in the union is mutated and you have to manually keep a track of these mutations. * Avoid 'naked\u2019 unions; wrap them in a class together with a type field; Compare this to a struct : it\u2019s a wardrobe with a slot per item \u2014an int slot, a float slot, a char slot\u2014each with its own space (e.g., 9 bytes total for struct Data { int i; float f; char c; }; ). 1 2 3 4 union Data { int i ; float f ; char c ; }; //one 4-byte chunk (sized for float) Data d ; d . i = 42 ; printf ( \"%d \\n \" , d . i ); // 42 d . f = 3.14 ; printf ( \"%f \\n \" , d . f ); // f overwrites i enum { INT , FLOAT , CHAR } type ; type = FLOAT ; // you decide what\u2019s live.","title":"Unions"},{"location":"CFA/Annuity%20Calculation/","text":"For a loan taken for an amount \\(PV\\) for \\(N\\) periods at at an interest rate \\(r\\) per period, the annuity \\(A\\) is given as: \\[ \\begin{align} PV &= \\sum_{n=0}^N \\frac{A}{(1+r)^n} = NA \\sum_{n=0}^N \\frac{1}{(1+r)^n} \\\\[12pt] A &= \\frac{N \\cdot PV}{1-(1+r)^{-N}} \\end{align} \\] Amortisation Schedule OpeningPrincipal LoanDuration Rate Annuity Interest PricipalRepaid 619.00 5.00 0.07 151.78 44.57 107.21 511.79 5.00 0.07 151.78 36.85 114.93 396.86 5.00 0.07 151.78 28.57 123.20 273.66 5.00 0.07 151.78 19.70 132.07 141.58 5.00 0.07 151.78 10.19 141.58","title":"Annuity Calculation"},{"location":"CFA/Annuity%20Calculation/#amortisation-schedule","text":"OpeningPrincipal LoanDuration Rate Annuity Interest PricipalRepaid 619.00 5.00 0.07 151.78 44.57 107.21 511.79 5.00 0.07 151.78 36.85 114.93 396.86 5.00 0.07 151.78 28.57 123.20 273.66 5.00 0.07 151.78 19.70 132.07 141.58 5.00 0.07 151.78 10.19 141.58","title":"Amortisation Schedule"},{"location":"CFA/Contingency%20Provisions/","text":"Based on contingency provisions, bonds can be categorised into two types: 1. Straight or Option free bonds: 2. Option Embedded bonds: - Callable bonds are bonds the issuer can pull back before maturity. You buy a 10-year paying 6%. After 5 years, rates drop to 4%. The issuer calls it, pays you back, and refinances cheaper. - In Puttable bond, you can \u201cput\u201d the bond back to the issuer at par (or a preset price) on certain dates before maturity. Example: You buy a 10-year bond at 5%, but it\u2019s puttable after year 3. If rates spike to 7% in year 3, you don\u2019t have to sit on a bad deal\u2014you exercise the put, redeem at par, and reinvest at higher yields. It\u2019s downside protection for the holder, paid for with a lower coupon. Caution Unlike equities, bonds and call options do not trade seprately. The option is baked into the contract. Convertible bonds: A convertible bond with a warrant attached is debt with two sweeteners stacked on top: 1. Convertible bond \u2192 lets you swap your bond into equity of the issuing company at a set price. 2. Warrant \u2192 a separate, detachable option to buy more shares later at a fixed strike. The warrant is just a free call option stapled to the bond. It gives you the right (not obligation) to buy stock at a preset strike. Difference is: the convertible feature consumes the bond itself\u2014you give up the bond when you convert. The warrant is separate and detachable\u2014you can sell it, trade it, or exercise it without touching the bond. So you\u2019re lending money, earning coupons, and holding optionality twice: you can convert your bond into stock and you can keep (or even sell) the warrant. Example: A company issues a 5-year 5% bond. You can convert it into shares at $50 each. On top, you get a warrant giving you the right to buy more shares at $55 anytime in the next 3 years. Caution Unlike Callable Bonds, Warrants on Convertible Bonds can be traded seprately. In case of warrants, you will get new shares from the company, unlike a call option where existing shares are exchanged. It\u2019s debt, equity, and options stitched together. 4. Contingent Convertible Bonds (COCO Bonds): Contingent Convertible Bonds ( CoCos ) are bank debt designed to flip under stress. They pay coupons like normal bonds, but if the bank\u2019s capital ratio falls below a set trigger, the bond either converts into equity or gets written down. Example: a bank issues a $1,000 CoCo at 8%. As long as capital stays above the threshold, you collect coupons. If it drops below, your bond turns into shares at a preset price or loses part of its value. CoCos exist to give banks an automatic capital buffer when conditions deteriorate.","title":"Contingency Provisions"},{"location":"CFA/CouponStructures/","text":"By convention always assume, coupons are semi-annual. Floating Rate Notes Floating rate notes, often called \"floaters,\" are a type of bond where the interest payments aren't fixed, but instead adjust up or down based on a market interest rate benchmark like the federal funds rate or SOFR, plus a set extra amount called a credit spread. Attention Spread on a floating-rate note doesn\u2019t change once the bond is issued. It\u2019s baked into the deal, hard-coded in the fine print Inverse floaters They\u2019re issued by companies or governments betting they\u2019ll look more creditworthy later, luring investors with the promise of higher payouts down the line while keeping early costs cheap. Step-up Coupon Bonds These bonds have predetermined rate hikes baked into the contract\u2014say, 3% for the first two years, 4% for the next three, 5% after that. Payment in Kind Bonds Payment-in-kind (PIK) bonds let issuers skip cash interest payments by giving you more bonds or inflating your principal instead. It\u2019s a move for companies too broke to pay now. Index Linked Bonds Principal, coupons, or both are pegged to an index (usually inflation). Example: TIPS in the U.S. - Buy $1,000 at 2% coupon. - CPI rises 3% \u2192 principal becomes $1,030. - Coupon = 2% \u00d7 1,030 = $20.60. They hedge inflation, but tank in deflation or when real yields rip higher (see 2022 TIPS bloodbath).","title":"CouponStructures"},{"location":"CFA/CouponStructures/#floating-rate-notes","text":"Floating rate notes, often called \"floaters,\" are a type of bond where the interest payments aren't fixed, but instead adjust up or down based on a market interest rate benchmark like the federal funds rate or SOFR, plus a set extra amount called a credit spread. Attention Spread on a floating-rate note doesn\u2019t change once the bond is issued. It\u2019s baked into the deal, hard-coded in the fine print","title":"Floating Rate Notes"},{"location":"CFA/CouponStructures/#inverse-floaters","text":"They\u2019re issued by companies or governments betting they\u2019ll look more creditworthy later, luring investors with the promise of higher payouts down the line while keeping early costs cheap.","title":"Inverse floaters"},{"location":"CFA/CouponStructures/#step-up-coupon-bonds","text":"These bonds have predetermined rate hikes baked into the contract\u2014say, 3% for the first two years, 4% for the next three, 5% after that.","title":"Step-up Coupon Bonds"},{"location":"CFA/CouponStructures/#payment-in-kind-bonds","text":"Payment-in-kind (PIK) bonds let issuers skip cash interest payments by giving you more bonds or inflating your principal instead. It\u2019s a move for companies too broke to pay now.","title":"Payment in Kind Bonds"},{"location":"CFA/CouponStructures/#index-linked-bonds","text":"Principal, coupons, or both are pegged to an index (usually inflation). Example: TIPS in the U.S. - Buy $1,000 at 2% coupon. - CPI rises 3% \u2192 principal becomes $1,030. - Coupon = 2% \u00d7 1,030 = $20.60. They hedge inflation, but tank in deflation or when real yields rip higher (see 2022 TIPS bloodbath).","title":"Index Linked Bonds"},{"location":"CFA/Legal%20Terms%20Related%20to%20Bonds/","text":"Indentures * Definition (Cause) : A formal contract between issuer and bondholders. * Effect : Sets the legal framework \u2014 rights, duties, enforcement. * Purpose : Prevents ambiguity. Covenants Definition (Cause) : Specific terms inside the indenture. Effect : They govern the issuer\u2019s behavior during the life of the bond. Purpose : Protect bondholders from reckless or unfair issuer actions. Affirmative Covenants Cause : Obligations imposed on the issuer. Effect : Forces transparency and accountability. Purpose : Ensures investors get necessary info and compliance. Example : Provide audited financial statements. Negative Covenants Cause : Restrictions placed on the issuer. Effect : Limits risky financial decisions. Purpose : Shields bondholders from dilution of claims. Examples : No pledging collateral twice. Maintain debt/equity within limits. Bond Trustee Definition (Cause) : Independent party appointed in the indenture. Effect : Acts on behalf of bondholders collectively. Purpose : Centralises enforcement, since individual investors lack leverage. Cross-Default Clause Definition (Cause) : Provision linking defaults across multiple bonds. Effect : Default on one bond triggers default on others. Purpose : Prevents the issuer from selectively defaulting while paying other creditors. Example : Miss coupon on Bond B \u2192 holders of Bonds A and C can also declare default. Pari Passu Definition (Cause) : Clause that equalises ranking across bonds. Effect : New bonds cannot jump ahead of existing ones in priority. Purpose : Protects bondholders from subordination and preserves fairness.","title":"Legal Terms Related to Bonds"},{"location":"Competitive%20Probability/Coins/","text":"Question Anna and Brenda are playing a game. They repeatedly toss a coin. Anna wins if 3 heads appear. Brenda wins if 3 tails appear. The heads and tails do not need to be consecutive. What is the expected number of coin tosses for a winner to be determined?","title":"Coins"},{"location":"Competitive%20Probability/Collecting%20Toys%20I/","text":"Every box of cereal contains one toy from a group of 5 distinct toys, each of which is mutually independent from the others and is equally likely to be within a given box. How many distinct toys can you expect to collect if you bought 7 boxes?","title":"Collecting Toys I"},{"location":"Competitive%20Probability/Combinatorics/","text":"1. Permute 2 objects of 2 distinct types in 4 spots. You're placing 2 A-type and 2 B-type objects into 4 positions. The number of distinct permutations is: \\[ \\frac{4!}{2! \\cdot 2!} = 6 \\] Memory Trick: Think of the 4 spots as a 4-letter word made of 2 As and 2 Bs (e.g., AABB). To remember the count: * Total ways to arrange 4 items = 4! * But A and B repeat , so divide by repeats : * 2! for A's * 2! for B's * So: $$ \\frac{4!}{2! \\cdot 2!} = 6 $$","title":"Combinatorics"},{"location":"Competitive%20Probability/Combinatorics/#1-permute-2-objects-of-2-distinct-types-in-4-spots","text":"You're placing 2 A-type and 2 B-type objects into 4 positions. The number of distinct permutations is: \\[ \\frac{4!}{2! \\cdot 2!} = 6 \\]","title":"1. Permute 2 objects of 2 distinct types in 4 spots."},{"location":"Competitive%20Probability/Combinatorics/#memory-trick","text":"Think of the 4 spots as a 4-letter word made of 2 As and 2 Bs (e.g., AABB). To remember the count: * Total ways to arrange 4 items = 4! * But A and B repeat , so divide by repeats : * 2! for A's * 2! for B's * So: $$ \\frac{4!}{2! \\cdot 2!} = 6 $$","title":"Memory Trick:"},{"location":"Competitive%20Probability/Coupon%20Collector/","text":"TL;DR If there are \\(N\\) different items (like toys or coupons), and each time you pick one at random, it gets harder to find the missing ones as your collection grows. * First item is always new. * Second one is still easy to find. * But the last few are rare\u2014you keep getting duplicates. The average number of total picks you need to get all \\(N\\) different items is about: $$ N \\times (\\ln N + 0.577) $$ This grows a bit faster than linear. For 50 items, you'd need about 225 tries. Problem Statement You have \\(N\\) unique coupons. Each time you draw, you get one coupon chosen uniformly at random (with replacement). Goal: Find how many draws it takes on average to collect all \\(N\\) distinct coupons. Building Blocks [[Geometric Distribution]]: Geometric distribution tells you the probability of first success in a series of repeated experiments. The expectation of Geometric Distribution is 1/p. [[Expectation]]: It is probability weighted average of a distribution. Thing of it as centre of gravity. Linearity of [[Expectation]]: Sum of expectations is Expectation of the sum. How to think about it? Think of collecting the coupons one by one: Stage 1: How many draws to get the first new coupon? \u2192 Always 1 (since you start with none). Let us call it \\(T_1\\) Stage 2: Now you have 1 unique coupon. What's the chance the next draw gives a new one? Here we measure the probability of success till we get our first success. This is where [[Geometric Distribution]] is useful. Let us call this \\(T_2\\) \u2192 Probability = \\(\\frac{N - 1}{N}\\) , so expected number of tries = \\(\\frac{N}{N - 1}\\) Stage 3: Now you have 2 unique coupons. Probability next is new = \\(\\frac{N - 2}{N}\\) , expected tries = \\(\\frac{N}{N - 2}\\) . Let us call this \\(T_3\\) And so on... By linearity of expectations: $$ E(T) = E(T_1 + \\dots + T_n) = E(T_1) + \\dots + E(T_n)$$ This implies: $$\\begin{aligned} E(T) &= 1+ \\frac{N}{N-1} + \\frac{N}{N-2} + \\dots + \\frac{N}{N-k+1} \\ &= N \\left[\\frac{1}{N} + \\dots + \\frac{1}{N-k+1} + 1 \\right] \\end{aligned} $$ The term in the bracket is a [[Harmonic Sum]], which is \\(\\log(n)\\) Hence Expectation is \\(n \\log(n)\\)","title":"Coupon Collector"},{"location":"Competitive%20Probability/Coupon%20Collector/#tldr","text":"If there are \\(N\\) different items (like toys or coupons), and each time you pick one at random, it gets harder to find the missing ones as your collection grows. * First item is always new. * Second one is still easy to find. * But the last few are rare\u2014you keep getting duplicates. The average number of total picks you need to get all \\(N\\) different items is about: $$ N \\times (\\ln N + 0.577) $$ This grows a bit faster than linear. For 50 items, you'd need about 225 tries.","title":"TL;DR"},{"location":"Competitive%20Probability/Coupon%20Collector/#problem-statement","text":"You have \\(N\\) unique coupons. Each time you draw, you get one coupon chosen uniformly at random (with replacement). Goal: Find how many draws it takes on average to collect all \\(N\\) distinct coupons.","title":"Problem Statement"},{"location":"Competitive%20Probability/Coupon%20Collector/#building-blocks","text":"[[Geometric Distribution]]: Geometric distribution tells you the probability of first success in a series of repeated experiments. The expectation of Geometric Distribution is 1/p. [[Expectation]]: It is probability weighted average of a distribution. Thing of it as centre of gravity. Linearity of [[Expectation]]: Sum of expectations is Expectation of the sum.","title":"Building Blocks"},{"location":"Competitive%20Probability/Coupon%20Collector/#how-to-think-about-it","text":"Think of collecting the coupons one by one: Stage 1: How many draws to get the first new coupon? \u2192 Always 1 (since you start with none). Let us call it \\(T_1\\) Stage 2: Now you have 1 unique coupon. What's the chance the next draw gives a new one? Here we measure the probability of success till we get our first success. This is where [[Geometric Distribution]] is useful. Let us call this \\(T_2\\) \u2192 Probability = \\(\\frac{N - 1}{N}\\) , so expected number of tries = \\(\\frac{N}{N - 1}\\) Stage 3: Now you have 2 unique coupons. Probability next is new = \\(\\frac{N - 2}{N}\\) , expected tries = \\(\\frac{N}{N - 2}\\) . Let us call this \\(T_3\\) And so on... By linearity of expectations: $$ E(T) = E(T_1 + \\dots + T_n) = E(T_1) + \\dots + E(T_n)$$ This implies: $$\\begin{aligned} E(T) &= 1+ \\frac{N}{N-1} + \\frac{N}{N-2} + \\dots + \\frac{N}{N-k+1} \\ &= N \\left[\\frac{1}{N} + \\dots + \\frac{1}{N-k+1} + 1 \\right] \\end{aligned} $$ The term in the bracket is a [[Harmonic Sum]], which is \\(\\log(n)\\) Hence Expectation is \\(n \\log(n)\\)","title":"How to think about it?"},{"location":"Competitive%20Probability/Expectation/","text":"Linearity of Expectation Linearity comes straight from the definition of expectation as a weighted average. Because summation (or integration) is a linear operator, expectation inherits linearity. \\[ \\mathbb{E}[X + Y] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty (x + y) f_{X,Y}(x, y) \\, dx\\,dy $$ $$ = \\int x f_{X,Y}(x, y)\\,dx\\,dy + \\int y f_{X,Y}(x, y)\\,dx\\,dy = \\mathbb{E}[X] + \\mathbb{E}[Y] \\] Because integration is linear , expectation is linear.","title":"Expectation"},{"location":"Competitive%20Probability/Expectation/#linearity-of-expectation","text":"Linearity comes straight from the definition of expectation as a weighted average. Because summation (or integration) is a linear operator, expectation inherits linearity. \\[ \\mathbb{E}[X + Y] = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty (x + y) f_{X,Y}(x, y) \\, dx\\,dy $$ $$ = \\int x f_{X,Y}(x, y)\\,dx\\,dy + \\int y f_{X,Y}(x, y)\\,dx\\,dy = \\mathbb{E}[X] + \\mathbb{E}[Y] \\] Because integration is linear , expectation is linear.","title":"Linearity of Expectation"},{"location":"Competitive%20Probability/Geometric%20Distribution/","text":"Info A geometric distribution models the number of trials you need to run until you get your first success in a sequence of independent attempts, each with the same probability of success. Picture flipping a coin until you hit heads: it\u2019s a simple waiting game where the odds don\u2019t change, and the question is \u201chow long until I win?\u201d Let\u2019s build it up. Start with a single trial \u2014say, a coin flip with success probability - \\(p\\) (e.g., \\(p = 0.5\\) for heads). - Failure is \\(1 - p\\) , and each trial is independent, like rolling a die unaware of past rolls. - Now, imagine waiting for the first success: you might fail \\(k-1\\) times (probability \\((1-p)^{k-1}\\) ), - Then succeed on the \\(k_{th}\\) try (probability \\(p\\) ). The geometric distribution\u2019s probability mass function is: \\(P(X = k) = (1-p)^{k-1} \\cdot p\\) where \\(k = 1, 2, 3, \\ldots\\) . It\u2019s a chain of failures capped by a win, derived from multiplying these basic probabilities together\u2014exponential decay until the breakthrough. Expectation You're trying to get a new coupon you don't already have. If you already have 1 out of \\(N\\) coupons, then there are \\(N - 1\\) new ones left. The chance the next coupon is new = \\(\\frac{N - 1}{N}\\) So, the expected number of draws to get a new one is: \\[ \\text{Expected tries} = \\frac{1}{\\text{Probability}} = \\frac{N}{N - 1} \\] This is just how probability works: if success chance is \\(p\\) , then expected number of trials until success = \\(\\frac{1}{p}\\) . Let \\(X\\) denote the number of tries till the first success. \\( \\(P(X=n) = (1-p)^n \\times p\\) \\) Now the expectation can be written as: $$\\sum^\\infty n (1-p)^n p $$ The sum of a Arithmetic-Geometric progression is: $$ S = \\frac{a}{1-r} + \\frac{dr}{(1-r)^2}$$ Hence the expectation is: \\( \\(E(X) = \\frac{1}{p}\\) \\)","title":"Geometric Distribution"},{"location":"Competitive%20Probability/Geometric%20Distribution/#expectation","text":"You're trying to get a new coupon you don't already have. If you already have 1 out of \\(N\\) coupons, then there are \\(N - 1\\) new ones left. The chance the next coupon is new = \\(\\frac{N - 1}{N}\\) So, the expected number of draws to get a new one is: \\[ \\text{Expected tries} = \\frac{1}{\\text{Probability}} = \\frac{N}{N - 1} \\] This is just how probability works: if success chance is \\(p\\) , then expected number of trials until success = \\(\\frac{1}{p}\\) . Let \\(X\\) denote the number of tries till the first success. \\( \\(P(X=n) = (1-p)^n \\times p\\) \\) Now the expectation can be written as: $$\\sum^\\infty n (1-p)^n p $$ The sum of a Arithmetic-Geometric progression is: $$ S = \\frac{a}{1-r} + \\frac{dr}{(1-r)^2}$$ Hence the expectation is: \\( \\(E(X) = \\frac{1}{p}\\) \\)","title":"Expectation"},{"location":"Competitive%20Probability/Markov%20Processes/","text":"TLDR: For any Markov chain problem, Always ask: What\u2019s my state? Where does it end? How does it move? That\u2019s the recipe for modeling any Markov problem. Problem 1: You roll a fair 6-sided die repeatedly. What's the probability that you see all odd values {1,3,5} before you see any even value {2,4,6}? Step 1: Define States Let \\(P_k\\) be the probability of success after seeing \\(k\\) distinct odd numbers, for \\(k=\\{0,1,2,3\\}\\) : State 0 : no odd values seen State 1 : seen 1 distinct odd value State 2 : seen 2 distinct odd values State 3 : absorbing success state (all 3 seen) Any even \u2192 absorbing failure state (probability = 0) Note What is absorbing? In a Markov chain, a state is absorbing if once you enter it, you can\u2019t leave. It\u2019s a \u201cterminal\u201d outcome. In the problem: Success state: all 3 odd faces seen \u2192 absorbing (you stop rolling) and Failure** state: any even face appears \u2192 absorbing (you stop rolling) Step 2: Set Boundary Conditions \\(P_3 = 1\\) (success: already collected all 3 odds) If an even number is drawn in any state \u2192 probability = 3 Step 3: Write Recurrence (Transition Probabilities) At state \\(k\\) (for \\(k<3\\) ): - With probability \\(1/2\\) , you roll an even \u2192 failure \u2192 contribute 0 - With probability \\(1/2\\) , you roll an odd: - Chance it\u2019s a new odd: \\(\\frac{3-k}{3}\\) \u2192 go to state \\(k+1\\) - Chance it\u2019s a repeat odd: \\(\\frac{k}{3}\\) \u2192 stay in state \\(k\\) So, The total probability of eventual success from state \\(k\\) is the expected value of what happens after the next roll**, weighted by the probabilities of each type of outcome. \\[ P_k = \\underbrace{\\frac{1}{2} \\cdot 0}_{\\text{even} \\to \\text{failure}} + \\underbrace{\\frac{1}{2}}_{\\text{odd roll}} \\cdot \\left( \\underbrace{\\frac{3-k}{3} \\cdot P_{k+1}}_{\\text{new odd} \\to \\text{progress}} + \\underbrace{\\frac{k}{3} \\cdot P_k}_{\\text{repeat} \\to \\text{stay}} \\right) \\] Step 4: Solve Recursively (Work Backwards) Start from \\(P_3 = 1\\) . Now solve for \\(P_2\\) } Solve: Final Answer: \\[ \\boxed{P(\\text{all odd before even}) = \\frac{1}{20}} \\] Problem 2: Gambler's Ruin Problem Find the expected number of flips of a fair coin needed to have 10 more heads flipped more than tails or 7 more tails flipped more than heads. insight When you have sequence of events think of Markov Processes or Random Walks . Let \\(X_n\\) be the head-tail difference. $$ X_{n+1} =\\begin{cases} X_n+1,&\\tfrac12\\,,\\ X_n-1,&\\tfrac12\\, \\end{cases}$$ The difference increases by 1 with probability 1/2 and vice-versa. Now we need to find the probability: \\(P(X_n = 10 \\ \\cup \\ X_n = -7)\\) . Here 10 and -7 and absorption states, where the process terminates. Everything in between is a transient state \u2014you keep walking until you fall into a boundary. What are absorption states? In random walks, absorbing states are like traps \u2014 once you're in, you're stuck. The rest of the states are transient \u2014 you might visit them many times, but you'll eventually leave. The question becomes: \"How long does the walker wander before falling into one of the traps?\" Define a new RV \\(i = X_n + 7\\) $$ i_{n+1} =\\begin{cases} i+1,&\\tfrac12\\,,\\ i-1,&\\tfrac12\\, \\end{cases}$$ Now we need to find the probability: \\(i = 7 \\ \\cup \\ i = 17\\) We start from 7 and terminate at 17. Here \\(N=17\\) and \\(i=7\\) probability_concept **Mean absorption time of a Markov Process: \\[E_i = i(N - i ) = 7 \\times (17 - 7) = 70\\]","title":"Markov Processes"},{"location":"Competitive%20Probability/Markov%20Processes/#problem-1","text":"You roll a fair 6-sided die repeatedly. What's the probability that you see all odd values {1,3,5} before you see any even value {2,4,6}?","title":"Problem 1:"},{"location":"Competitive%20Probability/Markov%20Processes/#step-1-define-states","text":"Let \\(P_k\\) be the probability of success after seeing \\(k\\) distinct odd numbers, for \\(k=\\{0,1,2,3\\}\\) : State 0 : no odd values seen State 1 : seen 1 distinct odd value State 2 : seen 2 distinct odd values State 3 : absorbing success state (all 3 seen) Any even \u2192 absorbing failure state (probability = 0) Note What is absorbing? In a Markov chain, a state is absorbing if once you enter it, you can\u2019t leave. It\u2019s a \u201cterminal\u201d outcome. In the problem: Success state: all 3 odd faces seen \u2192 absorbing (you stop rolling) and Failure** state: any even face appears \u2192 absorbing (you stop rolling)","title":"Step 1: Define States"},{"location":"Competitive%20Probability/Markov%20Processes/#step-2-set-boundary-conditions","text":"\\(P_3 = 1\\) (success: already collected all 3 odds) If an even number is drawn in any state \u2192 probability = 3","title":"Step 2: Set Boundary Conditions"},{"location":"Competitive%20Probability/Markov%20Processes/#step-3-write-recurrence-transition-probabilities","text":"At state \\(k\\) (for \\(k<3\\) ): - With probability \\(1/2\\) , you roll an even \u2192 failure \u2192 contribute 0 - With probability \\(1/2\\) , you roll an odd: - Chance it\u2019s a new odd: \\(\\frac{3-k}{3}\\) \u2192 go to state \\(k+1\\) - Chance it\u2019s a repeat odd: \\(\\frac{k}{3}\\) \u2192 stay in state \\(k\\) So, The total probability of eventual success from state \\(k\\) is the expected value of what happens after the next roll**, weighted by the probabilities of each type of outcome. \\[ P_k = \\underbrace{\\frac{1}{2} \\cdot 0}_{\\text{even} \\to \\text{failure}} + \\underbrace{\\frac{1}{2}}_{\\text{odd roll}} \\cdot \\left( \\underbrace{\\frac{3-k}{3} \\cdot P_{k+1}}_{\\text{new odd} \\to \\text{progress}} + \\underbrace{\\frac{k}{3} \\cdot P_k}_{\\text{repeat} \\to \\text{stay}} \\right) \\]","title":"Step 3: Write Recurrence (Transition Probabilities)"},{"location":"Competitive%20Probability/Markov%20Processes/#step-4-solve-recursively-work-backwards","text":"Start from \\(P_3 = 1\\) . Now solve for \\(P_2\\) } Solve:","title":"Step 4: Solve Recursively (Work Backwards)"},{"location":"Competitive%20Probability/Markov%20Processes/#final-answer","text":"\\[ \\boxed{P(\\text{all odd before even}) = \\frac{1}{20}} \\]","title":"Final Answer:"},{"location":"Competitive%20Probability/Markov%20Processes/#problem-2-gamblers-ruin-problem","text":"Find the expected number of flips of a fair coin needed to have 10 more heads flipped more than tails or 7 more tails flipped more than heads.","title":"Problem 2: Gambler's Ruin Problem"},{"location":"Competitive%20Probability/Markov%20Processes/#insight-when-you-have-sequence-of-events-think-of-markov-processes-or-random-walks","text":"Let \\(X_n\\) be the head-tail difference. $$ X_{n+1} =\\begin{cases} X_n+1,&\\tfrac12\\,,\\ X_n-1,&\\tfrac12\\, \\end{cases}$$ The difference increases by 1 with probability 1/2 and vice-versa. Now we need to find the probability: \\(P(X_n = 10 \\ \\cup \\ X_n = -7)\\) . Here 10 and -7 and absorption states, where the process terminates. Everything in between is a transient state \u2014you keep walking until you fall into a boundary. What are absorption states? In random walks, absorbing states are like traps \u2014 once you're in, you're stuck. The rest of the states are transient \u2014 you might visit them many times, but you'll eventually leave. The question becomes: \"How long does the walker wander before falling into one of the traps?\" Define a new RV \\(i = X_n + 7\\) $$ i_{n+1} =\\begin{cases} i+1,&\\tfrac12\\,,\\ i-1,&\\tfrac12\\, \\end{cases}$$ Now we need to find the probability: \\(i = 7 \\ \\cup \\ i = 17\\) We start from 7 and terminate at 17. Here \\(N=17\\) and \\(i=7\\)","title":"insight When you have sequence of events think of Markov Processes or Random Walks."},{"location":"Competitive%20Probability/Markov%20Processes/#probability_concept-mean-absorption-time-of-a-markov-process","text":"\\[E_i = i(N - i ) = 7 \\times (17 - 7) = 70\\]","title":"probability_concept **Mean absorption time of a Markov Process:"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/","text":"The expected number of trials to get \\(n\\) successes in a Bernoulli process with success probability \\(p\\) is: \\[ \\mathbb{E}[\\text{trials}] = \\frac{n}{p} \\] Why? (First Principles) Each success takes on average \\(\\frac{1}{p}\\) trials (this is just the expectation of a geometric distribution). So if you need \\(n\\) successes, and each is independent: \\[ \\mathbb{E}[\\text{trials}] = \\underbrace{\\frac{1}{p} + \\frac{1}{p} + \\cdots + \\frac{1}{p}}_{n \\text{ times}} = \\frac{n}{p} \\] Example If the probability of success is 0.2 (e.g., hitting a target with 20% accuracy), and you want 5 hits: $$ \\mathbb{E}[\\text{trials}] = \\frac{5}{0.2} = 25 $$ On average, you'd expect to take 25 shots to land 5 hits. Relationship with [[Geometric Distribution]] The Negative Binomial Distribution generalizes this by modeling the number of trials needed to achieve r successes . It\u2019s essentially the sum of r independent geometric distributions (each representing the trials until a single success). The Geometric Distribution is a special case of the Negative Binomial Distribution where r=1r = 1r=1. The Negative Binomial extends the Geometric Distribution by counting the trials needed for more than one success, not just the first one.","title":"Negative Binomial Distribution"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/#why-first-principles","text":"Each success takes on average \\(\\frac{1}{p}\\) trials (this is just the expectation of a geometric distribution). So if you need \\(n\\) successes, and each is independent: \\[ \\mathbb{E}[\\text{trials}] = \\underbrace{\\frac{1}{p} + \\frac{1}{p} + \\cdots + \\frac{1}{p}}_{n \\text{ times}} = \\frac{n}{p} \\]","title":"Why? (First Principles)"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/#example","text":"If the probability of success is 0.2 (e.g., hitting a target with 20% accuracy), and you want 5 hits: $$ \\mathbb{E}[\\text{trials}] = \\frac{5}{0.2} = 25 $$ On average, you'd expect to take 25 shots to land 5 hits.","title":"Example"},{"location":"Competitive%20Probability/Negative%20Binomial%20Distribution/#relationship-with-geometric-distribution","text":"The Negative Binomial Distribution generalizes this by modeling the number of trials needed to achieve r successes . It\u2019s essentially the sum of r independent geometric distributions (each representing the trials until a single success). The Geometric Distribution is a special case of the Negative Binomial Distribution where r=1r = 1r=1. The Negative Binomial extends the Geometric Distribution by counting the trials needed for more than one success, not just the first one.","title":"Relationship with [[Geometric Distribution]]"},{"location":"Competitive%20Probability/Optimal%20Substructure%20Reasoning/","text":"You are given 8 fair coins and flip all of them at once. Afterwards, you are allowed to reflip as many coins as you would like one time each. At the end, you are given $1 for each head that appears. Assuming optimal play, find the fair value of this game. Each coin's state (H or T) can be treated independently .So for a single coin: Decision: If the coin is Head (H) : Keep: get $1 Reflip: 50% H, 50% T \u2192 EV = $0.5 Best choice: keep it (since $1 > $0.5) If the coin is Tail (T) : Keep: $0 Reflip: EV = $0.5 Best choice: reflip it (since $0.5 > $0) This decision is independent for each coin \u2192 meaning we solve a subproblem for each coin , and just sum up the optimal outcomes.","title":"Optimal Substructure Reasoning"},{"location":"Competitive%20Probability/Optimal%20Substructure%20Reasoning/#decision","text":"If the coin is Head (H) : Keep: get $1 Reflip: 50% H, 50% T \u2192 EV = $0.5 Best choice: keep it (since $1 > $0.5) If the coin is Tail (T) : Keep: $0 Reflip: EV = $0.5 Best choice: reflip it (since $0.5 > $0) This decision is independent for each coin \u2192 meaning we solve a subproblem for each coin , and just sum up the optimal outcomes.","title":"Decision:"},{"location":"Competitive%20Probability/Place%20and%20Take/","text":"hard You are playing a one-player game with two opaque boxes. At each turn, you can choose to either \"place\" or \"take\". \"Place\" places $1 from a third party into one box randomly. \"Take\" empties out one box randomly and that money is yours. This game consists of 100 turns where you must either place or take. Assuming optimal play, what is the expected payoff of this game? Note that you do not know how much money you have taken until the end of the game. Thought Process Think of every dollar you drop in as a goldfish in a pond. Each time you \u201ctake,\u201d you scoop one of the two ponds at random, so any given goldfish has a 1/2 chance of getting caught. If you plan to scoop k times, the chance a goldfish eventually gets caught is $$ 1 - (\\frac{1}{2})^k $$ How? Probability of not getting caught in \\(k\\) attempts = \\((1/2)^k\\) Probability of getting caught atleast once = \\(1 - (1/2)^k\\) You take \\(k\\) attempts so you place \\(100-k\\) attempts. So payoff function: $$ (100-k)(1 - (\\frac{1}{2})^k) $$ Maximise this payoff as: \\[ \\text{arg max}_k (100-k)(1 - 0.5^k) \\] Differential it wrt \\(k\\) and set the derivative to 0. \\[ \\begin{aligned} \\text{Let} \\ u &= 1-0.5^k \\\\ du/dk &= k0.5^{k-1} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{Let} \\ v &= 100-k \\\\ du/dv &= -1 \\\\ \\end{aligned} \\] $$ \\begin{aligned} 0.5^k -1 + k0.5^{k-1}(100-k) = 0 \\ 0.5^k + 2k0.5^{k}(100-k) = 1 \\ \\text{Let} \\ 0.5^k = T \\ T - 200kT - 2k^2T =1 \\end{aligned} $$","title":"hard"},{"location":"Competitive%20Probability/Place%20and%20Take/#hard","text":"You are playing a one-player game with two opaque boxes. At each turn, you can choose to either \"place\" or \"take\". \"Place\" places $1 from a third party into one box randomly. \"Take\" empties out one box randomly and that money is yours. This game consists of 100 turns where you must either place or take. Assuming optimal play, what is the expected payoff of this game? Note that you do not know how much money you have taken until the end of the game.","title":"hard"},{"location":"Competitive%20Probability/Place%20and%20Take/#thought-process","text":"Think of every dollar you drop in as a goldfish in a pond. Each time you \u201ctake,\u201d you scoop one of the two ponds at random, so any given goldfish has a 1/2 chance of getting caught. If you plan to scoop k times, the chance a goldfish eventually gets caught is $$ 1 - (\\frac{1}{2})^k $$ How? Probability of not getting caught in \\(k\\) attempts = \\((1/2)^k\\) Probability of getting caught atleast once = \\(1 - (1/2)^k\\) You take \\(k\\) attempts so you place \\(100-k\\) attempts. So payoff function: $$ (100-k)(1 - (\\frac{1}{2})^k) $$ Maximise this payoff as: \\[ \\text{arg max}_k (100-k)(1 - 0.5^k) \\] Differential it wrt \\(k\\) and set the derivative to 0. \\[ \\begin{aligned} \\text{Let} \\ u &= 1-0.5^k \\\\ du/dk &= k0.5^{k-1} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\text{Let} \\ v &= 100-k \\\\ du/dv &= -1 \\\\ \\end{aligned} \\] $$ \\begin{aligned} 0.5^k -1 + k0.5^{k-1}(100-k) = 0 \\ 0.5^k + 2k0.5^{k}(100-k) = 1 \\ \\text{Let} \\ 0.5^k = T \\ T - 200kT - 2k^2T =1 \\end{aligned} $$","title":"Thought Process"},{"location":"DSA/Graphs/","text":"A beautiful way to recurse in Python 1 2 3 def find_group_leader ( node : int ) -> int : # If node is its own leader, return it; otherwise, find the leader recursively return node if group_leader [ node ] == node else find_group_leader ( group_leader [ node ])","title":"Graphs"},{"location":"DSA/Trie/","text":"A Trie data structure is simply two things. An array of length (number of characters) and a boolean flag. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 struct Node { Node * links [ 26 ]; // Array of references to next nodes bool flag = false , // If true, no next reference exists in links array. bool contains_key ( char ch ) { return ( links [ ch - 'a' ] != NULL ); // ch - 'a' is the ASCII of char ch minus start -> 'a'. This gives you point in the array } void put_key ( char ch , Node * node ) { links [ ch - 'a' ] = node } Node * get_node ( char ch ) { return links [ ch - 'a' ]; } void mark_end () : flag = true ; } class Trie { public : Trie () { root new Node (); } void insert ( string word ) { Node * node = root ; for ( int i = 0 ; i <= word . length (); i ++ ) { char ch = word [ i ]; if ! root -> contains_key ( ch ) { root -> put_key ( ch , new Node ()) } node -> get_node ( ch ); } } } Every Trie node holds a reference to the next node. Where is the reference stored? >","title":"Trie"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/","text":"To resolve merge conflicts with master so that your changes override the conflicting changes from master , follow these steps: 1. Fetch and merge master 1 2 git fetch origin git merge origin/master If there are conflicts, Git will pause and mark the conflicting files. What is the difference between git fetch and git pull ?: git fetch downloads objects/refs from another repository (or remote) but doesn't merge them into your working directory. git pull does both: it fetches and then merges. 2. Overwrite conflicts with your version For each conflicted file: 1 git checkout --ours <file> --ours = your branch (i.e., your changes override master ) Repeat this for all conflicted files or run: 1 git diff --name-only --diff-filter = U | xargs git checkout --ours 3. Mark conflicts resolved 1 git add . 4. Complete the merge 1 git commit Alternative: Rebase with your changes overriding master 1 git rebase origin/master Then during each conflict: 1 2 3 git checkout --ours <file> git add <file> git rebase --continue Warning: This strategy discards all changes made in master for the conflicted files, so use it only when you're sure your changes must prevail.","title":"Resolve Merge Conflicts"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#1-fetch-and-merge-master","text":"1 2 git fetch origin git merge origin/master If there are conflicts, Git will pause and mark the conflicting files. What is the difference between git fetch and git pull ?: git fetch downloads objects/refs from another repository (or remote) but doesn't merge them into your working directory. git pull does both: it fetches and then merges.","title":"1. Fetch and merge master"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#2-overwrite-conflicts-with-your-version","text":"For each conflicted file: 1 git checkout --ours <file> --ours = your branch (i.e., your changes override master ) Repeat this for all conflicted files or run: 1 git diff --name-only --diff-filter = U | xargs git checkout --ours","title":"2. Overwrite conflicts with your version"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#3-mark-conflicts-resolved","text":"1 git add .","title":"3. Mark conflicts resolved"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#4-complete-the-merge","text":"1 git commit","title":"4. Complete the merge"},{"location":"Dark%20Arts/Resolve%20Merge%20Conflicts/#alternative-rebase-with-your-changes-overriding-master","text":"1 git rebase origin/master Then during each conflict: 1 2 3 git checkout --ours <file> git add <file> git rebase --continue Warning: This strategy discards all changes made in master for the conflicted files, so use it only when you're sure your changes must prevail.","title":"Alternative: Rebase with your changes overriding master"},{"location":"Dark%20Arts/Valeyere%20%282025%29/","text":"Keep two running trackers: - tracker A = \u201chow jumpy prices have been lately\u201d (volatility) - tracker B = \u201chow prices have moved lately\u201d (returns), but scaled by tracker A Update both after every new price. The ratio tells you how many standard deviations today\u2019s price move is, smoothed over time. Large positive = trend up, large negative = trend down. Step-by-step Compute a scale-free return $$r_{t}= \\frac{P_t-P_{t-1}}{P_{t-1}}\\quad\\text{or}\\quad\\ln(P_t/P_{t-1}) $$ Using percent/log makes the metric independent of the absolute price level. Update volatility (\u03c3\u00b2) first Exponential moving average of squared returns with decay \u03b7: $$\\sigma^{2} {t}= (1-\\eta)\\,\\sigma^{2} $$ This is a one-line \u201cmemory\u201d of recent squared moves; smaller \u03b7 = longer memory.}+\\eta\\,r_{t}^{2 Normalise today\u2019s return $$z_t = \\frac{r_t}{\\sigma_t+\\epsilon} $$ \u03b5 is a tiny constant to prevent division by zero. Update the signal Another exponential average, but of the z-scores: $$ \\phi_t = (1-\\eta)\\,\\phi_{t-1} + \\sqrt{\\eta}\\,z_t $$ The \\(\\sqrt{\\eta}\\) factor keeps the signal\u2019s variance near 1 when returns are white noise. So \u03c6 = +2.0 means: the exponentially weighted average of 112 day returns is two standard deviations above zero.","title":"Valeyere (2025)"},{"location":"Linear%20Algebra/Inverse%20of%20the%20Matrix/","text":"Tip A matrix that can be inverted is one that preserves the essential structure of the space\u2014 it may stretch, rotate, reflect, but it does not collapse dimensions or fold the space upon itself in an irreversible way . The determinant, that mysterious quantity, what is it but a measure of how the transformation changes the \"volume\" of space?","title":"Inverse of the Matrix"},{"location":"Linear%20Algebra/Linear%20Map/","text":"A linear map is a function that takes vectors, transforms them into new vectors, and makes sure that adding and scaling vectors still works the same way before and after the transformation. A linear map is just a rule that takes a vector and turns it into another vector without bending or breaking its structure . It respects addition and scaling. Think of it like applying a fixed transformation that preserves lines and the origin. Definition A linear map (or linear transformation ) is a function \\(T: V \\to W\\) between two [[Vector Space]] such that for all vectors \\(\\mathbf{u}, \\mathbf{v} \\in V\\) and all scalars \\(c \\in \\mathbb{R}\\) (or any field), it satisfies: Additivity : Transformation on addition is addition of transformations: \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) Homogeneity (scalar multiplication) : Transformation on a vector scalar product is same as scalar multiplied to a transformed vector. \\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) These two together mean: $$ T(a\\mathbf{u} + b\\mathbf{v}) = aT(\\mathbf{u}) + bT(\\mathbf{v}) $$ Key Properties The origin maps to origin : \\(T(\\mathbf{0}) = \\mathbf{0}\\) Linear maps can be represented as matrices when vector spaces are finite-dimensional. If \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) , then \\(T(\\mathbf{x}) = A\\mathbf{x}\\) for some \\(m \\times n\\) matrix \\(A\\) . Examples Scaling : \\(T(x) = 2x\\) \u2192 doubles every input \u2192 linear. Rotation in the plane \u2192 linear. \\(T(x) = x + 1\\) \u2192 not linear because it doesn\u2019t preserve the origin. Geometric Intuition A linear map: * Preserves straight lines. * Preserves ratios along lines. * Doesn\u2019t twist, curve, or displace the origin. Why It Matters Linear maps are the building blocks of all transformations in linear algebra. Every matrix is a linear map. They're fundamental in: Solving systems of equations. Understanding projections, rotations, shears. Machine learning (every neural net layer is mostly a linear map + nonlinearity).","title":"Linear Map"},{"location":"Linear%20Algebra/Linear%20Map/#definition","text":"A linear map (or linear transformation ) is a function \\(T: V \\to W\\) between two [[Vector Space]] such that for all vectors \\(\\mathbf{u}, \\mathbf{v} \\in V\\) and all scalars \\(c \\in \\mathbb{R}\\) (or any field), it satisfies: Additivity : Transformation on addition is addition of transformations: \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) Homogeneity (scalar multiplication) : Transformation on a vector scalar product is same as scalar multiplied to a transformed vector. \\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) These two together mean: $$ T(a\\mathbf{u} + b\\mathbf{v}) = aT(\\mathbf{u}) + bT(\\mathbf{v}) $$","title":"Definition"},{"location":"Linear%20Algebra/Linear%20Map/#key-properties","text":"The origin maps to origin : \\(T(\\mathbf{0}) = \\mathbf{0}\\) Linear maps can be represented as matrices when vector spaces are finite-dimensional. If \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) , then \\(T(\\mathbf{x}) = A\\mathbf{x}\\) for some \\(m \\times n\\) matrix \\(A\\) .","title":"Key Properties"},{"location":"Linear%20Algebra/Linear%20Map/#examples","text":"Scaling : \\(T(x) = 2x\\) \u2192 doubles every input \u2192 linear. Rotation in the plane \u2192 linear. \\(T(x) = x + 1\\) \u2192 not linear because it doesn\u2019t preserve the origin.","title":"Examples"},{"location":"Linear%20Algebra/Linear%20Map/#geometric-intuition","text":"A linear map: * Preserves straight lines. * Preserves ratios along lines. * Doesn\u2019t twist, curve, or displace the origin.","title":"Geometric Intuition"},{"location":"Linear%20Algebra/Linear%20Map/#why-it-matters","text":"Linear maps are the building blocks of all transformations in linear algebra. Every matrix is a linear map. They're fundamental in: Solving systems of equations. Understanding projections, rotations, shears. Machine learning (every neural net layer is mostly a linear map + nonlinearity).","title":"Why It Matters"},{"location":"Linear%20Algebra/Linear%20Regression%20as%20Projections/","text":"You start with a target vector \\(y \\in \\mathbb{R}^n\\) . The outcomes observed. Then you\u2019ve got a design matrix \\(X \\in \\mathbb{R}^{n \\times p}\\) . Each column of \\(X\\) is a predictor, and the span of those columns is a \\(p\\) -dimensional [[Subspace]] of \\(\\mathbb{R}^n\\) . Imagine \\(y\\) as a point floating in 3D space, and \\(X\\) as a tilted 2D plane. Regression doesn\u2019t invent a new direction; it just drops \\(y\\) straight down onto the plane. In higher dimensions, same story \u2014 only now the plane is a \\(p\\) -dimensional slab inside \\(\\mathbb{R}^n\\) . You\u2019ve got \\(y \\in \\mathbb{R}^3\\) , a point floating in space. You\u2019ve got \\(X\\) , whose columns span a tilted 2D plane slicing through the origin. The regression problem is: where does \\(y\\) land if you drop it straight down onto that plane? That landing spot is \\(\\hat{y} = X\\hat{\\beta}\\) . It\u2019s the orthogonal projection of \\(y\\) onto the column space of \\(X\\) . The residual \\(r = y - \\hat{y}\\) is the vertical drop \u2014 and it\u2019s perpendicular to the plane. So the question linear regression asks is brutally geometric: Among all possible linear combinations of the columns of \\(X\\) , which one is nearest to \\(y\\) ? Formally: find \\(\\hat{y} = X\\hat{\\beta}\\) such that $$ |y - \\hat{y}|_2 $$ is minimized. That\u2019s just saying: project \\(y\\) orthogonally onto the subspace \\(\\text{Col}(X)\\) . Now, what does it mean to be a projection? * If \\(\\hat{y}\\) is the projection, then the residual \\(r = y - \\hat{y}\\) must be orthogonal to the subspace. * Orthogonal to the subspace means orthogonal to each column of \\(X\\) . * Which gives the normal equations: $$ X^\\top (y - X\\hat{\\beta}) = 0. $$ That condition pins down \\(\\hat{\\beta}\\) . Solving it yields the familiar formula: $$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y. $$ So, the coefficients \\(\\hat{\\beta}\\) are nothing mystical \u2014 they\u2019re just the coordinates of the projection of \\(y\\) onto the span of the predictors. The whole operation can be packaged into a single matrix: $$ P = X (X^\\top X)^{-1} X^\\top. $$ This is the projection matrix . It\u2019s symmetric, idempotent ( \\(P^2 = P\\) ), and it maps any \\(y\\) to its fitted values: $$ \\hat{y} = Py. $$ That\u2019s linear regression boiled down: every time you \u201cfit a line,\u201d what you\u2019re actually doing is multiplying \\(y\\) by \\(P\\) and dropping it onto a lower-dimensional plane carved out by the predictors.","title":"Linear Regression as Projections"},{"location":"Linear%20Algebra/Matrix%20Multiplication/","text":"When I multiply A by B, I think of it as A acting on each column of B separately. Each column of the result is A transforming one of B's columns. Each row of C would be some combination of the rows of B, weighted by the corresponding row of A! That matrix multiplication represents the composition of linear transformations - the chaining together of ways to reshape space itself. but necessary for the transformations to be composable. And what of the result? If A is m\u00d7n and B is n\u00d7p, what can we say about AB? B transforms p-dimensional vectors to n-dimensional vectors, then A transforms those to m-dimensional vectors. The composition goes from p dimensions to m dimensions. First transformation B acts on space, then transformation A acts on the result. AB represents the combined effect - what happens when we perform B first, then A.","title":"Matrix Multiplication"},{"location":"Linear%20Algebra/Subspace/","text":"A subspace is just the span of some vectors that is every point you would reach if you were to take all possible linear combinations. Take \\(v_1, v_2, \\dots, v_k \\in \\mathbb{R}^n\\) . The subspace they generate is $$ \\text{span}{v_1, v_2, \\dots, v_k} = { a_1 v_1 + a_2 v_2 + \\cdots + a_k v_k : a_i \\in \\mathbb{R} }. $$ That set is guaranteed to: * contain the origin ( \\(0 = 0 \\cdot v_1 + \\cdots + 0 \\cdot v_k\\) ), * be closed under addition, * be closed under scalar multiplication. In \\(\\mathbb{R}^3\\) , this could be a line through the origin, a plane through the origin, or the whole space itself.","title":"Subspace"},{"location":"Linear%20Algebra/The%20Deeper%20Meaning%20of%20Matrix%20Transpose/","text":"What are Covectors? Covectors are linear functions that take a vector as input and produce a scalar (a number) as output. They are described as \"measuring devices\" that assign a value to a vector in a way that respects linearity. For example: If a covector measures vector \\(v_1\\) as \\(a_1\\) and \\(v_2\\) as \\(a_2\\) , it measures: \\(v_1 + v_2\\) as \\(a_1 + a_2\\) . If a vector \\(v_1\\) is scaled by a factor \\(\\lambda\\) , the covector measures \\(\\lambda v_1\\) as \\(\\lambda a_1\\) . Example in 2D: A covector represented as \\((a, b)\\) measures a vector \\((x, y)\\) by computing \\(ax + by\\) , a linear combination that yields a scalar.","title":"The Deeper Meaning of Matrix Transpose"},{"location":"Linear%20Algebra/The%20Deeper%20Meaning%20of%20Matrix%20Transpose/#what-are-covectors","text":"Covectors are linear functions that take a vector as input and produce a scalar (a number) as output. They are described as \"measuring devices\" that assign a value to a vector in a way that respects linearity. For example: If a covector measures vector \\(v_1\\) as \\(a_1\\) and \\(v_2\\) as \\(a_2\\) , it measures: \\(v_1 + v_2\\) as \\(a_1 + a_2\\) . If a vector \\(v_1\\) is scaled by a factor \\(\\lambda\\) , the covector measures \\(\\lambda v_1\\) as \\(\\lambda a_1\\) . Example in 2D: A covector represented as \\((a, b)\\) measures a vector \\((x, y)\\) by computing \\(ax + by\\) , a linear combination that yields a scalar.","title":"What are Covectors?"},{"location":"Linear%20Algebra/Vector%20Projection/","text":"Vector projection is equivalent to the dot product. The dot product of r and s is the projection of the vector \\(r\\) on the vector \\(s\\) . For two n component vectors, the dot product is defined as: $$ a . b = a_1b_1 + a_2b_2 \\dots + a_nb_n $$ Geometrically, for two vectors r and s, the projection appears as follows: It is equivalent to: $$ r . s = |r| |s| \\cos \\theta $$ Basis To be considered as a basis, a set of vectors must: - Be linearly independent - Span the space Basis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis). ![[Pasted image 20250429102423.png]] Change of Basis Suppose we have a vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) or \\(1\\hat{i} + 3\\hat{j}\\) . Here \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis vectors. These vectors are also known as a standard basis. Here, \\(\\hat{i}\\) is a unit vector in the \\(x\\) direction and \\(\\hat{j}\\) is a unit vector in the \\(y\\) direction. The vector \\(\\begin{pmatrix} 5 \\cr -1 \\end{pmatrix}\\) can be written as a linear combination of the basis vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) as follows: $$\\begin{pmatrix}1 & 0\\\\ 0 & 1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} $$ Now suppose we want to change the basis vectors to \\(\\begin{pmatrix} 1 \\cr 1 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 1 \\cr -1 \\end{pmatrix}\\) . Now instead of \\(\\hat{i}\\) and \\(\\hat{j}\\) , we have \\(\\hat{i'}\\) and \\(\\hat{j'}\\) . We can write the vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) as a linear combination of the new basis vectors as follows: $$\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} = 2 \\cdot \\begin{pmatrix}2 \\cr 3\\end{pmatrix} $$ Useful Links to Consult: https://www.nagwa.com/en/explainers/792181370490/","title":"Vector Projection"},{"location":"Linear%20Algebra/Vector%20Projection/#basis","text":"To be considered as a basis, a set of vectors must: - Be linearly independent - Span the space Basis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis). ![[Pasted image 20250429102423.png]]","title":"Basis"},{"location":"Linear%20Algebra/Vector%20Projection/#change-of-basis","text":"Suppose we have a vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) or \\(1\\hat{i} + 3\\hat{j}\\) . Here \\(\\hat{i}\\) and \\(\\hat{j}\\) are the basis vectors. These vectors are also known as a standard basis. Here, \\(\\hat{i}\\) is a unit vector in the \\(x\\) direction and \\(\\hat{j}\\) is a unit vector in the \\(y\\) direction. The vector \\(\\begin{pmatrix} 5 \\cr -1 \\end{pmatrix}\\) can be written as a linear combination of the basis vectors \\(\\hat{i}\\) and \\(\\hat{j}\\) as follows: $$\\begin{pmatrix}1 & 0\\\\ 0 & 1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} $$ Now suppose we want to change the basis vectors to \\(\\begin{pmatrix} 1 \\cr 1 \\end{pmatrix}\\) and \\(\\begin{pmatrix} 1 \\cr -1 \\end{pmatrix}\\) . Now instead of \\(\\hat{i}\\) and \\(\\hat{j}\\) , we have \\(\\hat{i'}\\) and \\(\\hat{j'}\\) . We can write the vector \\(\\begin{pmatrix} 1 \\cr 3 \\end{pmatrix}\\) as a linear combination of the new basis vectors as follows: $$\\begin{pmatrix}1 & 1\\\\ 1 & -1\\end{pmatrix} \\times \\begin{pmatrix} 5 \\cr -1 \\end{pmatrix} = 2 \\cdot \\begin{pmatrix}2 \\cr 3\\end{pmatrix} $$","title":"Change of Basis"},{"location":"Linear%20Algebra/Vector%20Projection/#useful-links-to-consult","text":"https://www.nagwa.com/en/explainers/792181370490/","title":"Useful Links to Consult:"},{"location":"Linear%20Algebra/Vector%20Space/","text":"TL;DR (First Principles): A vector space is where linear algebra happens. If you can: * Add any two elements, * Multiply them by any scalar, * And still stay inside the same set, Then you're in a vector space. Examples \\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) . Formal Definition A vector space \\(V\\) over a field \\(\\mathbb{F}\\) (usually \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) ) is a set equipped with two operations: Vector addition : \\(\\mathbf{u} + \\mathbf{v} \\in V\\) Scalar multiplication : \\(c\\mathbf{v} \\in V\\) , where \\(c \\in \\mathbb{F}\\) These operations must satisfy 8 axioms (rules), such as: Closure under addition and scalar multiplication Associativity , commutativity of addition Distributivity of scalar multiplication over scalars and vectors Existence of zero vector and additive inverses Multiplying by 1 leaves vector unchanged Intuition If you can: Add any two elements, Multiply them by any scalar, And still stay inside the same set, Then you're in a vector space. Examples \\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) . Non-Examples Vectors with a non-standard addition (e.g., max instead of +) Vectors that can\u2019t be scaled (e.g., integers aren't closed under division) Why It Matters Vector spaces provide the stage for linear algebra. Once you\u2019re in a vector space: You can apply linear maps (transformations). You can define bases , dimension , and coordinates . Concepts like span, linear independence, subspaces, and projections make sense. In short, vector spaces are where linear algebra happens .","title":"Vector Space"},{"location":"Linear%20Algebra/Vector%20Space/#examples","text":"\\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) .","title":"Examples"},{"location":"Linear%20Algebra/Vector%20Space/#formal-definition","text":"A vector space \\(V\\) over a field \\(\\mathbb{F}\\) (usually \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) ) is a set equipped with two operations: Vector addition : \\(\\mathbf{u} + \\mathbf{v} \\in V\\) Scalar multiplication : \\(c\\mathbf{v} \\in V\\) , where \\(c \\in \\mathbb{F}\\) These operations must satisfy 8 axioms (rules), such as: Closure under addition and scalar multiplication Associativity , commutativity of addition Distributivity of scalar multiplication over scalars and vectors Existence of zero vector and additive inverses Multiplying by 1 leaves vector unchanged","title":"Formal Definition"},{"location":"Linear%20Algebra/Vector%20Space/#intuition","text":"If you can: Add any two elements, Multiply them by any scalar, And still stay inside the same set, Then you're in a vector space.","title":"Intuition"},{"location":"Linear%20Algebra/Vector%20Space/#examples_1","text":"\\(\\mathbb{R}^n\\) : Ordinary n-dimensional real space. Set of all \\(n \\times m\\) real matrices. Set of all polynomials with real coefficients. Set of all continuous functions on an interval. Set of 2D vectors lying in a plane through the origin in \\(\\mathbb{R}^3\\) .","title":"Examples"},{"location":"Linear%20Algebra/Vector%20Space/#non-examples","text":"Vectors with a non-standard addition (e.g., max instead of +) Vectors that can\u2019t be scaled (e.g., integers aren't closed under division)","title":"Non-Examples"},{"location":"Linear%20Algebra/Vector%20Space/#why-it-matters","text":"Vector spaces provide the stage for linear algebra. Once you\u2019re in a vector space: You can apply linear maps (transformations). You can define bases , dimension , and coordinates . Concepts like span, linear independence, subspaces, and projections make sense. In short, vector spaces are where linear algebra happens .","title":"Why It Matters"},{"location":"Linear%20Algebra/Vector%20Subspace/","text":"A subspace is just a smaller vector space sitting inside a bigger one. It\u2019s a set of vectors closed under addition and scalar multiplication. Think of it as any line or plane through the origin that you can move around in freely, but you can\u2019t \u201cstep out\u201d of it by adding or scaling vectors. A classic example of a subspace is all vectors in 3D space whose last component is zero, like (x, y, 0). This forms a flat plane through the origin. It contains the zero vector, always stays on the plane when you add vectors or multiply by numbers, and never escapes its own \u201cclubhouse\u201d. An example of something that is not a subspace : all points in 2D space where x + y = 1. This is a line, but it doesn\u2019t go through the origin, so it fails the subspace test\u2014the zero vector (0, 0) isn\u2019t on this line, and scalar multiplication can take you off the line.[5][3] Q: What if the set is a line through the origin? A: That's always a subspace\u2014origin is included, and the other rules work out. Q: Why does missing the origin break the rules? A: The zero vector (all zeros) must always be part of every subspace. If not, it's out! Q: Is the set of all positive vectors a subspace? A: No\u2014the sum of two positive vectors is positive, but multiplying by a negative number gives negatives, which aren\u2019t in the set. Q: So subspaces are \u201cclosed\u201d under addition and multiplication? A: Exactly! They keep everything in their \u201cclubhouse.\u201d","title":"Vector Subspace"},{"location":"Polish/Connectors/","text":"Here\u2019s a concise Polish connecting words (conjunctions) cheat sheet with English meanings and usage context: Cause / Reason Polish English Example bo because Nie id\u0119, bo jestem chory. poniewa\u017c because / since Zosta\u0142em, poniewa\u017c pada\u0142o. gdy\u017c because (formal) Nie przyszed\u0142, gdy\u017c by\u0142 zaj\u0119ty. Effect / Result Polish English Example dlatego that\u2019s why Pada, dlatego nie id\u0119. wi\u0119c so Jest p\u00f3\u017ano, wi\u0119c id\u0119 spa\u0107. zatem therefore (formal) By\u0142o ciemno, zatem zapali\u0142 \u015bwiat\u0142o. Contrast / Opposition Polish English Example ale but Chc\u0119 i\u015b\u0107, ale jestem zm\u0119czony. jednak however / yet By\u0142o ciep\u0142o, jednak pada\u0142o. lecz but / yet (formal) Chcia\u0142, lecz nie m\u00f3g\u0142. mimo \u017ce although Mimo \u017ce pada\u0142o, wyszed\u0142. Purpose Polish English Example \u017ceby so that / in order to Ucz\u0119 si\u0119, \u017ceby zda\u0107 egzamin. aby so that (formal) Pracuj\u0119, aby mie\u0107 pieni\u0105dze. Condition Polish English Example je\u015bli if Je\u015bli chcesz, mo\u017cemy i\u015b\u0107. je\u017celi if (formal) Je\u017celi pada, zostaj\u0119 w domu. gdy when / if Gdy b\u0119dzie gotowe, dam zna\u0107. Want more categories (time, comparisons, etc.) or flashcard format?","title":"Connectors"},{"location":"Polish/notes/","text":"Serial, Polish, English, Pronunciation 1, Kto to jest , Who is this?, Kto to yest 2, Pami\u0119tam , I remember, Paa-mie(n)-entam 3, Czytamy , To read, Chetame 4, Ale , But, Aale 5, Dlaczego , Why, Dlachego 6, Z\u0142y , Bad, Zwee 7, Zeszyt , Notebook, Zesh-zet 8, Gumka , Eraser 9, P\u0142yn , Liquid, Pwin 10, D\u0142ugopis , Pen , Dwoo-go-pis 11, Okno , Window, Okno 12, Klucz , Key, Kluch 13, Ksi\u0105\u017cka , Book, Kshyanka 14, Tablica , Board, Taa-blee-tsa 15, Krzes\u0142o , Chair, Ksheshwo 16, Segregator , Folder 17, Kubek , Mug 18, Gumka , Eraser 19, Mapa , Map 20, Znany , Known / Famous, 21, Nieznany , Unknown 22, Znajomy , Friend, Znayome 23, To jest Kom\u00f3rka , This is a cellphone 24, A co to jest? , And what is this?, Aa so to yest? 24, To jestem ja , This is me, To yestem ya 25, Kole\u017canka , Classmate, Kolezhanka 26, Wakacje , Holidays, Vakatsye 27, Zdj\u0119cie , Photo, 28, Prawda , Truth 29, Fa\u0142sz , Lie 30, Modna , Fashionable 31, Nic Specjalnego , Nothing special, Nits Spets-yalnego 32, O\u0142\u00f3wek , Pencil 33, Mam tylko jeden , I have only one 34, Ja mam d\u0142ugopis , I have a pen 35, Mami , Mom 36, Tati , Dad 37, Przystojny , Handsome 38, I co tam jest? , And what's there? 39, Mam Pyntanie , I have a question 40, Modne , Fashionable 41, Rodzaj , Kind 42, Nijaki , Neutral 43, Na Pocz\u0105tku , At the beginning 44, Na ko\u0144cu , At the end, Naa koen-tsu 45, Mi\u0142y , Nice, Meewe 46, Bogaty , Rich 47, Biedny , Poor 48, Pomara\u0144czowy , Orange 49, Zielony , Green 50, Rozowy , Pink 51, Niebieski , Blue 52, Czerwony , Red 53, Granatowy , Navy Blue 54, Br\u0105zowy , Brown 55, Szary , Grey 56, Srebrny , Silver 57, Z\u0142oty , Gold 58, Czarny , Black 59, Bia\u0142y , White 60, \u017b\u00f3\u0142ty , Yellow 61, Jak si\u0119 masz? , How are you? 62, Dobrze , Good 63, \u0179le , Bad 64, S\u0142owa , Words 65, Zdania , Sentences 66, Czytamy , We read 67, Ta Kom\u00f3rka , This cellphone 68, Ten artysta jest przystojny , This artist is handsome 69, Ta pani jest bogata , This lady is rich 70, Stolik , Table 71, Ta gumka jest r\u00f3\u017cowa , This eraser is pink 72, Ta Kom\u00f3rka jest czarna , This cellphone is black 73, Ta torba jest fioletowa , This bag is purple 74, Ten Klucz jest srebrny , This key is silver 75, Pantera , Panther 76, Humor , Mood 77, Niebo , Sky 78, Czy to jest kawa? , Is this coffee? 79, O\u0142owek jest szara , The pencil is grey 80, Znam Angielski , I know English 81, Co to znaczy? , What does it mean? 82, Zart , Joke 83, Pami\u0119tam , I remember 84, Ale , But 85, \u015bwietnie , Great 86, Troch\u0119 , A little 87, Wiem , I know 88, Racj\u0119 , Right 89, Mnie Te\u017c , Me too 90, i teraz , And now 91, ja lubi\u0119 , I like 92, ty lubisz , You like 93, Co to ciebie , What about you? 94, Oni mieszkaj\u0105 , They live 95, \u0142atwo , Easy 96, My mieszkamy , We live 97, Czy oni ta\u0144cz\u0105 sals\u0119? , Do they dance salsa? 98, Wy mieszkacie w Warszawie , You live in Warsaw 99, Palicie , You smoke 100, My , We 101, Ona lubi , She likes 102, Oni lubi\u0105 , They like 103, On lubi , He likes 104, One lubi\u0105 , They like 105, Oni nie pal\u0105 , They do not smoke 106, Ja cieszy\u0107 si\u0119 , I enjoy 107, Smutna , Sad 108, My\u015ble\u0107 , To think 109, Wysoki , Tall 110, Niski , Short 111, Trudny , Difficult 112, Zdrowy , Healthy 113, Chora , Sick 114, Brudna , Dirty 115, Czysta , Clean 116, Gruby , Fat 117, Szczup\u0142y , Slim 118, M\u0142ody , Young 119, Stary , Old 120, \u0141adny , Pretty 121, Brzydki , Ugly 122, Zm\u0119czony , Tired 123, Leniwa , Lazy 124, Pi\u0119kna i Smutna , Beautiful and sad 125, Ciekawe, jaki on jest , I wonder what he is like 126, Umie szybko decydowa\u0107 , He can decide quickly 127, Zdecydowany , Decisive 128, Wie, ile jest warty i jakie ma mo\u017cliwo\u015bci , He knows how much he is worth and what he is capable of 129, Nie lubi m\u00f3wi\u0107 o swoich uczuciach , He does not like to talk about his feelings 130, Otwarta , Open 131, Chaotyczny , Chaotic 132, Uczuciach , Feelings 133, Towarzyski , Sociable 134, Ciekawy , Curious 135, Energiczny , Energetic 136, Romanticzny , Romantic 137, \u017byczliwo\u015b\u0107 , Kindness 138, Szczery , Honest 139, Weso\u0142y , Cheerful 140, Nie pami\u0119tam dobrze , I do not remember well 141, Jestem pewny \u017ce ona jest energiczna, spontaniczna i bardzo sympatyczna , I am sure she is energetic, spontaneous and very nice 142, Atrakcyjna , Attractive 143, Zdanie , Sentence 144, 11, Jedena\u015bcie, Yedenashchie 145, 12, Dwana\u015bcie, Dvanashchie 146, 13, Trzyna\u015bcie, Trinashchie 147, 14, Czterna\u015bcie, Chetirnashchie 148, 15, Pi\u0119tna\u015bcie, Pyentnashchie 149, 16, Szesna\u015bcie, Shesnashchie 150, 17, Siedemna\u015bcie 151, 18, Osiemna\u015bcie 152, 19, Dziewi\u0119tna\u015bcie 153, 20, Dwadzie\u015bcia 154, 10, Dziesi\u0119\u0107 155, 20, Dwadzie\u015bcia 156, 30, Trzydzie\u015bci 157, 40, Czterdzie\u015bci 158, 50, Pi\u0119\u0107dziesi\u0105t 159, 60, Sze\u015b\u0107dziesi\u0105t 160, 70, Siedemdziesi\u0105t 161, 80, Osiemdziesi\u0105t 162, 90, Dziewi\u0119\u0107dziesi\u0105t 163, 100, Sto 164, Ile masz lat? , How old are you? 165, Mam 24 lata , I am 20 years old 166, Mam 25 lat , I am 25 years old 167, Mam 22 lata , I am 22 years old 168, Mam 23 lata , I am 23 years old 169, Mam 21 lat , I am 21 years old 170, Aktor , Actor 171, Aktorka , Actress 172, Piosenkarz , Singer 173, Piosenkarka , Singer 174, Muzyk , Musician 175, Zawody , Professions, Zaaavody 176, Lekarz , Doctor 177, Kucharz , Cook, kukharz 178, In\u017cynier , Engineer, Inzineer 179, Policjant , Policeman, Politsyant 180, Dziennikarz , Journalist, 181, Kelner , Waiter 182, Informatyk , IT specialist 183, Rolnik , Farmer 184, Sprzedawca , Salesman, 185, Emeryt , Pensioner 186, Kelner , Waiter 187, Tancerz , Dancer, Taanserka 188, Urz\u0119dnik , Clerk, 189, Kim jeste\u015b? , Who are you? 190, Jestem Analitykiem Ryzyka, I am a Risk analyst 191, \u017bona, Wife 192, Syn, Son, Sen 193, Skomplikowany, Complicated 194, Gram na gitarze, I play the guitar 196, Tu jest moja rodzina, Here is my family 197, Utalentowana, Talented, Ootalentovana 198. Ksi\u0119gowa, Accountant, Ksyegova 199, Krawiec, Tailor, Krawiets 200, Stolarz, Carpenter, Stolarz 201, Kierowca, Driver, Kierovtsa 202, Listonosz, Postman, Listonosh 203, Stra\u017cak, Fireman, Strazhak 204, Piel\u0119gniarka, Nurse, Pieliegnarka 205, Graj\u0105 na gitarze, They play the guitar 206, Co pan robi? , What are you doing? 207, Chwileczk\u0119, Just a moment, Khweeletschka 208, Nudz\u0119 si\u0119, I am bored, Noodzhe she 209, Chodzi\u0107 na spacery, To go for a walk, Khodzits na spatseree 210, Robi\u0107 Zdj\u0119cia, To take pictures, Robich Zdjenshia 211, Czyta\u0107 ksi\u0105\u017cki, To read books, Shitach ksiyonszhki 212, P\u0142ywa\u0107, To swim, Pwivach 213, Robi\u0107 Zdj\u0119cia, To take pictures, Robich Zdjenshia 214, Lubi\u0119 czyta\u0107 ksi\u0105\u017cki, I like to read books, Lubien shitach ksiyonszhki 215, Mam dwa psy, I have two dogs, Mam Dva psi 216, On ma psa, He has a dog, On ma psa 217, Politycy Przepraszaj\u0105 za swoje b\u0142\u0119dy, Politicians apologize for their mistakes, Politsytsi Przepraszayon za svoye bwendy 218, Oni maj\u0105 psa, They have a dog, Oni mayom psa 219, Ona pyta o drog\u0119, She asks for directions, Ona peta o drogen 220, Czy ty mowisz po angielsku? , Do you speak English? 221, Ja my\u015bl\u0119, \u017ce polski jest trudny, I think Polish is difficult, Ya meswen, zhe polski yest troodne 222, Czy oni ta\u0144cz\u0105 sals\u0119? , Do they dance salsa?, Shay oni taanshchayon salsen 223, Zdanje, Sentence, Zdanye 224, Nie rozumiem to zdanie, I do not understand this sentence, Nye rozoomyem to zdanye 225, A ty co lubisz robi\u0107? , And what do you like to do?, A ti tso lubish robich 226, To fantastyczne, It is fantastic, To fantastishne 227, Idziemy ta\u0144czy\u0107, Let's go dancing, Idzhiemy taanshich 228, Gospodyni, Hostess, Gospodeni 229, Gospodarz, Host, Gospodarz 230, Co robisz po lekcji, What are you doing after class, Tso robish po lektsyi 231, No nie wiem, Well, I do not know, No nye vyem 232, Mami mailuje, Mom is emailing, Mami maylooye 233, Oni du\u017co podr\u00f3\u017cuj\u0105, They travel a lot, Oni doozho podroozhooyon 234, Co ty teraz gotujesz? , What are you cooking now?, Tso ti teraz gotuyesh 234, Uwe telephonuje, Uwe is calling, Uwe telephonoye 235, Gdjie jestes spaceruj\u0119 po parku, Where are you walking in the park, Gdjie yestesh spatserooyen po parkoo 236, Czy wy pracujecie w weekendy? , Do you work on weekends?, Tsi vi pratsuyetse v vyekendy 237, Relaxowa\u0107, To relax, Relaksowach 238, Je\u017cd\u017c\u0119 na rowerze, I ride a bike, Yezhdzhe na rovetshe 239, Chodz\u0119 na zakupy, I go shopping, Khodzhe na zakopy 240, Ogl\u0105dam telewizj\u0119, I watch TV, Oglondam televeezyen 241, Lubisz s\u0142ucha\u0107 muzyki? , Do you like listening to music?, Lubish swhukhach mooziky 243, Gra\u0107 w pi\u0142k\u0119 no\u017cn\u0105, To play football, Grach v pionka nozhna 244, Jestem Zm\u0119czany, I am tired, Yestem zmenchany 245, Masz wolny czas czy jeste\u015b zaj\u0119ty? , Do you have free time or are you busy?, Mash volny chas tsi yestesh zayenty 246, Rozumiem troch\u0119 po polsku, I understand a little Polish, Rozoomyem trokhe po polsku 247, Jutro, Tomorrow, Yootro 248, Wczoraj, Yesterday, Vchoray 249, Dzi\u015b, Today, Dzish 250, Przystojnego brata, Handsome brother, Pristoyenego brata 251, \u0142adn\u0105 siostr\u0119, Pretty sister, Ladnaun siostraun 252, Monika lubi kaw\u0119, Monika likes coffee, Monika lubi kave 253, Lubi\u0119 muzyk\u0119 klasyczn\u0105, I like classical music, Lubie mooziky klasichnay 254, Ochot\u0119 na kaw\u0119, I feel like coffee, Okhoty na kave 255, sp\u00f3\u0142g\u0142oska, consonant, spoolgoshka 256, Czy co\u015b jeszcze, Anything else, Tsi tsoys yeshche 257, Lubi\u0119 polsk\u0105 literatur\u0119, I like Polish literature, Lubie polskaun literatooren 258, Pada deszcz, It is raining, Pada deshch 259, Czy c\u00f3s jeszcze, Anything else, Tsi tsoys yeshche 260, Pi\u0119tro, Floor, Pyentro 261, Opcja, Option, Optsya 262, S\u0142odkie Czerwony, Sweet Red, Swodkie Chervony 263, Morela, Apricot, Morela 264, Brzoskwinia, Peach, Brzoskvinia 265, Seler, Celery, Seler 266, Cebula, Onion, Tseboola 267, Czosnek, Garlic, Chosnek 268, Czerwona papryka, Red pepper, Chervona paprika 269, Pomidor, Tomato, Pomidor 270, Og\u00f3rek, Cucumber, Ogorek 271, Sa\u0142ata, Lettuce, Salata 272, Marchewka, Carrot, Markhevka 273, Ziemniak, Potato, Zhemnyak 274, Jab\u0142ko, Apple, Yabko 275, Gruszka, Pear, Grooshka 276, Banan, Banana, Banan 277, Cytryna, Lemon, Tsitryna 278, Pomara\u0144cza, Orange, Pomarancha 279, Mandarynka, Tangerine, Mandarynka 280, Grejpfrut, Grapefruit, Greypfroot 281, Winogrono, Grape, Vinogrono 282, Malina, Raspberry, Malina 283, Truskawka, Strawberry, Trooskafka 284, Wi\u015bnia, Cherry, Veesnya 285, Kszta\u0142t, Shape, Kshtawt 286, S\u0142odki, Sweet, Swodki 287, Kwa\u015bny, Sour, Kwashny 288, Gorzki, Bitter, Gorzki 289, Ostry, Spicy, Ostry 290, S\u0142ony, Salty, Swony 291, Delikatny, Delicate, Delikatny 292, Pod\u0142uzny, Longitudinal, Podwoozhny 293, Okr\u0105g\u0142y, Round, Okrongwy 294, Kwadratowy, Square, Kwadratowy 295, Tr\u00f3jk\u0105tny, Triangular, Trookontny 296, Prostok\u0105tny, Rectangular, Prostokontny 297, Pomara\u0144cza jest kwa\u015bna i okr\u0105g\u0142a, The orange is sour and round, Pomarancha yest kwashna i okrongwa","title":"Notes"},{"location":"Polish/rules/","text":"a is female o and e is child everything else is male How to identify the gender in Polish? \u0142, k, g, h - Rodzaj m\u0119ski a, \u0119, i, y - Rodzaj \u017ce\u0144ski o, e, m - Rodzaj nijaki Jaki is male Jaka is female Jakie is child For human male subjects Use 'tego' for adjectives Use 'a' for nouns Use 'o' for things For human female subjects Use '\u0105' for adjectives Use '\u0119' for nouns 'Wy' - you (plural) - 'Wy jeste\u015bcie' - you are 'My' - We (plural) - 'My jeste\u015bmy' - we are 'Ja' - I (singular) - 'Ja jestem' - I am 'Ty' - You (singular) - 'Ty jeste\u015b' - you are 'On' - He (singular) - 'On mieszka' - he lives 'Ona' - She (singular) - 'Ona mieszka' - she lives 'Ono' - It (singular) - 'Ono mieszka' - it lives 'Oni' - They (plural) - 'Oni mieszkaj\u0105' - they live Adjectives This is dirty - To jest brudne This is clean - To jest czyste He is fat - On jest gruby He is slim - On jest szczup\u0142y He is young - On jest m\u0142ody He is old - On jest stary She is pretty - Ona jest \u0142adna She is ugly - Ona jest brzydka She is sick - Ona jest chora He is healthy - On jest zdrowy This tea is hot - Ta herbata jest gor\u0105ca I am rich - Ja jestem bogata I am poor - Ja jestem biedna This drink is cold - Ten nap\u00f3j jest zimny Are you old? - Czy ty jeste\u015b stary? No I am young - Nie, ja jestem m\u0142ody Polish is easy - Polski jest \u0142atwy Polish is difficult - Polski jest trudny","title":"Rules"},{"location":"Polish/rules/#how-to-identify-the-gender-in-polish","text":"\u0142, k, g, h - Rodzaj m\u0119ski a, \u0119, i, y - Rodzaj \u017ce\u0144ski o, e, m - Rodzaj nijaki Jaki is male Jaka is female Jakie is child For human male subjects Use 'tego' for adjectives Use 'a' for nouns Use 'o' for things For human female subjects Use '\u0105' for adjectives Use '\u0119' for nouns 'Wy' - you (plural) - 'Wy jeste\u015bcie' - you are 'My' - We (plural) - 'My jeste\u015bmy' - we are 'Ja' - I (singular) - 'Ja jestem' - I am 'Ty' - You (singular) - 'Ty jeste\u015b' - you are 'On' - He (singular) - 'On mieszka' - he lives 'Ona' - She (singular) - 'Ona mieszka' - she lives 'Ono' - It (singular) - 'Ono mieszka' - it lives 'Oni' - They (plural) - 'Oni mieszkaj\u0105' - they live","title":"How to identify the gender in Polish?"},{"location":"Polish/rules/#adjectives","text":"This is dirty - To jest brudne This is clean - To jest czyste He is fat - On jest gruby He is slim - On jest szczup\u0142y He is young - On jest m\u0142ody He is old - On jest stary She is pretty - Ona jest \u0142adna She is ugly - Ona jest brzydka She is sick - Ona jest chora He is healthy - On jest zdrowy This tea is hot - Ta herbata jest gor\u0105ca I am rich - Ja jestem bogata I am poor - Ja jestem biedna This drink is cold - Ten nap\u00f3j jest zimny Are you old? - Czy ty jeste\u015b stary? No I am young - Nie, ja jestem m\u0142ody Polish is easy - Polski jest \u0142atwy Polish is difficult - Polski jest trudny","title":"Adjectives"},{"location":"Polish/sentences/","text":"Serial, Polish, English, Rule 1, Lubi\u0119 je\u015b\u0107 bu\u0142ka z mas\u0142em i pi\u0107 herbat\u0119 na \u015bniadanie., I like to eat bread with butter and drink tea for breakfast., 2, Ja wol\u0119 je\u015b\u0107 kanapka z serem, I prefer to eat a sandwich with cheese, With 'Ja' use '\u0119'. 3, Wol\u0119 pi\u0107 indii herbat\u0119 4, Ja lubi\u0119 je\u015b\u0107 kanapka z serem i pi\u0107 herbat\u0119 na \u015bniadanie., 5,","title":"Sentences"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/","text":"Quadratic utility is defined as: $$ U(W) = aW - bW^2 $$ where: - \\(W\\) = wealth, - \\(a, b > 0\\) are constants. Why utility function is assumed this way? If utility is quadratic: $$ U(W) = aW - bW^2 $$ then the expected utility is: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b \\mathbb{E}[W^2] $$ Expand \\(\\mathbb{E}[W^2]\\) using the formula: $$ \\mathbb{E}[W^2] = (\\mathbb{E}[W])^2 + \\text{Var}(W) $$ This comes from the relationship: \\(\\text{Var}(X) = E(X)^2 - E(X^2)\\) thus: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b\\left((\\mathbb{E}[W])^2 + \\text{Var}(W)\\right) $$ Grouping terms: $$ \\mathbb{E}[U(W)] = (a - 2b \\mathbb{E}[W])\\mathbb{E}[W] - b \\, \\text{Var}(W) $$ Interpretation: - You maximize expected utility by increasing expected return \\(\\mathbb{E}[W]\\) - and minimizing variance \\(\\text{Var}(W)\\) . Thus, maximizing expected quadratic utility reduces to a trade-off between expected return and variance , which is exactly the objective of mean-variance optimization . Why Mean Variance Optimization is a projection problem? You are projecting the expected return vector \\(\\alpha\\) onto the weight space, using a distance measured by the covariance matrix \\(\\Omega\\) . The space (portfolio space or weight space) is \\(\\mathbb{R}^n\\) for \\(n\\) assets. Instead of normal Euclidean distance, you use risk-based distance : $$ d(w) = \\sqrt{w^T \\Omega w} $$ This risk-based distance stretches and twists the space \u2014 the geometry is no longer flat like Euclidean space. \"We are finding the weight vector \\(\ud835\udc64\\) that points in the best possible direction towards the return vector \\(\\alpha\\) , but measured using risk distance defined by \\(\\omega\\) . Optimization Problem $$ \\max_w \\quad \\alpha^T w \\quad \\text{subject to} \\quad w^T \\Omega w \\leq \\sigma^2 $$ Lagrangian leads to: $$ \\Omega w \\propto \\alpha \\quad \\Rightarrow \\quad w \\propto \\Omega^{-1} \\alpha $$ Thus, optimal weight is proportional to \\(\\Omega^{-1} \\alpha\\) \u2014 meaning alpha corrected by risk geometry .","title":"Mean Variance Portfolio Optimization"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/#why-utility-function-is-assumed-this-way","text":"If utility is quadratic: $$ U(W) = aW - bW^2 $$ then the expected utility is: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b \\mathbb{E}[W^2] $$ Expand \\(\\mathbb{E}[W^2]\\) using the formula: $$ \\mathbb{E}[W^2] = (\\mathbb{E}[W])^2 + \\text{Var}(W) $$ This comes from the relationship: \\(\\text{Var}(X) = E(X)^2 - E(X^2)\\) thus: $$ \\mathbb{E}[U(W)] = a \\mathbb{E}[W] - b\\left((\\mathbb{E}[W])^2 + \\text{Var}(W)\\right) $$ Grouping terms: $$ \\mathbb{E}[U(W)] = (a - 2b \\mathbb{E}[W])\\mathbb{E}[W] - b \\, \\text{Var}(W) $$ Interpretation: - You maximize expected utility by increasing expected return \\(\\mathbb{E}[W]\\) - and minimizing variance \\(\\text{Var}(W)\\) . Thus, maximizing expected quadratic utility reduces to a trade-off between expected return and variance , which is exactly the objective of mean-variance optimization .","title":"Why utility function is assumed this way?"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/#why-mean-variance-optimization-is-a-projection-problem","text":"You are projecting the expected return vector \\(\\alpha\\) onto the weight space, using a distance measured by the covariance matrix \\(\\Omega\\) . The space (portfolio space or weight space) is \\(\\mathbb{R}^n\\) for \\(n\\) assets. Instead of normal Euclidean distance, you use risk-based distance : $$ d(w) = \\sqrt{w^T \\Omega w} $$ This risk-based distance stretches and twists the space \u2014 the geometry is no longer flat like Euclidean space. \"We are finding the weight vector \\(\ud835\udc64\\) that points in the best possible direction towards the return vector \\(\\alpha\\) , but measured using risk distance defined by \\(\\omega\\) .","title":"Why Mean Variance Optimization is a projection problem?"},{"location":"Portfolio%20Management/Mean%20Variance%20Portfolio%20Optimization/#optimization-problem","text":"$$ \\max_w \\quad \\alpha^T w \\quad \\text{subject to} \\quad w^T \\Omega w \\leq \\sigma^2 $$ Lagrangian leads to: $$ \\Omega w \\propto \\alpha \\quad \\Rightarrow \\quad w \\propto \\Omega^{-1} \\alpha $$ Thus, optimal weight is proportional to \\(\\Omega^{-1} \\alpha\\) \u2014 meaning alpha corrected by risk geometry .","title":"Optimization Problem"},{"location":"Quant%20Finance/Factor%20Mimicking%20Portfolio/","text":"Daily asset returns r_{i,t} for a liquid universe. Daily VIX return f_{\\text{VIX},t} and any other observed factors f_{\\ell,t}. Estimate betas (time-series regressions) r_{i,t} \\;=\\; \\beta_{i,\\text{VIX}}\\,f_{\\text{VIX},t}\\;+\\;\\sum_{\\ell\\neq\\text{VIX}}\\beta_{i,\\ell}\\,f_{\\ell,t}\\;+\\;\\varepsilon_{i,t}. Collect all \\beta_{i,\\cdot} into the loading matrix B and The phrase means: once you\u2019ve run each time\u2010series regression r_{i,t} \\;=\\;\\sum_{\\ell}\\beta_{i,\\ell}\\,f_{\\ell,t}\\;+\\;\\varepsilon_{i,t}, you collect all \\beta_{i,\\ell} into one matrix \\[ B = \\begin{pmatrix} \\beta_{1,1} & \\beta_{1,2} & \\cdots\\\\ \\beta_{2,1} & \\beta_{2,2} & \\cdots\\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix}, \\] and you form the vector of residuals for each asset \\varepsilon_{i,t}. Then \\Omega_{\\varepsilon} \\;=\\; \\operatorname{Cov}\\bigl(\\varepsilon_{t}\\bigr) is the N\\times N covariance matrix whose (i,j) entry is \\mathbb{E}[\\varepsilon_{i,t}\\,\\varepsilon_{j,t}].","title":"Factor Mimicking Portfolio"},{"location":"Quant%20Finance/Fixed%20Income/","text":"Deterministic Cashflow","title":"Fixed Income"},{"location":"Quant%20Finance/GARCH%20%281%2C1%29/","text":"The general principle is to model volatility as a time-varying, latent stochastic process whose current value depends on past information.","title":"GARCH (1,1)"},{"location":"Quant%20Finance/Linear%20Model%20of%20Returns/","text":"Volatility Decomposition $$ \\Omega_r = B\\, \\Omega_f\\, B^T + \\Omega_\\epsilon $$ This equation tells us what causes the ups and downs in a group of asset returns, and how those sources mix together. If you want to understand risk, build portfolios, or analyze any group of assets, this formula is a fundamental \u201crecipe\u201d for how returns vary together. Start from the recipe for asset returns: $$ r = Bf + \\epsilon $$ The covariance (how things wiggle together) of \\(r\\) is: $$ \\Omega_r = \\mathrm{Var}(r) $$ Plug in our recipe: $$ = \\mathrm{Var}(Bf + \\epsilon) $$ If errors and factors are independent, the variance of a sum is just the sum of variances: $$ = \\mathrm{Var}(Bf) + \\mathrm{Var}(\\epsilon) $$ When you scale factors by B, variances get mapped like so: $$ \\mathrm{Var}(Bf) = B\\, \\mathrm{Var}(f)\\, B^T $$ $$ \\Omega_r = B\\, \\Omega_f\\, B^T + \\Omega_\\epsilon $$ Socratic Q\\&A Q: What does the first term \\(B \\Omega_f B^T\\) mean in words? A: It shows how group movements (factors) flow through to each asset and spread out to influence asset risk. Q: Why do we need \\(B\\) and \\(B^T\\) , not just \\(\\Omega_f\\) ? A: Because \\(B\\) tailors the factor wiggles to each asset, and $B^T $connects back to all assets, creating a full map of shared risk. Q: What does \\(\\Omega_\\epsilon\\) add to the mix? A: It captures each asset\u2019s personal quirks\u2014randomness that factors cannot explain. Q: Why is the sum of two covariances\u2014the shared and the unique? A: Because return variations come from big market shocks (factors) and small asset-specific surprises (errors). Q: What\u2019s the practical importance of splitting up risks this way? A: It lets investors see what risks can be diversified (factor risks) and what is stubborn (idiosyncratic risk). [^1]","title":"Linear Model of Returns"},{"location":"Quant%20Finance/Linear%20Model%20of%20Returns/#volatility-decomposition","text":"$$ \\Omega_r = B\\, \\Omega_f\\, B^T + \\Omega_\\epsilon $$ This equation tells us what causes the ups and downs in a group of asset returns, and how those sources mix together. If you want to understand risk, build portfolios, or analyze any group of assets, this formula is a fundamental \u201crecipe\u201d for how returns vary together. Start from the recipe for asset returns: $$ r = Bf + \\epsilon $$ The covariance (how things wiggle together) of \\(r\\) is: $$ \\Omega_r = \\mathrm{Var}(r) $$ Plug in our recipe: $$ = \\mathrm{Var}(Bf + \\epsilon) $$ If errors and factors are independent, the variance of a sum is just the sum of variances: $$ = \\mathrm{Var}(Bf) + \\mathrm{Var}(\\epsilon) $$ When you scale factors by B, variances get mapped like so: $$ \\mathrm{Var}(Bf) = B\\, \\mathrm{Var}(f)\\, B^T $$ $$ \\Omega_r = B\\, \\Omega_f\\, B^T + \\Omega_\\epsilon $$","title":"Volatility Decomposition"},{"location":"Quant%20Finance/Linear%20Model%20of%20Returns/#socratic-qa","text":"Q: What does the first term \\(B \\Omega_f B^T\\) mean in words? A: It shows how group movements (factors) flow through to each asset and spread out to influence asset risk. Q: Why do we need \\(B\\) and \\(B^T\\) , not just \\(\\Omega_f\\) ? A: Because \\(B\\) tailors the factor wiggles to each asset, and $B^T $connects back to all assets, creating a full map of shared risk. Q: What does \\(\\Omega_\\epsilon\\) add to the mix? A: It captures each asset\u2019s personal quirks\u2014randomness that factors cannot explain. Q: Why is the sum of two covariances\u2014the shared and the unique? A: Because return variations come from big market shocks (factors) and small asset-specific surprises (errors). Q: What\u2019s the practical importance of splitting up risks this way? A: It lets investors see what risks can be diversified (factor risks) and what is stubborn (idiosyncratic risk). [^1]","title":"Socratic Q\\&amp;A"},{"location":"Quant%20Finance/Linear%20Regressions/","text":"","title":"Linear Regressions"},{"location":"Quant%20Finance/Stylized%20Facts%20for%20Return%20Distribution/","text":"Stylized facts are empirical features of financial return data Lack of Autocorrelation in Returns Daily returns typically show little to no correlation with their past values. Heavy Tails Return distributions exhibit more frequent extreme outcomes (both positive and negative) than would be expected under a normal distribution. This property is known as \u201cheavy tails\u201d or \u201cleptokurtosis.\u201d In practice, this means large losses or gains happen more often than simple models assume, which has implications for risk management. Volatility Clustering Periods of high volatility tend to follow other high-volatility periods, and the same applies to low-volatility periods. Although returns themselves are not significantly correlated, the magnitude or intensity of returns tends to be. This persistence in volatility is known as volatility clustering, and it suggests that volatility is time-varying rather than constant. Aggregational Gaussianity Although daily returns are not normally distributed, when returns are aggregated over longer time periods (such as weeks or months), their distribution tends to look more like a normal distribution.","title":"Stylized Facts for Return Distribution"},{"location":"Rust/Async%20Rust/","text":"What is a runtime? Tokio is the async runtime for Rust. Think: thread pool + task scheduler + event loop. It drives your futures to completion. It gives you #[tokio::main] to start your async main() . It provides async I/O (files, network, timers, etc.). Tokio's Job: Poll your futures repeatedly . Wait for external events (e.g., HTTP done, file ready). Wake up your future and call poll() again only when needed . One-liner intuition: poll() = \"Are you done yet?\" Future: \"No.\" \u2192 Pending Future: \"Yes!\" \u2192 Ready(output) FuturesUnordered","title":"Async Rust"},{"location":"Rust/Async%20Rust/#what-is-a-runtime","text":"Tokio is the async runtime for Rust. Think: thread pool + task scheduler + event loop. It drives your futures to completion. It gives you #[tokio::main] to start your async main() . It provides async I/O (files, network, timers, etc.).","title":"What is a runtime?"},{"location":"Rust/Async%20Rust/#tokios-job","text":"Poll your futures repeatedly . Wait for external events (e.g., HTTP done, file ready). Wake up your future and call poll() again only when needed .","title":"Tokio's Job:"},{"location":"Rust/Async%20Rust/#one-liner-intuition","text":"poll() = \"Are you done yet?\" Future: \"No.\" \u2192 Pending Future: \"Yes!\" \u2192 Ready(output)","title":"One-liner intuition:"},{"location":"Rust/Async%20Rust/#futuresunordered","text":"","title":"FuturesUnordered"},{"location":"Rust/Concepts/","text":"Semicolons Q : What do semicolons do in Rust? A : Think of it this way: Rust forces you to pick a side. Are you producing a value, or are you just doing something? Q : So an expression produces a value, like 1 + 2 \u2192 3 ? A : Exactly. A statement is the opposite\u2014it\u2019s an action with no return value. let x = 5; or println!() don\u2019t yield anything useful, they just do . Q : And blocks like { 1 + 2 } ? A : That\u2019s an expression. The block evaluates to whatever its last line evaluates to. { let x = 5; x + 1 } \u2192 6 . Q : So what happens if I slap a semicolon on the end: { let x = 5; x + 1; } ? A : You just killed it. The semicolon discards the value. It converts an expression into a statement. No return, no value, nothing to pass back. Q : So semicolons are basically Rust\u2019s way of saying: \u201cshut up, I don\u2019t care about your result.\u201d A : That\u2019s it. And this matters, because Rust won\u2019t let you be vague. A function must either return something or not\u2014no half-measures. That\u2019s why you drop the semicolon in function bodies, async blocks, and match arms when you do want the value to flow out. derive keyword [derive(TraitName)] is a compile-time code generation mechanism that uses attributes to automatically implement specific, common traits for your data structures. It's a significant productivity booster and a core feature for writing idiomatic Rust.","title":"Concepts"},{"location":"Rust/Concepts/#semicolons","text":"Q : What do semicolons do in Rust? A : Think of it this way: Rust forces you to pick a side. Are you producing a value, or are you just doing something? Q : So an expression produces a value, like 1 + 2 \u2192 3 ? A : Exactly. A statement is the opposite\u2014it\u2019s an action with no return value. let x = 5; or println!() don\u2019t yield anything useful, they just do . Q : And blocks like { 1 + 2 } ? A : That\u2019s an expression. The block evaluates to whatever its last line evaluates to. { let x = 5; x + 1 } \u2192 6 . Q : So what happens if I slap a semicolon on the end: { let x = 5; x + 1; } ? A : You just killed it. The semicolon discards the value. It converts an expression into a statement. No return, no value, nothing to pass back. Q : So semicolons are basically Rust\u2019s way of saying: \u201cshut up, I don\u2019t care about your result.\u201d A : That\u2019s it. And this matters, because Rust won\u2019t let you be vague. A function must either return something or not\u2014no half-measures. That\u2019s why you drop the semicolon in function bodies, async blocks, and match arms when you do want the value to flow out.","title":"Semicolons"},{"location":"Rust/Concepts/#derive-keyword","text":"","title":"derive keyword"},{"location":"Rust/Concepts/#derivetraitname-is-a-compile-time-code-generation-mechanism-that-uses-attributes-to-automatically-implement-specific-common-traits-for-your-data-structures-its-a-significant-productivity-booster-and-a-core-feature-for-writing-idiomatic-rust","text":"","title":"[derive(TraitName)] is a compile-time code generation mechanism that uses attributes to automatically implement specific, common traits for your data structures. It's a significant productivity booster and a core feature for writing idiomatic Rust."},{"location":"Rust/Error/","text":"derive keyword #[derive(TraitName)] is a specific type of attribute that tells the Rust compiler to generate a default implementation for the specified TraitName based on the structure of the item it's attached to.","title":"Error"},{"location":"Rust/Error/#derive-keyword","text":"#[derive(TraitName)] is a specific type of attribute that tells the Rust compiler to generate a default implementation for the specified TraitName based on the structure of the item it's attached to.","title":"derive keyword"},{"location":"Rust/Rust%20Jupyter%20Notebooks/","text":"1 2 cargo install evcxr_jupyter evcxr_jupyter --install Select Rust Jupyter Kernel in VSCode Caveats Dependencies are added via :dep <DEPENDENCY_NAME> = \"<VERSION_NAME> and are compiled everytime you restart kernel. Keep all dependencies in separate cell.","title":"Rust Jupyter Notebooks"},{"location":"Rust/Rust%20Jupyter%20Notebooks/#caveats","text":"Dependencies are added via :dep <DEPENDENCY_NAME> = \"<VERSION_NAME> and are compiled everytime you restart kernel. Keep all dependencies in separate cell.","title":"Caveats"},{"location":"Rust/Tying%20Traits%20to%20Lifetime/","text":"Every method in this trait that hands out or consumes references is chained to a specific borrow duration. 1 2 3 4 5 6 7 struct Person <' a > { name : & ' a str , } trait Named { fn name ( & self ) -> & str ; } // Compiler will shout an error if you implement Named on person 1 2 3 trait Named <' a > { fn name ( & self ) -> & ' a str ; } Now the methods in the trait are parameterized with a lifetime parameter. Now we\u2019re saying: \u201cWhen something implements Named<'a> , its name() method gives back a string slice that lives as long as 'a .\u201d","title":"Tying Traits to Lifetime"},{"location":"Spells/Rsync/","text":"Synchronise files in Linux 1 rsync -av --exclude-from=.gitignore path1/ path2/","title":"Rsync"},{"location":"Spells/Rsync/#synchronise-files-in-linux","text":"1 rsync -av --exclude-from=.gitignore path1/ path2/","title":"Synchronise files in Linux"}]}